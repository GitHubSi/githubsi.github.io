<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2019-05 on 渐行渐远</title>
    <link>/categories/2019-05/</link>
    <description>Recent content in 2019-05 on 渐行渐远</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 18 May 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/2019-05/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>数据一致性（三）</title>
      <link>/blog/2019/19-05-18-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/</link>
      <pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-05-18-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/</guid>
      <description>一个人的时候多一点努力，才能让自己的爱情，少一点条件，多一点纯粹
 Quorum 在有冗余数据的分布式存储系统中，数据会在不同的机器上存放多份拷贝。但是同一时刻一个对象的多份拷贝只能用于读或者用于写。该算法可以保证同一份数据对象的多份拷贝不会被超过两个访问对象读写。
分布式系统中每一份数据拷贝对象被赋予一票。每一个读操作获得的票数必须大于最小读票数 $V_r$，每个写操作获得的票数必须大于最小写票数 $V_w$才能读或者写。如果系统有V票，那么最小读写票数应该满足如下限制：
 $V_r$ + $V_j$ &amp;gt; V $V_w$ &amp;gt; V/2  第一条规则保证了一个数据不会被同时读写。当一个写操作请求过来的时候，它必须要获得$V_w$个冗余拷贝的许可。而剩下的数量是V-$V_w$不够$V_r$，因此不能再有读请求过来。同理，当读请求已经获得了$V_r$个冗余拷贝的许可时，写请求就无法获得许可了。
第二条规则保证了数据的串行化修改。一份数据的冗余拷贝不可能同时被两个写请求修改。
注：上述内容来自于维基百科：Quorum
上述算法的思想来自于鸽巢原理，也是摘自维基百科：鸽巢原理
假如有n个笼子和n+1个鸽子，所有的鸽子都被关在鸽笼里，那么至少有一个笼子有至少2只鸽子。
集合论的表述如下：若A是n+1个元素，B是n个元素，则不存在从A到B的单射。这里也体现了Lamport的思想。
$V_r$和$V_w$的设置会直接影响系统的性能、扩展性和一致性。比如将$V_r$或者$V_w$设置为1，会体现出系统对读和写的不同侧重。
Two-phase commit 在分布式系统中，虽然每个节点内部的事务可以保证，但却无法保证其他所有节点的操作都成功或失败。当一个事物跨多个节点时，为了保证数据的一致性，需要引入一个协调者来统一掌控所有节点的操作。
第一阶段提交请求，参与者节点执行事务的操作，并将Undo信息和Redo信息写入日志。第二阶段参与者正式完成操作，并释放整个事务期间占用的资源。
 协调者 参与者 QUERY TO COMMIT --------------------------------&amp;gt; VOTE YES/NO prepare*/abort* &amp;lt;------------------------------- commit*/abort* COMMIT/ROLLBACK --------------------------------&amp;gt; ACKNOWLEDGMENT commit*/abort* &amp;lt;-------------------------------- end  注：内容来自于维基百科：二阶段提交
Optimistic Lock 在数据库低的隔离级别中，乐观锁也可以保证数据的一致性。区别于Pessimistic Lock，实际上Optimistic Lock并没有对数据库上锁，性能要优Pessimistic Lock。它的操作流程：
 更新一个新值 校验在更新时数据是否发生改变  Optimistic Lock大多数基于数据版本记录实现，或者是不可逆的状态字段。实际工作中，Pessimistic和Optimistic需要根据实际需要来使用，比如，如果要对多个数据表进行更新操作，Optimistic Lock就有点力不从心了。
Summary 当意见不一致的时候，大家可以选择投票。当票数一致的时候，大家还是会有很多策略来执行最优选择。计算机的世界也一样。</description>
    </item>
    
    <item>
      <title>mongo EOF（二）</title>
      <link>/blog/2019/19-05-11-mongo-eof%E4%BA%8C/</link>
      <pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-05-11-mongo-eof%E4%BA%8C/</guid>
      <description>任何事情的成功都需要掐准时间
 上一节mongo EOF中，关于容器的配置，只是粗略的使用了Docker-Compose-MongoDB-Replica-Set项目提供好的docker-compose.yml文件。在使用过程中，我发现这个文件本身一些不如意的地方。首先，services中的creator服务，entrypoint指令太长了，不美；其次，所有的service都没有给容器外部暴露端口，导致外部无法访问容器；再次，直接使用mongo repliSet的连接串进行访问，无法正常访问mongo服务。
在上一篇文章的基础上，这篇文章对docker-compse.yml做了一些调整，并且也包含了docker使用的入门介绍。更新后的docker-compose.yml请访问githubsi。
depends_on  However, for startup Compose does not wait until a container is “ready” (whatever that means for your particular application) - only until it’s running. There’s a good reason for this.
 在creator service中使用了该指令， 但是，实际中creator不会等到mongo1、mongo2、mongo3容器ready后再启动，而是等到它们启动就开始启动。这也是我在setup脚本中执行sleep操作的原因。
creator: build: context: . dockerfile: dockerfile entrypoint: [&amp;quot;/data/conf/setup.sh&amp;quot;] depends_on: - mongo1 - mongo2 - mongo3  entrypoint  Entrypoint sets the command and parameters that will be executed first when a container is run.</description>
    </item>
    
  </channel>
</rss>