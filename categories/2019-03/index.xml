<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2019-03 on 渐行渐远</title>
    <link>/categories/2019-03/</link>
    <description>Recent content in 2019-03 on 渐行渐远</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 30 Mar 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/2019-03/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Go 调度模型（二）</title>
      <link>/blog/2019/19-03-30-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%BA%8C/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-30-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%BA%8C/</guid>
      <description>真的猛士，敢于直面惨淡的人生，敢于正视淋漓的鲜血。这是怎样的哀痛者和幸福者？然而造化又常常为庸人设计，以时间的流驶，来洗涤旧迹，仅使留下淡红的血色和微漠的悲哀。在这淡红的血色和微漠的悲哀中，又给人暂得偷生，维持着这似人非人的世界。我不知道这样的世界何时是一个尽头！
 有一种感觉，Go调度模型可能还得再来几篇博客，才能真正读出感觉来。现在还是一个门外汉。越是深入了解，越觉得知之甚少，后背发凉。
OS调度结构 Go Runtime实现了自己的调度策略，从OS调度结构的演变来看，调度思想都是相似的。当然，我始终觉得设计背后的思想才是整个系统的核心。思想从无到有、从中心化到去中心化、从单任务到并行多任务，当然这肯定不是调度策略的专利，在很多场景都有这种思想的体现，比如分布式设计。
那为什么我还是没有一眼看出来呢，可能是阅历太少，思考的深度不够，积极接受现状，懒于了解历史吧。不过，越是去了解，就会发现不会的太多，思维发散的太广，难以为继。
single scheduler 存在一个全局的任务队列和一个全局的调度器，因为整个过程不需要加锁，所以单核吞吐量很高，但无法充分利用多核资源。
有点类似于：给一个数据表中的所有用户PUSH消息，虽然我们有10台服务器，但我们只在其中一台服务上执行该任务。优点是设计开发简单，缺点是没有充分利用资源，效率不高。
multi scheduler with global queue 多个调度器共享一个全局的任务队列，该模型需要频繁的对任务队列进行加锁，并发性能存在明显的瓶颈。这同时让我想起了Go的并发问题中介绍的例子，加锁保证了计算的正确性，但却牺牲了效率。当然，这不仅仅是调度系统会面临的问题，比如本地缓存BigCache也遇到了同样的问题。
还接着上面的例子说，当在多台服务器上都启动Task执行任务时，为了避免同一个用户不被重复PUSH多次，势必也面临着对单条记录加锁的问题。
multi scheduler with local queue 给每个调度器分配一个本地的任务队列，这样调度器就可以无锁的操作本地任务队列，显著减少锁竞争，提高多核下的调度效率。同时还要保证让各个调度器随时都有事情可做，所以也存在一些任务迁移或者任务窃取的方案。到了这里，我们就已经看到了Go Scheduler的雏形。其实思维很简单，将全局的任务队列划分成多个小的任务队列，各个调度器处理自己的任务队列，跟Database Sharding异曲同工。
继续上面的例子，我们只需要给各个服务器分配用户表的小块数据，当Task执行完分配的数据块后，再去请求新的数据块就可以了。
GPM 现在提到Go Scheduler就会直接想到GPM，但之前的设计里Scalable Go Scheduler Design Doc其实并不存在P。P的引入直接将调度模型由multi scheduler with global queue跨越到multi scheduler with local queue。
 每个Goroutine需要对应一个G结构体，而G保存了当前Goroutine的运行堆栈和状态信息。Goroutine通过G中保存的信息可以执行或恢复执行。 P是专门被引入用来优化原始Go调度系统所抽象的逻辑对象，操作系统并不知道P的存在。对M而言，P提供了其执行的相关环境、以及Goroutine的任务队列等。 M是OS线程的抽象，是物理存在的。M只有和P绑定之后，才可以执行G代码。M本身也不会保存G的状态，在需要任务切换时，M会将堆栈状态保存回G中，任何M都可以根据G中的信息恢复执行。  M阻塞 当M准备执行Goroutine时，首选需要关联一个P，然后从P的队列中取出一个G来执行。如果G中执行的代码使M发生阻塞，比如唤起系统调用。那么M将会一直阻塞，直到系统调用返回。此时全局空闲M队列的另一个M会被唤醒，同时，阻塞状态的M会与P解绑。这样做也是为了保证其他G不会因为缺少M而被阻塞执行。
但如果Goroutine在channel通讯过程中发生阻塞，M并不会展示相似的阻塞行为。因为OS并不了解channel 的执行机制，channel是被Go Runtime来处理的。如果一个Goroutine在channel通讯上发生了阻塞，那没有任何理由让运行它的M也阻塞。这种情况下，G的状态会被设置为等待，M会继续执行别的Goroutine。当G重新变成可运行状态时，等待别的M去执行。
P的改进 原始Go的调度并没有P，仅有G、M以及Sched。当时系统只存在一个全局的G队列，通过Sched锁来进行并发控制。存在的问题有：
 调度的执行依赖全局的Sched锁，修改全局的M队列和G队列、或者其他全局的Sched字段都需要加锁 M的内存问题，执行的内存是跟M相关联的。但即使M并不执行G代码，它也会申请2MB的MCache空间，而这些空间只有M在执行G时才需要。同时，一个阻塞中的M也是不需要MCache的。 系统调用不够清晰，M在执行中会频繁阻塞和恢复，浪费CPU时间 M之间频繁的传递G，而不是选择自己执行它，这增加了系统的额外负载。每个M必须能够执行任何可运行的G，特别是刚刚创建了G的M。  P引入之后，从之前的M和Sched中抽取了部分字段，这样做带来了很多好处：
 MCache就被移动到了P中，而系统最多存在GOMAXPROCES个P，解决了不必要的内存浪费问题 G freelist被移动到P中，每个P都有了一个可运行的本地G队列。本地G队列缓解了全局Sched锁的问题。 当一个G被M创建，它被追加到对应P的本地队列末尾，以保证每个G都能被执行。  参考文章：</description>
    </item>
    
    <item>
      <title>基于Go的Cron Job实现</title>
      <link>/blog/2019/19-03-25-%E5%9F%BA%E4%BA%8Ego%E7%9A%84cron-job%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-25-%E5%9F%BA%E4%BA%8Ego%E7%9A%84cron-job%E5%AE%9E%E7%8E%B0/</guid>
      <description>随风要稳，逆风要浪
 timer  The Timer type represents a single event. When the Timer expires, the current time will be sent on C.
 下面使用timer实现在固定时间点执行task任务。
处理思路：每次在执行task前，计算当前时间和执行时间点的差值，通过设置timer未来的触发时间来执行任务。在完成本次task之后，重置timer的触发时间，等待下一次执行。
const IntervalPeriod time.Duration = 24 * time.Hour // 核心函数：在h:m:s的时候执行task任务 func runningRoutine(hour, minute, second int, task func() error) { ticket := time.NewTimer(GetNextTickDuration(hour, minute, second)) for { &amp;lt;-ticket.C if err := task(); err != nil { } ticket.Reset(GetNextTickDuration(hour, minute, second)) } } // 获取Task执行的时间 func GetNextTickDuration(hour, minute, second int) time.</description>
    </item>
    
    <item>
      <title>Go 调度模型（一）</title>
      <link>/blog/2019/19-03-24-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%80/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-24-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%80/</guid>
      <description>想清楚了就去做，做的时候不要再回头想。
 OS Scheduler 在操作系统中保存了运行的进程列表，以及进程的运行状态(运行中、可运行及不可运行)。当进程运行时长超过了被分配的时间片(比如每10ms)，那么该进程会被系统抢占，然后在该CPU上执行别的进程。所以，OS的调度是抢占式的，可能抢占策略略有不同。
当进程被抢占时，需要保存该进程运行的上下文，并被重新放回到调度器，等待下一次被执行。
Golang Scheduler  Goroutine scheduler
The scheduler&amp;rsquo;s job is to distribute ready-to-run goroutines over worker threads.
 如图所示，OS层看到是只有Go进程以及运行的多个线程，而Goroutine本身是被Golang Runtime Scheduler调度管理的。
对OS而言，Go Binary是一个系统进程。内部Go Program对系统API的调度都是通过Runtime level解释来实现。Runtine记录了每个Goroutine的信息，在当前进程的线程池中按照顺序依次调度Goroutine。
Golang在Runtime内部实现了自己的调度，并不是基于时间切片的抢占式调度，而是基于Goroutines的协作式调度，目的就是要让Goroutine在OS-Thread中发挥出更多的并发优势。所以，在Runtime过程中，只有当正在运行的Goroutine被阻塞或者运行结束时，别的Goroutine才会被调度。常见的阻塞情形包括：
 阻塞的系统调用方式，比如文件或网络操作 垃圾自动回收  整体而言，Goroutine的数量大于Threads数量会更有优势，这样当其他Goroutine阻塞时，别的Goroutine就会被执行。
Goroutine G用于表示Goroutine及它所包含的栈和状态信息。Goroutine存在于Go Runtime的的虚拟空间，而非OS中。
// src/runtime/runtime2.go // 以下结构体精简了很多字段 type g struct { stack stack // offset known to runtime/cgo m *m // current m; offset known to arm liblink sched gobuf stktopsp uintptr // expected sp at top of stack, to check in traceback param unsafe.</description>
    </item>
    
    <item>
      <title>无聊的自我期待</title>
      <link>/life/2019/19-03-24-%E6%97%A0%E8%81%8A%E7%9A%84%E8%87%AA%E6%88%91%E6%9C%9F%E5%BE%85/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/life/2019/19-03-24-%E6%97%A0%E8%81%8A%E7%9A%84%E8%87%AA%E6%88%91%E6%9C%9F%E5%BE%85/</guid>
      <description>我慢慢明白了我为什么不快乐，因为我总是期待一个结果。
 事情并不会按你的意愿去发展 每一个买彩票的人，或者在被抽奖之前，都觉得自己是天选之人。然而，这样的个人意愿甚至至今都没能实现。客观理性的讲，无须执着低概率的事情，由它去吧，脑补它的结果，没有任何意义。
这种幻想发横财的事情如此，真实的世界亦如此。
人与人之间，如果不存在某种特殊关系，比如上下级，那么任何人没有义务执行你的设定。毕竟人是有主观想法的高等动物，有时候不给你使坏就很好很好了。</description>
    </item>
    
    <item>
      <title>数据一致性（二）</title>
      <link>/blog/2019/19-03-16-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E4%BA%8C/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-16-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E4%BA%8C/</guid>
      <description>我们流连于事物的表象，满足浅尝辄止的片刻欢愉，却几乎从不久留。我们在人生的道路上争先恐后，却吝于用片刻思考目标和方向。
 概述 至今没有接触过MySQL多主的情况，即存在多个MySQL实例同时负责读写请求（抛弃只读库）。思考后认为：没有这么实现的技术难点在于：数据的一致性得不到保证。此外，还会涉及：
 MySQL采用自增主键索引的话，多主之间的数据同步简直是灾难。 内部锁机制的优势大打折扣，跨主库间的锁应该也是灾难级别的吧。  那么支持分布式的其他数据库又是怎么搞定这个问题的呢？比如Cassandra，多个节点之间可以同时处理读写请求，那么它是如何处理节点间数据同步以保证一致性的呢？
MySQL数据的一致性  We think this is an unacceptable burden to place ondevelopers and that consistency problems should be solved at the database level
 细细想想，MySQL自身实现的数据一致性也是相当复杂的。以Innodb举例，如果通过普通索引执行查询，首先获取到的仅仅是主键索引，后面还需要通过主键索引来获取完整的记录。查询如此，更新亦如此。
Master-Slave模式 通常情况下，MySQL部署都是一主多从。Master作为更新DB的入口，而Slave的数据通过binlog来进行同步。所以大胆想一想，有没有可能出现一种情况（假设id=1记录原始的name值为neojos）：
## 第一次同步数据 update s-1 set name=&amp;quot;neojos-1&amp;quot; where id = 1; ## 失败 update s-2 set name=&amp;quot;neojos-1&amp;quot; where id = 1; ## 成功 update s-3 set name=&amp;quot;neojos-1&amp;quot; where id = 1; ## 成功 ## 第二次同步数据 update s-1 set name=&amp;quot;neojos-2&amp;quot; where id = 1; ## 成功 update s-2 set name=&amp;quot;neojos-2&amp;quot; where id = 1; ## 失败 update s-3 set name=&amp;quot;neojos-2&amp;quot; where id = 1; ## 成功  最后，数据库从某一个时间点开始，Master和Salve的数据会变得不一致了当然不可能，MySQL在数据同步上做了非常硬的约束。包括Slave_IO_Running、Slave_SQL_Running以及Seconds_Behind_Master等。</description>
    </item>
    
    <item>
      <title>平常心</title>
      <link>/life/2019/19-03-12-%E5%B9%B3%E5%B8%B8%E5%BF%83/</link>
      <pubDate>Tue, 12 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/life/2019/19-03-12-%E5%B9%B3%E5%B8%B8%E5%BF%83/</guid>
      <description>愿你在所得少于付出时，不会终日愤愤；愿你在所得超过付出时，不必终日惶恐。
—— 东野圭吾
 一切都很准时  New York is 3 hours ahead of California
But it does not make California slow
Someone graduated at the age of 22
but waited 5 years before securing a good job
Someone become a CEO at 25
and died at 50
while another become a CEO at 50
and lived to 90 years
Someone is still single
while someone else got married</description>
    </item>
    
    <item>
      <title>Database Sharding</title>
      <link>/blog/2019/19-03-09-database-sharding/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-09-database-sharding/</guid>
      <description>Sharding 可以简单地认为Sharding就是对数据进行分组的过程，即将整个大的数据集按照某种规则分割成多个小数据集。类似于网站服务，针对不同的服务，提供服务的链接地址也不相同，而这其实也是一个Sharding的过程。在业务层实现的Sharding，关键就在Route的过程，即将具体的数据请求，发送到对的数据集上。
基本要求：Sharding前后执行相同的查询，返回的结果也相同。
Why Sharding 在一些本地缓存的开发中，如果以map的形式存储数据集，因为该类型不支持并发操作。所以，在读写操作时就需要对map进行加锁，。可想而知，每次操作都加锁、解锁，而读写缓存又是一个高频操作，性能当然上不去。解决的思路就是对数据集进行Sharding操作，将整个数据集拆分成多个小块数据集，这样分别对小块数据集进行加锁、解锁，性能就提高了不少。
如果数据集过大，表的检索性能会越来越低。而如果对数据集进行分片，对分片数据并发检索，以及将某些分片数据直接加载到内存，都可以极大提高操作的效率。
数据均匀分布 拿博客网站举例，老用户发布的博客肯定要多于新注册的用户。那么，在对博客记录进行分表操作时，就需要考虑数据均匀分布的问题，避免老用户都分布在一张表内，造成某张表数据额外大。
解决的办法很简单，即对Sharding Key先做Hash处理，然后再实现数据Sharding过程。比如在系统设计初期，我们考虑基于用户UID将博客数据拆分成4个表。最基础的分表策略：
// 最终的结果就是blog_0, blog_1, blog_2, blog_3 tablePrefix := blog_%d tableName := fmt.Sprintf(&amp;quot;%s&amp;quot;, uid&amp;amp;3)  业务类型 分表过程需要结合实际项目，不同的业务场景，需求千差万别、更别说底层的数据了。拿购物场景来说，以商铺为分表Key，将同商铺的交易数据存储到一起，跟以用户为分表Key，将同用户的交易数据存储到一起。这两种情形完全不同，而这也将决定项目后期数据统计的方式。
所以分区的根本再于业务要执行的具体数据操作，要知道跨分区来之行Join之类的操作，不仅处理麻烦，性能也是问题。
服务化 实际项目中，按单一维度划分业务数据几乎是不可能的。比如上面说到的购物场景，商铺是一个维度，用户同样也是一个维度，当然，产品的类型还是一个维度。
这种情况下，我们一般选择将项目拆分成多个微服务，划分各个微服务间操作数据的边界范围。而服务与服务间的数据交互都通过调用API来实现。
需要注意的是，服务拆分一定要从具体业务来考虑，尽可能将相关的数据放在一起，提高检索性能，避免每次操作都需要通过网络来发送数据。
唯一ID 将数据集划分为多个分区后，基于不同的业务，查询场景也会面临各种各样的问题。比如在博客网站中，假设我们基于用户做了Sharding分表，而查询需求是：按照博客的发布时间顺序来展示列表。所以，这样的分页查询会异常痛苦。假设列表每页展示20条记录，那么代码就需要从每张表中都取出最近的20条记录做merge处理。如果Sharding的个数足够多，那简直无法继续了。
-- 获取最新的20条发帖记录。如果要翻页的话，将Now替换为上一批数据的最早时间 select * from blog_0 where create_time &amp;lt; Now() order by id desc; select * from blog_1 where create_time &amp;lt; Now() order by id desc; select * from blog_2 where create_time &amp;lt; Now() order by id desc; select * from blog_3 where create_time &amp;lt; Now() order by id desc;  正确的处理方式是创建一张关系表，记录所有博客元信息。而上述分页操作都基于该关系表来实现。但Sharding中的各表都有各自的自增主键，都从1开始自增，这就导致各个表中存在相同的标识符，再关系表中无法做区分。解决的办法大致有两种：</description>
    </item>
    
    <item>
      <title>读书有感</title>
      <link>/life/2019/19-03-03-%E8%AF%BB%E4%B9%A6%E6%9C%89%E6%84%9F/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/life/2019/19-03-03-%E8%AF%BB%E4%B9%A6%E6%9C%89%E6%84%9F/</guid>
      <description>19年开始读一些文学书，内心多了30的焦虑，希望从别人的故事中汲取力量。
 创业，从一个小目标开始 创办了zendesk的企业家写的一本回忆录，从透支信用卡、挤经济舱到开全球限量版轿车、坐私人飞机，其中的艰辛只有他自己最清楚。作为一个看客，最触动我的便是：他们年过30，随便去任何一家公司都能领到不错的薪水，但还是毅然决然地选择了自己创业，他们的理由很简单，如果这次创业机会不能好好把握，自己的人生便是死局。
创业，可能是每个有梦想的人都必须走的一条路吧。</description>
    </item>
    
  </channel>
</rss>