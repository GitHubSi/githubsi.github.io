<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2018 on 付辉</title>
    <link>/tags/2018/</link>
    <description>Recent content in 2018 on 付辉</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Jun 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/2018/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>订单系统初识</title>
      <link>/blog/2018/06-08-%E8%AE%A2%E5%8D%95%E7%B3%BB%E7%BB%9F%E5%88%9D%E8%AF%86/</link>
      <pubDate>Fri, 08 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/06-08-%E8%AE%A2%E5%8D%95%E7%B3%BB%E7%BB%9F%E5%88%9D%E8%AF%86/</guid>
      <description>计划将订单系统做一下梳理，包括之前写的IAP支付。其中一些细节，自己描述的也不是特别满意。后续慢慢的完善。
整个订单系统，概括的讲，其实就是创建订单、支付订单、交付权益这三个过程。但里面涉及的东西，可能必我们想象的要多很多。
订单号 在调用支付时，一般都会要求创建订单号。该订单号是标识本地交易的凭证，必须做到系统唯一。
创建订单号也有很多学问。最差也得做到：首先，不重复；其次，别人不能通过订单号，推测出任何有意义的信息；再次，订单号长度一定要做好控制。
订单再支付 描述一个场景：用户下单之后，并没有立即支付，而是过了一段时间，重新支付之前下的订单。这种情况，如果涉及到扣减库存，还会设置订单的过期时间。
重新支付的时候可能会有点问题：很多支付接口，不允许使用同一个订单号支付两次，即使上一次没有支付成功。换句话说，当你想重新支付时，系统需要生成一个新的订单号。
这样会导致：用户仅仅成功购买了一个商品，后台却生成了数笔订单。后期的统计变得麻烦了不少。
我们的做法便是：在确认支付时，生成两个订单，一个父订单，一个子订单。父订单用来标识用户的购买行为，子订单用户跟第三方支付。
订单统一管理 对于业务简单的部门来说，订单号自己创建，自己管理完全足够了。
但公司想汇总各个部门的订单数据时会变得异常麻烦。所以需要统一的订单创建平台。该平台负责订单的创建、以及后续订单的状态管理。
整个系统也开始变得复杂起来了。订单创建、订单状态更新都需要通知“订单系统”。数据也开始出现不一致。
交付 在用户下单、支付成功之后，将购买的商品或权益给到用户。
实物商品 将用户的购买行为添加到他的购买记录等，让快递员将商品送到用户手中等。
虚拟商品 用户在王者荣耀上买了一套皮肤，这就属于购买虚拟商品。最终腾讯只需要给用户的数据库写一条权益记录就好，不存在发货的过程。
签收 拆单 用户可能一次性购买多个商品，但系统只生成一个订单。对于实物商品，会涉及的拆单流程。比如用户买了肥皂和书，后台系统需要将肥皂交给卖家A来发货，将书交给卖家B来发。同样的商品，可能还会根据卖家和买家的距离来拆单。
签收 那么当更新订单签收状态时，就有疑问了。一个订单，它可能对应多个商品，那么只有商品全部被签收成功，才应该修改为已签收状态。但这种全部成功或者全部失败的状态，本身就很难保证。
所以签收应该针对具体商品。商城系统中，有两个常用的概念：SPU和SKU。那小米手机来举例，小米Note可以当作是一个SPU,而具体的红色-32G等能具体到一个实际的个体的就是SKU。
最终，签收状态应该是订单号+SKU_ID来决定的。
表结构设计 在用户下单、支付的过程中，跟订单内部商品的SKU关系不大，而且为了保证订单的幂等性,将订单设置为唯一索引是必须的。
在签收的过程中，无法做到针对订单来签收，而应该对订单下的SKU做签收。所以签收表应该独立出来。</description>
    </item>
    
    <item>
      <title>shell操作文本实例</title>
      <link>/blog/2018/05-15-shell%E6%93%8D%E4%BD%9C%E6%96%87%E6%9C%AC%E5%AE%9E%E4%BE%8B/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-15-shell%E6%93%8D%E4%BD%9C%E6%96%87%E6%9C%AC%E5%AE%9E%E4%BE%8B/</guid>
      <description>从掌握awk的基本指令，到在工作中熟练使用，中间还有一段路要走！通过总结一些工作中需要的案例，来加深理解。
打印符合条件的前一行记录 这个案例很抽象，可能非常难遇到，除非自己给自己挖坑。
业务代码中处理每条数据，都会顺序输出两条日志。第一条表示要处理的数据内容，第二条表示处理的结果。现在想滤出来所有处理成功的记录。比如日志文件如下：
uid=1 HAPPY uid=2 SAD uid=3 SAD  处理的AWK脚本如下：
#! /bin/bash # testPage表示日志文件 pieces=$(awk &#39;/HAPPY/{line=NR-1;print line}&#39; testPage | xargs) for piece in $pieces do echo $(awk NR==$piece testPage) done  脚本总结：
 首先通过正则表达式过滤，获取执行成功的数据行号。然后传递给xargs，转换为空格分隔的字符串。 遍历每个行号。值得注意，shell中对空格分隔的字符串可以直接使用for...in 通过awk条件判断，打印该行内容  求解两个文件的差集 使用comm实现 存在A和B两个系统，理论上A系统中的数据都应该存在于B系统中。但当核对数据时，发现两者数据不一致。如何有效的找出数据的差集。
具体到真实环境，通过MySQL，导出了满足条件的A、B系统数据的id，文件格式是csv。但当我执行如下列命令，获取仅仅在文件a中存在的记录时，发现数据完全不正确。
comm -2 -3 a.txt b.txt  调查发现，需要将两个文本先排好序，才能正常返回。
sort a.txt &amp;gt; a-sort.txt sort b.txt &amp;gt; b-sort.txt comm -2 -3 a-sort.txt b-sort.txt  使用uniq及sort实现 如下这种方式，仅仅可以找出不匹配的记录。无法区分数据是仅仅存在A系统，还是仅仅存在B系统。只能获取数据的交集和差集两部分。
原理很简单，将两个系统的文件合并到一个文件，然后排序。最终交集的数据，应该有2条记录。差集的记录，只有1条。
cat a.txt b.</description>
    </item>
    
    <item>
      <title>代码重构</title>
      <link>/blog/2018/05-12-%E4%BB%A3%E7%A0%81%E9%87%8D%E6%9E%84/</link>
      <pubDate>Sat, 12 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-12-%E4%BB%A3%E7%A0%81%E9%87%8D%E6%9E%84/</guid>
      <description>重构自己的代码是一件幸福的事，重构别人的代码确是一件不幸的事。尤其是被重构代码的人还没有离职的时候&amp;hellip;
重构切记引入意外复杂度，同时要保证代码功能单一，没有冗余的、意义不明的代码存在。还要保证最简API原则。一个接口好坏，一定要明确区分：它不能再处理更多业务了，还是这些业务已经不能再被剥离了。
临时变量 这是重构的一个重要切入点。方法体内临时变量太多，尤其是一些词不表意的临时变量，理解维护是一件非常痛苦的事情。
是否可以将临时变量直接替换成方法调用，或者将临时变量规整到一个粒度更小的方法体中。
重复 代码重复，多处copy相同的代码，会让人迫不及待的想要重构。
项目中重复代码过多，会给维护、开发带来很大的不便。一个简单的逻辑修改，但却涉及修改好多处代码，还不能保证涉及的部分都修改了。这确实是一件头疼的事。
所以代码要做好封装：
 封装的粒度要把握好。一方面便于测试需要，另一方面通过组合，还能满足其他需求。 封装要考虑参数如何传递。包括是否应该包含成员变量，参数的个数多少合适。 封装的方法应该如何归类。怎样可以将方法归到最合适的类。  多态 当遇到很多的switch或者if-elseif的时候，可以考虑是否能用多态来替换。
比如下面的方法：
switch type { case movie.TV: case movie.Release }  需要特别提醒：movie包的常量作为判断条件，该方法就最好应该在movie包中。这样当需求变更时，便体现出最小修改原则。
我们提取一个movieType的抽象类，然后依次对每个类型实现相应的方法，通过声明类型为movieType的成员变量，实现不同类型的统一调用。
这里体现的是模块化的思想。将系统拆分成独立的模块，降低耦合度。这样做的好处：便于扩展。当新的类型添加时，对老的业务来说：零干扰。
但某些情况下，这样的拆分模式可能会有小缺陷。当类型是一个频繁被添加、修改的参数时，这样的模式就显得很冗余。这时可以使用属性拆分。将各个类型中的属性实现多态，也可以称做是一种策略转移。
使用类替换枚举类型 当一个类内含有多个常量枚举类型时，可以考虑将枚举类型的值封装成新的对象。这也是一个切入点。举个例子：
// original code const( Month int = iota Year int ) //modified code //将这些常量封装到另一个package中  访问成员变量 两个分歧：直接访问VS间接访问。两者均有好处，</description>
    </item>
    
    <item>
      <title>MySQL使用总结(一)</title>
      <link>/blog/2018/05-09-mysql%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%E4%B8%80-/</link>
      <pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-09-mysql%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%E4%B8%80-/</guid>
      <description>查询的执行时间 第一次遇到查询时候报超时。很好奇，别的工具是如何修改查询的操时时间。
set max_statement_time = 0;  By using max_statement_time, it is possible to limit the execution time of individual queries.
 The MySQL version of max_statement_time is defined in millseconds, not seconds. MySQL&amp;rsquo;s implementation can only kill SELECTs.  left join 这个语句执行起来特别的费劲，但需求是：找出A表中存在，但B表中不存在的记录。
on条件 一直以为on是在执行表关联时的判断逻辑，即两个表的记录要不要关联，全靠on。直到遇到left join。发现它完全没有理会on提供的左表过滤条件，它返回了左表的全部记录，需要将条件放到where中才生效。
举个例子
-- table_a.id &amp;gt; 2018 无效 select * from table_a left join table_b on table_a.id = table_b.a_id and table_a.id &amp;gt; 2018 -- 正确的方式 select * from table_a left join table_b on table_a.</description>
    </item>
    
    <item>
      <title>Go test基础用法</title>
      <link>/blog/2018/05-02-go-test%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95/</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-02-go-test%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95/</guid>
      <description>当直接使用IDE进行单元测试时，有没有好奇它时如何实现的？比如GoLand写的测试用例。
 所有的代码都需要写测试用例。这不仅仅是对自己的代码负责，也是对别人的负责。
最近工作中使用glog这个库，因为它对外提供的方法都很简单，想封装处理一下。但却遇到了点麻烦：这个包需要在命令行传递log_dir参数，来指定日志文件的路径。
所以，正常运行的话，首先需要编译可执行文件，然后命令行指定参数执行。如下示例：
go build main.go ./main -log_dir=&amp;quot;/data&amp;quot; //当前目录作为日志输出目录  但在go test的时候，如何指定这个参数了？
调查发现，发现go test也可以生成可执行文件。需要使用-c来指定。示例如下：
go test -c param_test_dir //最后一个参数是待测试的目录  执行后就会发现：这样的做法，会运行所有的Test用例。如何仅仅执行某一个测试用例了（编译器到底是如何做到的？）。
这里有另一个属性-run，用来指定执行的测试用例的匹配模式。举个例子：
func TestGetRootLogger(t *testing.T) { writeLog(&amp;quot;测试&amp;quot;) } func TestGetRootLogger2(t *testing.T) { writeLog(&amp;quot;测试2&amp;quot;) }  当我在命令行明确匹配执行Logger2，运行的时候确实仅仅执行该测试用例
go test -v -run Logger2 ./util/ //-v表示verbose，输出相信信息  但是，我发现，在指定了c参数之后，run参数无法生效！这样的话，还真是没有好的办法来处理这种情况。
Benchmark测试 关于如何运行Benchmark测试，默认执行go test并不会执行Benchmark，需要在命令行明确加上-bench=标记，它接受一个表达式作为参数，匹配基准测试的函数，.表示运行所有基准测试。
go test -bench=. // 明确指定要运行那个测试，传递一个正则表达式给run属性 go test -run=XXX -bench=.  默认情况下，benchmark最小运行时长为1s。如果benchmark函数执行返回，但1s的时间还没有结束，b.N会根据某种机制依次递增。可以通过参数-benchtime=20s来改变这种行为。
还有一个参数：benchmem。可以提供每次操作分配内存的次数，以及每次操作分配的字节数。
go test -bench=Fib40 -benchtime=20s  覆盖率 跟执行go test不同的是，需要多加一个参数-coverprofile,所以完整的命令：</description>
    </item>
    
    <item>
      <title>Saga Pattern</title>
      <link>/blog/2018/04-24-saga-pattern/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-24-saga-pattern/</guid>
      <description> 在微服务中，用的比较多的分布式事务模式：SAGA。
插播：在你觉得英文很难读懂的时候，别人却只是觉得有些英文论文很难读懂。所以，有时间就看一点这篇论文，总会看完的。
saga是一个本地事务的序列，每个事务都在各个微服务内部完成。通过外部的请求来开始第一个事务，且当前面的事务完成后，后面的事务就会被触发。
简要描述一下：
# 中心系统 - 充当分布式事务管理中心 1. 请求 - 订单服务 - 用户下单 2. 请求 - 库存服务 - 减少库存  下面介绍实现saga最流行的两种方式：
 Events 不需要一个中心调度系统，每个服务生产、监听别的服务产生的事件，决定下一步怎么处理。 Command 有一个中心服务来协调管理业务逻辑，做saga决策。  Events 在Events的方式中，各个服务执行完成事务之后，会发布一个event。其他服务会监听这个event，然后执行自己本地的事务，发布一个新的event。
当最后一个服务执行了本地事务，没有发布新的event,或者发布了其他服务不监听的event。分布式事务终止。
对这种方式来说，有效跟踪事务的执行状态是一个痛点。但实际工作中，确实需要明确知道事务的执行路径。两种解决办法：
 每个服务都更新当前的事务记录，记录可以存储在DB中，有几个服务，记录就应该有几个状态。 插入一个服务，监听所有服务的event。  补偿逻辑 分布式事务执行过程中，当其中一个事务执行失败之后，事务需要触发补偿逻辑。其原理还是发送一个event，只不过其他服务监听到之后，处发事务的补偿逻辑，回滚之前的本地操作。
实现方式 采用消息队列实现，以NSQ为例，可以这样考虑：
 event应该有一个唯一的身份标识。 每个服务至少应该监听一个topic，且至少作为一个topic的producer。 可以考虑一个统一的topic，多个channel的实现方式。  总结 当服务比较多的时候，topic可能会有很多，程序复杂性提高了不少。而且，一个留神，很可能让监听变成了一个死循环。
command </description>
    </item>
    
    <item>
      <title>docker基本使用</title>
      <link>/blog/2018/04-20-docker%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-20-docker%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid>
      <description>知道一点比完全不知道要好，对问题有深入了解比仅知道皮毛要好。作为docker的一个初学者，现在对docker做简单记录。希望随着工作、生活，更深入的了解学习docker。这也是一件很有意义的事。
docker有几个相关的概念：
 image 镜像 container 容器  我觉得之所以说docker好用，是因为Docker Hub提供了很多镜像，比如MySQL、Redis等。对它们安装、卸载异常方便。
下面举个例子，我们想搭建测试服务，安装MySQL，Redis等依赖。我们将他们当作一个项目的依赖，声明一个配置文件·db.yml，然后将这些依赖，类似于composer编辑：
version: &amp;quot;3&amp;quot; services: db: image: mysql:5.7 volumes: - /Users/neojos/dockerData/mysql restart: always environment: MYSQL_ROOT_PASSWORD: paytest MYSQL_DATABASE: paytest MYSQL_USER: neojos MYSQL_PASSWORD: neojos-pwd ports: - &amp;quot;3306:3306&amp;quot; myredis: image: redis restart: always volumes: - /Users/neojos/dockerData/redis ports: - &amp;quot;6379:6379&amp;quot; command: redis-server --appendonly yes  执行如下命令，MySQL和Redis的服务就启动了
docker-composer -f db.yml up  可以通过执行如下命令查看，确认是否有两个容器在运行。
docker container ls  这样很好，但当我想进去MySQL的容器内执行一些命令时，该怎么办呢？比如，我想确认下面的MySQL连接语句是否正确,而且我还一定要进去容器内执行MySQL命令行语句：
mysql -h 127.0.0.1 -P 3306 -u neojos -p&#39;neojos-pwd&#39; paytest  很简单,只需要执行如下指令。可以发现，已经进到MySQL命令行了。</description>
    </item>
    
    <item>
      <title>xorm使用reverse指令创建模版</title>
      <link>/blog/2018/04-19-xorm%E4%BD%BF%E7%94%A8reverse%E6%8C%87%E4%BB%A4%E5%88%9B%E5%BB%BA%E6%A8%A1%E7%89%88/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-19-xorm%E4%BD%BF%E7%94%A8reverse%E6%8C%87%E4%BB%A4%E5%88%9B%E5%BB%BA%E6%A8%A1%E7%89%88/</guid>
      <description>这只能算作一次小的功能介绍
结合我们使用过的go数据操作类的库，执行的逻辑基本都是：将数据库返回的数据，转换成我们提前声明的结构体对象，然后返回。
今天要介绍的就是如何自动创建每个table对应的结构体。
查看 xorm tool的介绍：
xorm reverse mysql root:@/xorm_test?charset=utf8 templates/goxorm  初看这个介绍，让我费了一段时间才理解。你可以在命令行查看它的具体含义：
xorm help reverse  命令中templates/goxorm其实是xorm提供好的模版路径。我错误的理解成了：执行命令生成结果的存储路径。
tmplPath Template dir for generated. the default templates dir has provide 1 template  其次就是mysql的连接语句：一般来说，都是这样写的：
username:pwd@ip:port/db?charset=utf8  但是使用上述方式却无法正常执行命令，正确的方式是：
xorm reverse &amp;quot;username:pwd@tcp(ip:port)/db?charset=utf&amp;quot; templates/goxorm  </description>
    </item>
    
    <item>
      <title>了解Laravel依赖注入</title>
      <link>/blog/2018/04-05-%E4%BA%86%E8%A7%A3laravel%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-05-%E4%BA%86%E8%A7%A3laravel%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/</guid>
      <description>随笔 突然想了解一下Laravel，然后发现：它没有我想象的那么简单，很多的调用都找不到入口。加载view的逻辑，看了很长时间，还是没有搞明白：一个值传递的参数，怎么好好的就变了呢？下面都是看别的的文章的总结，我还要继续完善，直到搞清楚这个view是怎么实现的。
看了两篇文章，介绍了如何使用xdebug断点调试php及测试性能。作为了解Laravel的必要工具，也介绍进来。
 How to Install Xdebug with PHPStorm and Vagrant Debugging and Profiling PHP with Xdebug  概要 使用use Illuminate\Container\Container;作为参考的例子。
可以浏览原创：Laravel Container (容器) 深入理解 (下)。
摘抄Laravel
 The Laravel service container is a powerful tool for managing class dependencies and performing dependency injection.
 通过config/app.php可以查看Laravel的Service container。Service下的register便是用来创建binding的。通过php artisan make:provider CustomServiceProvider创建自定义的ServiceProvider。
 There is no need to bind classes into the container if they do not depend on any interfaces.</description>
    </item>
    
    <item>
      <title>包管理工具</title>
      <link>/blog/2018/03-31-%E5%8C%85%E7%AE%A1%E7%90%86/</link>
      <pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/03-31-%E5%8C%85%E7%AE%A1%E7%90%86/</guid>
      <description>反思之前的过程，一直没有试图跟上技术的发展。恍然觉得，其实技术比买股票更能让我找到快乐。新的技术越来越多，能做的便是，持续保持蜗牛锲而不舍的精神，慢慢爬！
首先感谢这篇文章2018 年了，你还是只会 npm install 吗？，让我重新开始审视包管理工具。因为在PHP开发中有Composer，在Go的开发中有glide。但却没有尝试思考它们背后的那些为什么。
npm包管理 我一直不理解package.json和package-lock.json这两个文件的作用。直观上看，前者是我们项目所依赖的包，后者是各个包自身的明细依赖。但这样的设计却是经过多个版本迭代最终确定的形式。
当我们执行install或者update的时候，package-lock.json会根据nodemodules的更新而进行相应更新。当前就理解到这里，请看Composer
包的版本 包的版本号采用semver约束，由3个数字组成，格式必须为 MAJOR.MINOR.PATCH, 意为： 主版本号.小版本号.修订版本号。
约束还有一条：主版本号相同的升级版本必须提供向下兼容，但这仅仅是口头约束。测试版本的匹配，可以访问网址：https://semver.npmjs.com/。
 ^开头的版本：主版本号相同，大于等于小版本号的所有版本。 ~开头的版本：主版本、小版本号相同，大于等于修正版本的版本。 *或者x的版本：两者表示通配符。 在常规仅包含数字的版本号之外：表示不稳定的发布版本。  管理依赖 有时候，项目和项目之间存在引用依赖关系。比如将多个项目间共同使用的类在common项目下维护，然后其他项目project-1和project-2分别引用项目common。当project项目变得越来越多时，每次新的项目都需要手动拷贝common代码。
可以将common做为一个包来管理。创建package.json文件，将common项目托管到git仓库。执行npm install git_url就可以将common作为依赖包进行安装了。
npm除了安装git仓库的代码，也可以安装本地的代码。
npm install file:local-package-path  版本管理 svn或者git只需要提交package.json, package-lock.json, 不需要提交node_modules目录。
每次升级或降级版本，执行如下代码，相应的package.json，package-lock.json会自动更新：
npm install &amp;lt;package-name&amp;gt;@&amp;lt;version&amp;gt;  删除依赖包：
npm uninstall &amp;lt;package&amp;gt;  Composer管理 Composer生成的包管理目录叫vendor，它也是生成两个文件composer.lock和composer.json。composer.lock描述了项目的依赖以及其它的一些元信息。
composer.lock用来明确锁定安装包的具体版本信息，包证所有人安装的版本都是一致的。具体的原因在于：
 composer.json中指定的安装包版本，比如^2.0，只能确定该包的主版本号一定是2，当Composer在install的过程中，具体安装了该包符合条件的哪个版本，是无法从.json中看出来的。 同理，还是上面的例子，如果一个同事，数月前执行install安装的版本是2.0.0，后来这个包在2版本下发布了一个小版本2.1.0。另一个同事后来执行install，很可能就安装成了2.1.0  综上所述，composer.lock用来保证安装包的一致性，避免安装到不同的版本包，给生产环境带来的不确定性。
install/update install主要用来安装新包。当安装新包的时候，需要首先查看.lock文件是否存在，如果存在，安装.lock中指定的具体版本。如果不存在，直接安装。同时更新.json和.lock两个文件。
update主要用来更新.lock中安装的包。随着时间的推移，.json中的包可能又发布了新版本，所以update就是用来检查.json中包的新版本，更新.lock文件用的。
我在使用的过程中，比较倾向于使用下面的单个包操作的方式：
php composer.phar update monolog/monolog [...]  版本管理 在git环境中.json和.lock都需要被提交的版本控制。vendor目录就不需要啦。
glide版本管理 glide是go的版本管理工具。其实glide也是参考composer设计的，所以上面对composer的说法也同样有效。
在项目开发中也仅需要对 glide.yaml 和 glide.</description>
    </item>
    
  </channel>
</rss>