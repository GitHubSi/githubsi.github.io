<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>translate on 道法自然</title>
    <link>/tags/translate/</link>
    <description>Recent content in translate on 道法自然</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 18 May 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/translate/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>数据一致性（三）</title>
      <link>/blog/2019/19-05-18-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/</link>
      <pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-05-18-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/</guid>
      <description>一个人的时候多一点努力，才能让自己的爱情，少一点条件，多一点纯粹
 Quorum 在有冗余数据的分布式存储系统中，数据会在不同的机器上存放多份拷贝。但是同一时刻一个对象的多份拷贝只能用于读或者用于写。该算法可以保证同一份数据对象的多份拷贝不会被超过两个访问对象读写。
分布式系统中每一份数据拷贝对象被赋予一票。每一个读操作获得的票数必须大于最小读票数 $V_r$，每个写操作获得的票数必须大于最小写票数 $V_w$才能读或者写。如果系统有V票，那么最小读写票数应该满足如下限制：
 $V_r$ + $V_j$ &amp;gt; V $V_w$ &amp;gt; V/2  第一条规则保证了一个数据不会被同时读写。当一个写操作请求过来的时候，它必须要获得$V_w$个冗余拷贝的许可。而剩下的数量是V-$V_w$不够$V_r$，因此不能再有读请求过来。同理，当读请求已经获得了$V_r$个冗余拷贝的许可时，写请求就无法获得许可了。
第二条规则保证了数据的串行化修改。一份数据的冗余拷贝不可能同时被两个写请求修改。
注：上述内容来自于维基百科：Quorum
上述算法的思想来自于鸽巢原理，也是摘自维基百科：鸽巢原理
假如有n个笼子和n+1个鸽子，所有的鸽子都被关在鸽笼里，那么至少有一个笼子有至少2只鸽子。
集合论的表述如下：若A是n+1个元素，B是n个元素，则不存在从A到B的单射。这里也体现了Lamport的思想。
$V_r$和$V_w$的设置会直接影响系统的性能、扩展性和一致性。比如将$V_r$或者$V_w$设置为1，会体现出系统对读和写的不同侧重。
Two-phase commit 在分布式系统中，虽然每个节点内部的事务可以保证，但却无法保证其他所有节点的操作都成功或失败。当一个事物跨多个节点时，为了保证数据的一致性，需要引入一个协调者来统一掌控所有节点的操作。
第一阶段提交请求，参与者节点执行事务的操作，并将Undo信息和Redo信息写入日志。第二阶段参与者正式完成操作，并释放整个事务期间占用的资源。
 协调者 参与者 QUERY TO COMMIT --------------------------------&amp;gt; VOTE YES/NO prepare*/abort* &amp;lt;------------------------------- commit*/abort* COMMIT/ROLLBACK --------------------------------&amp;gt; ACKNOWLEDGMENT commit*/abort* &amp;lt;-------------------------------- end  注：内容来自于维基百科：二阶段提交
Optimistic Lock 在数据库低的隔离级别中，乐观锁也可以保证数据的一致性。区别于Pessimistic Lock，实际上Optimistic Lock并没有对数据库上锁，性能要优Pessimistic Lock。它的操作流程：
 更新一个新值 校验在更新时数据是否发生改变  Optimistic Lock大多数基于数据版本记录实现，或者是不可逆的状态字段。实际工作中，Pessimistic和Optimistic需要根据实际需要来使用，比如，如果要对多个数据表进行更新操作，Optimistic Lock就有点力不从心了。
Summary 当意见不一致的时候，大家可以选择投票。当票数一致的时候，大家还是会有很多策略来执行最优选择。计算机的世界也一样。</description>
    </item>
    
    <item>
      <title>Go 调度模型（三）</title>
      <link>/blog/2019/19-04-14-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%89/</link>
      <pubDate>Sun, 14 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-04-14-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%89/</guid>
      <description>  别抱怨，也别自怜，所有的现状都是你自己选择的
 前面的章节中，我们介绍了操作系统的调度模型：N:1、1:1、M:N。而Go采用了更高效的方式M:N。从进程的角度来说，线程是最小的调度单元。而Go的模型下，可以把P作为最小单元的调度单元，即在单个线程上运行的Go代码。
执行模型 下图展示了Go的最小调度单元模型。其中的有两个线程，各维持一个P对象，而且正在执行一个G代码。为了运行G，M必须首先持有P对象。图中灰色的G表示还没有被执行，等待被调度。它们被组织在P的一个runqueues的队列中，当M创建新的Goroutine时，对应的G就会被追加到该队列的末尾。
阻塞模型 为什么要引入P结构，直接将runqueues放在M中，不就可以摆脱P了吗？当然不行，它存在的意义还在于：当M因为其他原因被阻塞时，我们需要将runqueues中的G交给别的M来继续处理。因为线程不可能既执行代码，又阻塞在系统上。
如上图所示，当M0阻塞在系统调用上时，它会放弃自己的P，以保证M1可以继续执行其他G。当M0系统调用返回时，M0为了继续执行G0，就必须尝试重新获取P对象。正常的执行流程是：它尝试去偷其它线程的P，如果不行，就将G0放到全局的runqueues中，之后进入休眠。
当P本地的runqueues运行完之后，M会去全局队列取G来执行。同时，全局队列的G也会被间歇性检查，否则里面的G可能永远都得不到执行了。
偷G模型 当runqueues分布不均衡时，可能存在其中一个M执行完了本地的P，而其他P的本地队列还有很多G等待被执行。如图所示，为了去继续运行Go代码，P1首先会尝试去全局队列获取。如果全局队列没有，那么它就会随机从别的P去偷一半回来。这样做也是用来保证所有线程都一直有工作可以做。
参考文章：
 The Go scheduler  </description>
    </item>
    
    <item>
      <title>Go 调度模型（二）</title>
      <link>/blog/2019/19-03-30-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%BA%8C/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-30-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%BA%8C/</guid>
      <description>真的猛士，敢于直面惨淡的人生，敢于正视淋漓的鲜血。这是怎样的哀痛者和幸福者？然而造化又常常为庸人设计，以时间的流驶，来洗涤旧迹，仅使留下淡红的血色和微漠的悲哀。在这淡红的血色和微漠的悲哀中，又给人暂得偷生，维持着这似人非人的世界。我不知道这样的世界何时是一个尽头！
 有一种感觉，Go调度模型可能还得再来几篇博客，才能真正读出感觉来。现在还是一个门外汉。越是深入了解，越觉得知之甚少，后背发凉。
OS调度结构 Go Runtime实现了自己的调度策略，从OS调度结构的演变来看，调度思想都是相似的。当然，我始终觉得设计背后的思想才是整个系统的核心。思想从无到有、从中心化到去中心化、从单任务到并行多任务，当然这肯定不是调度策略的专利，在很多场景都有这种思想的体现，比如分布式设计。
那为什么我还是没有一眼看出来呢，可能是阅历太少，思考的深度不够，积极接受现状，懒于了解历史吧。不过，越是去了解，就会发现不会的太多，思维发散的太广，难以为继。
single scheduler 存在一个全局的任务队列和一个全局的调度器，因为整个过程不需要加锁，所以单核吞吐量很高，但无法充分利用多核资源。
有点类似于：给一个数据表中的所有用户PUSH消息，虽然我们有10台服务器，但我们只在其中一台服务上执行该任务。优点是设计开发简单，缺点是没有充分利用资源，效率不高。
multi scheduler with global queue 多个调度器共享一个全局的任务队列，该模型需要频繁的对任务队列进行加锁，并发性能存在明显的瓶颈。这同时让我想起了Go的并发问题中介绍的例子，加锁保证了计算的正确性，但却牺牲了效率。当然，这不仅仅是调度系统会面临的问题，比如本地缓存BigCache也遇到了同样的问题。
还接着上面的例子说，当在多台服务器上都启动Task执行任务时，为了避免同一个用户不被重复PUSH多次，势必也面临着对单条记录加锁的问题。
multi scheduler with local queue 给每个调度器分配一个本地的任务队列，这样调度器就可以无锁的操作本地任务队列，显著减少锁竞争，提高多核下的调度效率。同时还要保证让各个调度器随时都有事情可做，所以也存在一些任务迁移或者任务窃取的方案。到了这里，我们就已经看到了Go Scheduler的雏形。其实思维很简单，将全局的任务队列划分成多个小的任务队列，各个调度器处理自己的任务队列，跟Database Sharding异曲同工。
继续上面的例子，我们只需要给各个服务器分配用户表的小块数据，当Task执行完分配的数据块后，再去请求新的数据块就可以了。
GPM 现在提到Go Scheduler就会直接想到GPM，但之前的设计里Scalable Go Scheduler Design Doc其实并不存在P。P的引入直接将调度模型由multi scheduler with global queue跨越到multi scheduler with local queue。
 每个Goroutine需要对应一个G结构体，而G保存了当前Goroutine的运行堆栈和状态信息。Goroutine通过G中保存的信息可以执行或恢复执行。 P是专门被引入用来优化原始Go调度系统所抽象的逻辑对象，操作系统并不知道P的存在。对M而言，P提供了其执行的相关环境、以及Goroutine的任务队列等。 M是OS线程的抽象，是物理存在的。M只有和P绑定之后，才可以执行G代码。M本身也不会保存G的状态，在需要任务切换时，M会将堆栈状态保存回G中，任何M都可以根据G中的信息恢复执行。  M阻塞 当M准备执行Goroutine时，首选需要关联一个P，然后从P的队列中取出一个G来执行。如果G中执行的代码使M发生阻塞，比如唤起系统调用。那么M将会一直阻塞，直到系统调用返回。此时全局空闲M队列的另一个M会被唤醒，同时，阻塞状态的M会与P解绑。这样做也是为了保证其他G不会因为缺少M而被阻塞执行。
但如果Goroutine在channel通讯过程中发生阻塞，M并不会展示相似的阻塞行为。因为OS并不了解channel 的执行机制，channel是被Go Runtime来处理的。如果一个Goroutine在channel通讯上发生了阻塞，那没有任何理由让运行它的M也阻塞。这种情况下，G的状态会被设置为等待，M会继续执行别的Goroutine。当G重新变成可运行状态时，等待别的M去执行。
P的改进 原始Go的调度并没有P，仅有G、M以及Sched。当时系统只存在一个全局的G队列，通过Sched锁来进行并发控制。存在的问题有：
 调度的执行依赖全局的Sched锁，修改全局的M队列和G队列、或者其他全局的Sched字段都需要加锁 M的内存问题，执行的内存是跟M相关联的。但即使M并不执行G代码，它也会申请2MB的MCache空间，而这些空间只有M在执行G时才需要。同时，一个阻塞中的M也是不需要MCache的。 系统调用不够清晰，M在执行中会频繁阻塞和恢复，浪费CPU时间 M之间频繁的传递G，而不是选择自己执行它，这增加了系统的额外负载。每个M必须能够执行任何可运行的G，特别是刚刚创建了G的M。  P引入之后，从之前的M和Sched中抽取了部分字段，这样做带来了很多好处：
 MCache就被移动到了P中，而系统最多存在GOMAXPROCES个P，解决了不必要的内存浪费问题 G freelist被移动到P中，每个P都有了一个可运行的本地G队列。本地G队列缓解了全局Sched锁的问题。 当一个G被M创建，它被追加到对应P的本地队列末尾，以保证每个G都能被执行。  参考文章：</description>
    </item>
    
    <item>
      <title>Go 调度模型（一）</title>
      <link>/blog/2019/19-03-24-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%80/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-24-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%80/</guid>
      <description>想清楚了就去做，做的时候不要再回头想。
 OS Scheduler 在操作系统中保存了运行的进程列表，以及进程的运行状态(运行中、可运行及不可运行)。当进程运行时长超过了被分配的时间片(比如每10ms)，那么该进程会被系统抢占，然后在该CPU上执行别的进程。所以，OS的调度是抢占式的，可能抢占策略略有不同。
当进程被抢占时，需要保存该进程运行的上下文，并被重新放回到调度器，等待下一次被执行。
Golang Scheduler  Goroutine scheduler
The scheduler&amp;rsquo;s job is to distribute ready-to-run goroutines over worker threads.
 如图所示，OS层看到是只有Go进程以及运行的多个线程，而Goroutine本身是被Golang Runtime Scheduler调度管理的。
对OS而言，Go Binary是一个系统进程。内部Go Program对系统API的调度都是通过Runtime level解释来实现。Runtine记录了每个Goroutine的信息，在当前进程的线程池中按照顺序依次调度Goroutine。
Golang在Runtime内部实现了自己的调度，并不是基于时间切片的抢占式调度，而是基于Goroutines的协作式调度，目的就是要让Goroutine在OS-Thread中发挥出更多的并发优势。所以，在Runtime过程中，只有当正在运行的Goroutine被阻塞或者运行结束时，别的Goroutine才会被调度。常见的阻塞情形包括：
 阻塞的系统调用方式，比如文件或网络操作 垃圾自动回收  整体而言，Goroutine的数量大于Threads数量会更有优势，这样当其他Goroutine阻塞时，别的Goroutine就会被执行。
Goroutine G用于表示Goroutine及它所包含的栈和状态信息。Goroutine存在于Go Runtime的的虚拟空间，而非OS中。
// src/runtime/runtime2.go // 以下结构体精简了很多字段 type g struct { stack stack // offset known to runtime/cgo m *m // current m; offset known to arm liblink sched gobuf stktopsp uintptr // expected sp at top of stack, to check in traceback param unsafe.</description>
    </item>
    
    <item>
      <title>数据一致性（二）</title>
      <link>/blog/2019/19-03-16-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E4%BA%8C/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-16-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E4%BA%8C/</guid>
      <description>我们流连于事物的表象，满足浅尝辄止的片刻欢愉，却几乎从不久留。我们在人生的道路上争先恐后，却吝于用片刻思考目标和方向。
 概述 至今没有接触过MySQL多主的情况，即存在多个MySQL实例同时负责读写请求（抛弃只读库）。思考后认为：没有这么实现的技术难点在于：数据的一致性得不到保证。此外，还会涉及：
 MySQL采用自增主键索引的话，多主之间的数据同步简直是灾难。 内部锁机制的优势大打折扣，跨主库间的锁应该也是灾难级别的吧。  那么支持分布式的其他数据库又是怎么搞定这个问题的呢？比如Cassandra，多个节点之间可以同时处理读写请求，那么它是如何处理节点间数据同步以保证一致性的呢？
MySQL数据的一致性  We think this is an unacceptable burden to place ondevelopers and that consistency problems should be solved at the database level
 细细想想，MySQL自身实现的数据一致性也是相当复杂的。以Innodb举例，如果通过普通索引执行查询，首先获取到的仅仅是主键索引，后面还需要通过主键索引来获取完整的记录。查询如此，更新亦如此。
Master-Slave模式 通常情况下，MySQL部署都是一主多从。Master作为更新DB的入口，而Slave的数据通过binlog来进行同步。所以大胆想一想，有没有可能出现一种情况（假设id=1记录原始的name值为neojos）：
## 第一次同步数据 update s-1 set name=&amp;quot;neojos-1&amp;quot; where id = 1; ## 失败 update s-2 set name=&amp;quot;neojos-1&amp;quot; where id = 1; ## 成功 update s-3 set name=&amp;quot;neojos-1&amp;quot; where id = 1; ## 成功 ## 第二次同步数据 update s-1 set name=&amp;quot;neojos-2&amp;quot; where id = 1; ## 成功 update s-2 set name=&amp;quot;neojos-2&amp;quot; where id = 1; ## 失败 update s-3 set name=&amp;quot;neojos-2&amp;quot; where id = 1; ## 成功  最后，数据库从某一个时间点开始，Master和Salve的数据会变得不一致了当然不可能，MySQL在数据同步上做了非常硬的约束。包括Slave_IO_Running、Slave_SQL_Running以及Seconds_Behind_Master等。</description>
    </item>
    
    <item>
      <title>Mark-sweep GC</title>
      <link>/blog/2019/19-02-15-mark-sweep-gc/</link>
      <pubDate>Fri, 15 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-02-15-mark-sweep-gc/</guid>
      <description>把事做成的才是赢家，在口头上压倒对手，真的没有那么重要！
 Whirlwind introduce 当对象不再被引用时，对象不会立即被垃圾回收。也不存在任何子系统来专门记录使用的内存情况。
当系统没有内存空间时，触发GC处理。它首先会枚举所有的Root对象，然后递归的遍历根对象的引用关系。给遍历到的对象设置一个特殊标记，表明该对象是可达的，空间不能被回收。
当标记结束后，GC进入清洗阶段，任何在内存中没有被这次垃圾回收标记的对象都会被系统回收。
The algorithm 程序主要包含3个阶段：列举所有Root对象、标记起始于Root的对象引用、清除无效的对象。
void GC() { HaltAllProcessing(); ObjectCollection roots = GetRoots(); for(int i = 0; i &amp;lt; roots.Count(); ++i) Mark(roots[i]); Sweep(); }  Root Enumeration Root Enumeration会列举系统所有对象引用。运行系统需要为GC提供一种获取Root对象列表的机制。比如，在.NET中JIT维护了当前活跃的root对象，提供了获取根对象列表的API。
一个函数接受一个指针类型的参数，当方法返回时，jitter会识别出该参数不会再被使用，而将其从root中移除。
Mark 每个对象在创建时创建额外的空间，用于去mark这个对象，这个过程也是递归的
void Mark(Object* pObj) { if (!Marked(pObj)) // Marked returns the marked flag from object header { MarkBit(pObj); // Marks the flag in obj header // Get list of references that the current object has // and recursively mark them as well ObjectCollection children = pObj-&amp;gt;GetChildren(); for(int i = 0; i &amp;lt; children.</description>
    </item>
    
    <item>
      <title>垃圾回收之引用计数</title>
      <link>/blog/2019/19-02-01-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E4%B9%8B%E5%BC%95%E7%94%A8%E8%AE%A1%E6%95%B0/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-02-01-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E4%B9%8B%E5%BC%95%E7%94%A8%E8%AE%A1%E6%95%B0/</guid>
      <description>思来想去，决定总结一下垃圾回收机制。引用计数与我结缘最早，也比较简单、基础，遂决定从引用计数入手。
—— 不管人非笑，不管人毁谤，不管人荣辱，任他功夫有进有退，我只是这致良知的主宰不息，久久自然有得力处
 Reference Counting 对象在创建时保存一个自身被引用的计数，初始值为1。每次被新的变量引用，该值加1。相反，则减去1。当该值等于0时，占用空间被系统回收。
什么是对象呢？ var neojos int64 = 32 var ptrNeojos *int64 = &amp;amp;neojos  如上所示，我们创建了一个int64类型的object，命名为neojos。程序中对该object的操作都是通过使用neojos来实现的。而ptrNeojos其实又创建了一个*int64类型的object，但它的值保存的是neojos的地址。
对于ptrNeojos来说，它的生命周期跟普通变量的生命周期没有区别。唯一区别的是，当它生命周期结束后，ptrNeojos会被垃圾回收，而底层指向的object却不会。
如何计数呢？ Object * obj1 = new Object(); // RefCount(obj1) starts at 1 Object * obj2 = obj1; // RefCount(obj1) incremented to 2 as new reference is added Object * obj3 = new Object(); obj2-&amp;gt;SomeMethod(); obj2 = NULL; // RefCount(obj1) decremented to 1 as ref goes away obj1 = obj3; // RefCount(obj1) decremented to 0 and can be collected  obj1指向了一个匿名对象，为了方便，我们叫anonymousObj。上述代码展示了anonymousObj从创建到被垃圾回收的整个过程。垃圾回收对象的内存空间，上述过程中obj1对象的地址不会发生改变，只是底层引用的对象发生了变化。</description>
    </item>
    
    <item>
      <title>gRPC入门</title>
      <link>/blog/2019/19-01-26-grpc%E5%85%A5%E9%97%A8/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-01-26-grpc%E5%85%A5%E9%97%A8/</guid>
      <description>时间飞逝 如一名携带信息的邮差 但那只不过是我们的比喻 人物是杜撰的 匆忙是假装的 携带的也不是人的讯息
 为什么使用grpc 主要包括以下两点原因：
 protocl buffer一种高效的序列化结构。 支持http 2.0标准化协议。  很对人经常拿thrift跟grpc比较，现在先不发表任何看法，后续会深入thrift进行介绍。
http/2  HTTP/2 enables a more efficient use of network resources and a reduced perception of latency by introducing header field compression and allowing multiple concurrent exchanges on the same connection… Specifically, it allows interleaving of request and response messages on the same connection and uses an efficient coding for HTTP header fields.</description>
    </item>
    
    <item>
      <title>Float的基本介绍</title>
      <link>/blog/2019/19-01-16-float%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-01-16-float%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/</guid>
      <description>关于浮点数，为什么它生来就可能存在误差？带着好奇查阅了一些介绍，然后做了简单汇总。这只是一段知识的开始，后续还会继续完善。
—— 荡荡上帝，下民之辟。疾威上帝，其命多辟。天生烝民，其命匪谌。靡不有初，鲜克有终。
 Floating-point represent 浮点数在计算机中是如何表示的？因为它有小数点，那么小数点后面的数，该如何用二进制来表示呢？我们都知道，浮点数本身就存在误差，在工作中，你所使用的float都是一个近似数。这又是什么原因导致的呢？
1. Fixed-point fixed-point 是将bit位拆分成固定的两部分：小数点前的部分和小数点后的部分。拿32 bit的fixed-point表示举例，可以将其中的24 bit用于表示整数部分，剩余的8 bit表示小数部分。
假如要表示1.625，我们可以将小数点后面的第一个bit表示$\frac12$，第二个bit表示1/4，第三个1/8一直到最后一个1/256。最后的表示就是00000000 00000000 00000001 10100000。这样其实也好理解，因为小数点前是从$2^0$开始向左成倍递增，小数点后从$2^{-1}$开始向右递减。
因为小数点后面的部分始终小于1，上面这种表达方式能表达的最大数是255/256。再比这个数小，这种结构就无法表示了。
Floating-point basics 根据上面的思路，我们用二进制表达一下5.5这个十进制数，转化后是$101.1_{(2)}$。继续转换成二进制科学计数法的形式：$1.011_{(2)} * 2^2$。在转换的二进制科学计数法过程中，我们将小数点向左移了2位。就跟转换十进制的效果一样：$101.1_{(10)}$ 的科学计数形式为$1.011 * 10^2$。
对于二进制科学计数法表达的5.5，我们将其拆分成2部分，1.011是一部分，我们称为mantissa。指数2是另一部分，称为exponent。下面我们要将$1.011_{(2)} * 2^2$ 映射到计算机存储的8 bit结构上。
我们用第一个bit来表示正负符号，1表示负数，0表示正数。紧接着的4 bit用来表示exponent + 7后的值。 4 bit最大可以表示到15，这也就意味着当前的exponent不能超过8，不能低于-7。最后的3 bit用于存储mantissa的小数部分。你可能有疑问，它的整数部分怎么办呢？这里我们约定整数部分都调整成1，这样就可以节省1 bit了。举个例子，如果要表示的十进制数是0.5，那么最后的二进制数不是$0.1_{(2)}$，而是$1.0 * 2^{-1}$。最后表示的结果就是：0 1001 011。
再来一个decode的例子，即将0 0101 100还原回原始值。根据之前的描述0101表示的十进制是5，所以exponent = -2，表示回二进制科学计数法的结果：$1.100 * 2^{-2} = 0.011_{(2)}$。我们继续转换成真实精度的数：0.375。
最后可以看在，如果mantissa的长度超过3 bit表示的范围，那么数据的存储就会丢失精度，结果就是一个近似值了。
1. Representable numbers 继续按照上面的思路，现在8 bit的浮点表示能表示的数值区间更大。
要表示最小正数的话，sign置为0，接下来的4 bits置为0000，最后的mantissa也置为000。那么最终的表示结果就是：$1.000_{(2)} * 2^{-7} = 2^{-7} ≈ 0.0079_{(10)}$。</description>
    </item>
    
    <item>
      <title>探讨分布式ID生成系统</title>
      <link>/blog/2019/19-01-11-%E6%8E%A2%E8%AE%A8%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-01-11-%E6%8E%A2%E8%AE%A8%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F/</guid>
      <description>全称Universally Unique Identifier，UUID占128bit，也就是16个英文字符的长度（16byte），需要强调的是，它的生成无需中心处理程序。
UUID被用来标识URN(Uniform Resource Names)，对于Transaction ID以及其他需要唯一标志的场景都可以使用它。
UUID是空间和时间上的唯一标识，它长度固定，内部中包含时间信息。如果服务器时间存在不同步的情况，UUID可能会出现重复。
UUID构成 基本格式，由6部分组成：
time-low - time-mide - time-high-and-version - clock-seq-and-reserved &amp;amp; clock-seq-low - node  一个URN示例：f81d4fae-7dec-11d0-a765-00a0c91e6bf6。
因为UUID占128bit，16进制数占4bit，所以转换成16进制0-f的字符串总共有32位。组成的各个部分具体由几位16进制表示，请查阅 Namespace Registration Template
因为UUID太长且无序，导致其不适合做MySQL的主键索引。而且MySQL自带的auto-increment功能，选择bigint的话也只占用64bit。
 All indexes other than the clustered index are known as secondary indexes. In InnoDB, each record in a secondary index contains the primary key columns for the row, as well as the columns specified for the secondary index. InnoDB uses this primary key value to search for the row in the clustered index.</description>
    </item>
    
    <item>
      <title>Gin使用</title>
      <link>/blog/2019/19-01-06-gin%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-01-06-gin%E4%BD%BF%E7%94%A8/</guid>
      <description>Gin对net/http包做了封装，支持路由、中间件等特性，极大的方便对Http Server的开发。文章通过一个Test例子，来简要介绍。对于特别基础的部分，请阅读参考文章。
接口测试 Go中testing包为程序自测提供了便利。可以查阅之前写的博客Go test基础用法，对其内容，我还是挺满意的。
使用Postman 对于接口测试，很多情况都在使用Postman这样的工具。首先在本地启动服务，然后在Postman中配置请求的地址和参数、执行请求、查看结果。
这种方式唯一让人不满意的地方在于：每次修改都需要重启服务。跟直接执行一次Test相比，明显多了一步。
使用Test 测试基类 下面的代码作为接口测试的基类。
TestMain中，我们为所有的测试用例指定通用的配置。之后在执行其他Test前，都会先执行TestMain中的代码。有效的避免了代码冗余。
getRouter方法用于返回一个gin的实例。我们将服务的路由重新在Test中指定，并设置了中间件。
testHttpResponse是我们请求处理的核心代码，发送请求，并保存响应到w中
//common_test.go func TestMain(m *testing.M) { //声明执行Test前的操作 gin.SetMode(gin.TestMode) testutils.NewTestApp(&amp;quot;../conf.test.toml&amp;quot;) flag.Parse() os.Exit(m.Run()) } //设置路由，获取框架Gin的实例 func getRouter() *gin.Engine { router := gin.Default() //配置路由，这是我项目中的自定义配置 router.Use(middleware.HeaderProcess()) RouteAPI(router) return router } //统一处理请求返回结果 func testHttpResponse(t *testing.T, r *gin.Engine, req *http.Request, f func(w *httptest.ResponseRecorder) error) { w := httptest.NewRecorder() r.ServeHTTP(w, req) if err := f(w); err != nil { t.Fatal(err) } }  测试用例 下面是具体的测试用例。我们构造了一个Json数据格式的POST请求，然后通过调用testHttpResponse方法来读取接口的响应数据。</description>
    </item>
    
    <item>
      <title>How to use godog</title>
      <link>/blog/2018/12-29-how-to-use-godog/</link>
      <pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/12-29-how-to-use-godog/</guid>
      <description>首先访问Git的地址：Godog，它也是用来做Go Test一样的事情，只是换了一种形式。引入了一个概念：BDD。通俗的讲，就是虚拟现实场景，完成一个业务的测试。
Godog了解 首先介绍Godog是用来干什么的，我也是根据版本库提供的README来解释的，建议大家自己去看看。首先，我们要定义一个场景：feature。这里我们创建一个文件夹feature，专门用来存储这类文件。然后创建一个文件：godogs.feature。文件内容如下：
# file: $GOPATH/src/godogs/features/godogs.feature Feature: 购买红酒 这里是一堆对这个Feature的描述 描述的继续... Scenario: 买一瓶红酒 Given Neojos Has 5 coins When I buy Red wine Then should be 1 remaining  在控制台执行godog时，控制台会输出默认建议的代码。输出如下：
You can implement step definitions for undefined steps with these snippets: func neojosHasCoins(arg1 int) error { return godog.ErrPending } func iBuyRedWine() error { return godog.ErrPending } func shouldBeRemaining(arg1 int) error { return godog.ErrPending } func FeatureContext(s *godog.Suite) { s.Step(`^Neojos Has (\d+) coins$`, neojosHasCoins) s.</description>
    </item>
    
    <item>
      <title>Go net 超时处理</title>
      <link>/blog/2018/10-10-go-net-%E8%B6%85%E6%97%B6%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 10 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/10-10-go-net-%E8%B6%85%E6%97%B6%E5%A4%84%E7%90%86/</guid>
      <description>序 这篇文章详细介绍了，net/http包中对应HTTP的各个阶段，如何使用timeout来进行读/写超时控制以及服务端和客户端支持设置的timeout类型。本质上，这些timeout都是代码层面对各个函数设置的处理时间。比如，读取客户端读取请求头、读取响应体的时间，本质上都是响应函数的超时时间。
作者强烈不建议，在工作中使用net/http包上层封装的默认方法（没有明确设置timeout），很容易出现系统文件套接字被耗尽等令人悲伤的情况。比如：
// 相信工作中也不会出现这样的代码 func main() { http.ListenAndServe(&amp;quot;127.0.0.1:3900&amp;quot;, nil) }  正文 在使用Go开发HTTP Server或client的过程中，指定timeout很常见，但也很容易犯错。timeout错误一般还不容易被发现，可能只有当系统出现请求超时、服务挂起时，错误才被严肃暴露出来。
HTTP是一个复杂的多阶段协议，所以也不存在一个timeout值适用于所有场景。想一下StreamingEndpoint、JSON API、 Comet， 很多情况下，默认值根本不是我们所需要的值。
这篇博客中，我会对HTTP请求的各个阶段进行拆分，列举可能需要设置的timeout值。然后从客户端和服务端的角度，分析它们设置timeout的不同方式。
SetDeadline 首先，你需要知道Go所暴露出来的，用于实现timeout的方法：Deadline。
timeout本身通过 net.Conn包中的Set[Read|Write]Deadline(time.Time)方法来控制。Deadline是一个绝对的时间点，当连接的I/O操作超过这个时间点而没有完成时，便会因为超时失败。
Deadlines不同于timeouts. 对一个连接而言，设置Deadline之后，除非你重新调用SetDeadline，否则这个Deadline不会变化。前面也提了，Deadline是一个绝对的时间点。因此，如果要通过SetDeadline来设置timeout，就不得不在每次执行Read/Write前重新调用它。
你可能并不想直接调用SetDeadline方法，而是选择 net/http提供的更上层的方法。但你要时刻记住：所有timeout操作都是通过设置Deadline实现的。每次调用，它们并不会去重置的deadline。
Server Timeouts 关于服务端超时，这篇帖子So you want to expose Go on the Internet也介绍了很多信息，特别是关于HTTP/2和Go 1.7 bugs的部分.
对于服务端而言，指定timeout至关重要。否则，一些请求很慢或消失的客户端很可能导致系统文件描述符泄漏，最终服务端报错：
http: Accept error: accept tcp [::]:80: accept4: too many open files; retrying in 5ms  在创建http.Server的时候，可以通过ReadTimeout和WriteTimeout来设置超时。你需要明确的声明它们：
srv := &amp;amp;http.Server{ ReadTimeout: 5 * time.Second, WriteTimeout: 10 * time.Second, } log.</description>
    </item>
    
  </channel>
</rss>