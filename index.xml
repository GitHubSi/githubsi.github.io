<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>蜗牛保持乐观才会爬的更远 on 道法自然</title>
    <link>/</link>
    <description>Recent content in 蜗牛保持乐观才会爬的更远 on 道法自然</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Using context cancellation in Go</title>
      <link>/blog/2019/19-06-23-using-context-cancellation-in-go/</link>
      <pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-06-23-using-context-cancellation-in-go/</guid>
      <description>文章介绍最近工作中遇到的一个问题，其中50%以上的内容都是Go的源代码。剩下部分是自己的理解，如果有理解错误或探讨的地方，希望大家指正。
问题：针对同一个接口请求，绝大多数都可以正常处理，但却有零星的几请求老是处理失败，错误信息返回 context canceled。重放失败的请求，错误必现。
根据返回的错误信息，再结合我们工程中使用的golang.org/x/net/context/ctxhttp包。猜测可能是在请求处理过程中，异常调用了context 包的CancelFunc方法。同时，我们对比了失败请求和成功请求的区别，发现失败请求的Response.Body数据量非常大。
之后在Google上找到了问题的原因，还真是很容易被忽略，这里是文章的链接：Context cancellation flake。为了解决未来点进去404的悲剧，本文截取了其中的代码&amp;hellip;
Code 代码核心逻辑：向某个地址发送Get请求，并打印响应内容。其中函数fetch用于发送请求，readBody用于读取响应。例子中处理请求的逻辑结构，跟我们项目中的完全一致。
fetch方法中使用了默认的http.DefaultClient作为http Client，而它自身是一个“零值”，并没有指定请求的超时时间。所以，例子中又通过context.WithTimeout对超时时间进行了设置。
代码中使用context.WithTimeout来取消请求，存在两种可能情况。第一种，处理的时间超过了指定的超时时间，程序返回deadlineExceededError错误，错误描述context deadline exceeded。另一种是手动调用CancelFunc方法取消执行，返回Canceled错误，描述信息context canceled。
在fetch代码的处理逻辑中，当程序返回http.Response时，会执行cancel()方法，用于标记请求被取消。如果在readBody没读取完返回的数据之前，context被cancel掉了，就会返回context canceled错误。侧面也反映了，关闭Context.Done()与读取http.Response是一个时间赛跑的过程…..
package main import ( &amp;quot;context&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;log&amp;quot; &amp;quot;net/http&amp;quot; &amp;quot;time&amp;quot; &amp;quot;golang.org/x/net/context/ctxhttp&amp;quot; ) func main() { req, err := http.NewRequest(&amp;quot;GET&amp;quot;, &amp;quot;https://swapi.co/api/people/1&amp;quot;, nil) if err != nil { log.Fatal(err) } resp, err := fetch(req) if err != nil { log.Fatal(err) } log.Print(readBody(resp)) } func fetch(req *http.Request) (*http.Response, error) { ctx, cancel := context.WithTimeout(context.Background(), 5*time.</description>
    </item>
    
    <item>
      <title>里氏替换&amp;开放关闭</title>
      <link>/blog/2019/2019-06-06-%E9%87%8C%E6%B0%8F%E6%9B%BF%E6%8D%A2%E5%BC%80%E6%94%BE%E5%85%B3%E9%97%AD/</link>
      <pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/2019-06-06-%E9%87%8C%E6%B0%8F%E6%9B%BF%E6%8D%A2%E5%BC%80%E6%94%BE%E5%85%B3%E9%97%AD/</guid>
      <description>里氏替换  Let Φ(x) be a property provable about objects x of type T. Then Φ(y) should be true for objects y of type S where S is a subtype of T
 本质上就是类设计中的继承，它强调类所实现的行为。参数的类型指定为基类，而实际传参的时候使用具体的子类。每次扩展新的行为，都通过创建一个新的子类来实现。在Go的设计中，继承是通过接口类型来实现的。
开放关闭  Software entities (classes, modules, function, etc) should be open for extension, but closed for modification.
A class is closed, since it may be complied, stored in a library, baselined and used by client classes.</description>
    </item>
    
    <item>
      <title>database package</title>
      <link>/blog/2019/19-06-03-database-package/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-06-03-database-package/</guid>
      <description>清除无效连接 在database库下清除过期连接时，使用了如下的代码逻辑。其中freeConn是空闲连接池，d是连接可被重复使用的最长时间，nowFunc返回的是当前时间。最新生成的连接在freeConn的末尾，而清除的过程则是使用最新的、次新的连接依次替换最早过期的、次早过期的连接。
在for循环中直接使用len来获取总计数，在循环体内部将freeConn末尾的值替换首部的值，并将freeConn的len长度减去1。最后还做了i—操作，重复校验了一次。
expiredSince := nowFunc().Add(-d) var closing []*driverConn for i := 0; i &amp;lt; len(db.freeConn); i++ { c := db.freeConn[i] if c.createdAt.Before(expiredSince) { closing = append(closing, c) last := len(db.freeConn) - 1 db.freeConn[i] = db.freeConn[last] db.freeConn[last] = nil db.freeConn = db.freeConn[:last] i-- } }  参考点 slice中首部和尾部数据的交换过程，以及每次通过i--达到的重复校验的思路。
间隔执行 清除无效连接的工作是由一个goroutine在后台完成的，下面是截取的部分代码。for循环内部是处理连接的具体实现。每次清除操作完成后，通过Reset来重置Timer。
func (db *DB) connectionCleaner(d time.Duration) { const minInterval = time.Second if d &amp;lt; minInterval { d = minInterval } t := time.</description>
    </item>
    
    <item>
      <title>mongo中ObjectId</title>
      <link>/blog/2019/2019-06-11-mongo%E4%B8%ADobjectid/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/2019-06-11-mongo%E4%B8%ADobjectid/</guid>
      <description>ObjectId在mongo中是自动生成的_id字段，充当数据表的主键ID。按照_id排序基本上等于按照记录的创建时间排序，但还是必须注意：_id并不是严格单调递增的，前4个byte的也只是精确到了秒级，同一秒内的_id并不能保证有序。
 ObjectIds are small, likely unique, faster to generate, and ordered. ObjectId values consist of 12 bytes, where the first four bytes are a timestamp that reflect the objectId&amp;rsquo;s creation. Specifically - a 4-byte value representing the seconds since the Unix epoch - a 5-byte random value - a 3 byte counter, starting with a random value
 排序 使用github.com/globalsign/mgo的翻页逻辑：
func (detail *CounterDetailMapper) GetDetailsByDesc(ctx context.IContext, objectId string, size int, startPoint string, data *[]models.</description>
    </item>
    
    <item>
      <title>数据一致性（三）</title>
      <link>/blog/2019/19-05-18-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/</link>
      <pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-05-18-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/</guid>
      <description>一个人的时候多一点努力，才能让自己的爱情，少一点条件，多一点纯粹
 Quorum 在有冗余数据的分布式存储系统中，数据会在不同的机器上存放多份拷贝。但是同一时刻一个对象的多份拷贝只能用于读或者用于写。该算法可以保证同一份数据对象的多份拷贝不会被超过两个访问对象读写。
分布式系统中每一份数据拷贝对象被赋予一票。每一个读操作获得的票数必须大于最小读票数 $V_r$，每个写操作获得的票数必须大于最小写票数 $V_w$才能读或者写。如果系统有V票，那么最小读写票数应该满足如下限制：
 $V_r$ + $V_j$ &amp;gt; V $V_w$ &amp;gt; V/2  第一条规则保证了一个数据不会被同时读写。当一个写操作请求过来的时候，它必须要获得$V_w$个冗余拷贝的许可。而剩下的数量是V-$V_w$不够$V_r$，因此不能再有读请求过来。同理，当读请求已经获得了$V_r$个冗余拷贝的许可时，写请求就无法获得许可了。
第二条规则保证了数据的串行化修改。一份数据的冗余拷贝不可能同时被两个写请求修改。
注：上述内容来自于维基百科：Quorum
上述算法的思想来自于鸽巢原理，也是摘自维基百科：鸽巢原理
假如有n个笼子和n+1个鸽子，所有的鸽子都被关在鸽笼里，那么至少有一个笼子有至少2只鸽子。
集合论的表述如下：若A是n+1个元素，B是n个元素，则不存在从A到B的单射。这里也体现了Lamport的思想。
$V_r$和$V_w$的设置会直接影响系统的性能、扩展性和一致性。比如将$V_r$或者$V_w$设置为1，会体现出系统对读和写的不同侧重。
Two-phase commit 在分布式系统中，虽然每个节点内部的事务可以保证，但却无法保证其他所有节点的操作都成功或失败。当一个事物跨多个节点时，为了保证数据的一致性，需要引入一个协调者来统一掌控所有节点的操作。
第一阶段提交请求，参与者节点执行事务的操作，并将Undo信息和Redo信息写入日志。第二阶段参与者正式完成操作，并释放整个事务期间占用的资源。
 协调者 参与者 QUERY TO COMMIT --------------------------------&amp;gt; VOTE YES/NO prepare*/abort* &amp;lt;------------------------------- commit*/abort* COMMIT/ROLLBACK --------------------------------&amp;gt; ACKNOWLEDGMENT commit*/abort* &amp;lt;-------------------------------- end  注：内容来自于维基百科：二阶段提交
Optimistic Lock 在数据库低的隔离级别中，乐观锁也可以保证数据的一致性。区别于Pessimistic Lock，实际上Optimistic Lock并没有对数据库上锁，性能要优Pessimistic Lock。它的操作流程：
 更新一个新值 校验在更新时数据是否发生改变  Optimistic Lock大多数基于数据版本记录实现，或者是不可逆的状态字段。实际工作中，Pessimistic和Optimistic需要根据实际需要来使用，比如，如果要对多个数据表进行更新操作，Optimistic Lock就有点力不从心了。
Summary 当意见不一致的时候，大家可以选择投票。当票数一致的时候，大家还是会有很多策略来执行最优选择。计算机的世界也一样。</description>
    </item>
    
    <item>
      <title>mongo EOF（二）</title>
      <link>/blog/2019/19-05-11-mongo-eof%E4%BA%8C/</link>
      <pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-05-11-mongo-eof%E4%BA%8C/</guid>
      <description>任何事情的成功都需要掐准时间
 上一节mongo EOF中，关于容器的配置，只是粗略的使用了Docker-Compose-MongoDB-Replica-Set项目提供好的docker-compose.yml文件。在使用过程中，我发现这个文件本身一些不如意的地方。首先，services中的creator服务，entrypoint指令太长了，不美；其次，所有的service都没有给容器外部暴露端口，导致外部无法访问容器；再次，直接使用mongo repliSet的连接串进行访问，无法正常访问mongo服务。
在上一篇文章的基础上，这篇文章对docker-compse.yml做了一些调整，并且也包含了docker使用的入门介绍。更新后的docker-compose.yml请访问githubsi。
depends_on  However, for startup Compose does not wait until a container is “ready” (whatever that means for your particular application) - only until it’s running. There’s a good reason for this.
 在creator service中使用了该指令， 但是，实际中creator不会等到mongo1、mongo2、mongo3容器ready后再启动，而是等到它们启动就开始启动。这也是我在setup脚本中执行sleep操作的原因。
creator: build: context: . dockerfile: dockerfile entrypoint: [&amp;quot;/data/conf/setup.sh&amp;quot;] depends_on: - mongo1 - mongo2 - mongo3  entrypoint  Entrypoint sets the command and parameters that will be executed first when a container is run.</description>
    </item>
    
    <item>
      <title>mongo EOF</title>
      <link>/blog/2019/19-04-27-mongo-eof/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-04-27-mongo-eof/</guid>
      <description>很多事情仅仅的是严肃的提出问题都感觉很难，更何况还得要先发现它。
 Question 描述 项目中使用：github.com/globalsign/mgo这个库，在一次主从切换之后，mongo后续的操作都失败了, 错误信息输出：EOF。
引用网上遇到同样问题的其他描述：
 The problem I have is, when the connection to the mongodb server fails (the server drops the connection sometimes or mongodb server fails), then my pointer to the session variable doesn&#39;t work anymore. Even if the internet connection comes back, mgo driver doesn&#39;t reconnect anymore, instead of this I get the error (when I do Find().One() method call): &amp;quot;EOF&amp;quot;
 解决思路   Call Refresh on the session, which makes it discard (or put back in the pool, if the connection is good) the connection it&amp;rsquo;s holding, and pick a new one when necessary.</description>
    </item>
    
    <item>
      <title>Kafka中消息分配策略</title>
      <link>/blog/2019/19-04-24-kafka%E4%B8%AD%E6%B6%88%E6%81%AF%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-04-24-kafka%E4%B8%AD%E6%B6%88%E6%81%AF%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/</guid>
      <description>非淡泊无以明，非宁静无以致远
 Question 关于kafka中partation和consumer的是如何执行分配的。今早骑自行车的时候突然想起这个问题。它是怎么分配的，我记得我看到好几次相关的介绍文章，现在却想不起来？
sense 很多时候，我们在看完一篇技术文档时，感觉对其中的内容都了解了，其实不然。这也是所谓的被动输入和主动输出的区别所在。相比主动输出而言，被动输入缺少了深层思考的态度。得到一些老师的课里就谈到过主动输出的重要性。
很多概念都已经记不起来了，文章的内容可能也不够准确，但重在思考和想法的过程：
Thinking partaion是数据的存储单位，一个topic至少存在一个partation。consumer使用主动拉数据的方式来消费消息，这就是所有已知概念。那么partation和consumer是如何分配的呢？这里我们假设消息数据总量是一样的。
首先，应该存在一个类似LVS的负载均衡器，因为当consumer增加或者减少时，对应的分配策略也需要做相应的调整。在consumer注册成为消费者时，提交的信息中有consumer Group的概念，这非常好理解，即同一个组的consumer会独立处理一份数据。同时，因为有了Group，我们在管理的时候就会简单很多。当然，所有的讨论都是以一个Group为前提的。
1:1 从最简单的开始考虑，假设现在partation和consumer的数量比例是1:1，那就没有那么多事了，简单明了。
1:2 假设是1:2呢？问题来了，好比现在只有一个数据，但有AB两个线程都要读它，且A和B中有且只有一个能读到。这种情况在计算机中相当普遍，通用的处理方式就是加锁，以保证数据只被其中一个线程处理。但加锁就意味着性能开销，尤其是高并发的场景。
继续按照这个思路来考虑，两个consumer同时向一个partation发起请求，需要一个全局锁来控制每个消息只能返回给请求中的一个。相比较1:1的方式，这样的性能肯定是提高了不少。
2:1 假设是2:1呢？这个也很直观，两份数据，但只有一个消费者，那肯定都需要这一个消费者来处理了，就跟CPU任务调度还有些类似。
如果跟1:2的分配策略比较呢？这里通过将原来的一份数据平均分成两份，去掉了1:2中加锁的开销，但只有一个connsumer来消费数据。好比是单核CPU任意处理数据跟双核CPU加锁处理数据。
2:2 假设是2:2呢，我们可以平均分配partation和consumer，而且这样系统性能却得到最大提升，不仅去掉了锁的开销，还有两个线程来同时消费。
结论 通过上面的对比，我们可以清楚的发现，通过调整partation和consumer的数量就可以将系统性能达到最大，完全不需要引入锁机制。这样想的话，分配就很简单了。</description>
    </item>
    
    <item>
      <title>Go Module（一）</title>
      <link>/blog/2019/19-04-21-go-module%E4%B8%80/</link>
      <pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-04-21-go-module%E4%B8%80/</guid>
      <description>这世上太多的人，宁愿吃生活苦，也不愿吃自律的苦。大概是因为生活的苦，躺着就来了，而自律的苦，得自己去找。但只有吃得下自律的苦，才有成功的自由，没有一种成功是走得了捷径的，通向真正成功的唯一道路只有自律。越成功，越自律。越自律，越成功。
 GO111MODULE Go 1.1包含了对Go Modules的预支持，包括调整后的go get命令。后续版本总GOPATH和老的go get可能会被官方移除。
在Go Modules中支持了一个临时环境变量：GO111MODULE，可以赋值为off、on或auto。
 值为off，表示不支持Go Modules模式，Go仍然在vendor和GoPATH路径下查找依赖； 值为on，表示当前明确使用Go Modules，Go不再去GOPATH下查找任何依赖； 值为auto或未设置，表示是否启用Go Modules依赖当前的目录情况，当编译的项目在GoPATH/src之外，或者当前目录或子目录本身包含go.mod文件，则启用Go Modules模式。  Defining a module module通过源码根目录下的go.mod文件来定义。根路径下的module是项目依赖包的集合，但会排除子目录的go.mod文件。
下面是go mod文件模版：
module example.com/m require ( golang.org/x/text v0.3.0 gopkg.in/yaml.v2 v2.1.0 )  要开始使用go mod，仅需要在项目下执行go mod init命令创建go.mod文件即可。
go mod init example.com/m  Modules and vendoring 当使用module时，Go命令会完全忽略vendor目录。为了跟之前Go的依赖管理相兼容，我们可以使用go mod vendor 创建vendor目录来存储编译代码的依赖包。如果在编译的时候要使用vendor中的依赖包，需要使用go build -mod=vendor命令。
go mod vendor go build -mod=vendor  Go Get 首先，go get解析需要新增哪些依赖。可以通过在包名后添加@version或者@branch等方式来取代命令的默认更新行为。如果后缀指定为@none，则表明该依赖应该被移除。
其次，go get会下载、编译、安装指定的包。包的安装模式也是被允许的，比如使用go get golang.org/x/perf/cmd/..来更新cmd下的所有子包。</description>
    </item>
    
    <item>
      <title>Go 调度模型（三）</title>
      <link>/blog/2019/19-04-14-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%89/</link>
      <pubDate>Sun, 14 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-04-14-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%89/</guid>
      <description>  别抱怨，也别自怜，所有的现状都是你自己选择的
 前面的章节中，我们介绍了操作系统的调度模型：N:1、1:1、M:N。而Go采用了更高效的方式M:N。从进程的角度来说，线程是最小的调度单元。而Go的模型下，可以把P作为最小单元的调度单元，即在单个线程上运行的Go代码。
执行模型 下图展示了Go的最小调度单元模型。其中的有两个线程，各维持一个P对象，而且正在执行一个G代码。为了运行G，M必须首先持有P对象。图中灰色的G表示还没有被执行，等待被调度。它们被组织在P的一个runqueues的队列中，当M创建新的Goroutine时，对应的G就会被追加到该队列的末尾。
阻塞模型 为什么要引入P结构，直接将runqueues放在M中，不就可以摆脱P了吗？当然不行，它存在的意义还在于：当M因为其他原因被阻塞时，我们需要将runqueues中的G交给别的M来继续处理。因为线程不可能既执行代码，又阻塞在系统上。
如上图所示，当M0阻塞在系统调用上时，它会放弃自己的P，以保证M1可以继续执行其他G。当M0系统调用返回时，M0为了继续执行G0，就必须尝试重新获取P对象。正常的执行流程是：它尝试去偷其它线程的P，如果不行，就将G0放到全局的runqueues中，之后进入休眠。
当P本地的runqueues运行完之后，M会去全局队列取G来执行。同时，全局队列的G也会被间歇性检查，否则里面的G可能永远都得不到执行了。
偷G模型 当runqueues分布不均衡时，可能存在其中一个M执行完了本地的P，而其他P的本地队列还有很多G等待被执行。如图所示，为了去继续运行Go代码，P1首先会尝试去全局队列获取。如果全局队列没有，那么它就会随机从别的P去偷一半回来。这样做也是用来保证所有线程都一直有工作可以做。
参考文章：
 The Go scheduler  </description>
    </item>
    
    <item>
      <title>设计模式-适配器模式</title>
      <link>/blog/2019/19-04-07-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-04-07-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F/</guid>
      <description>当你眼里只有赚钱的时候，你就永远无法把事情本身给做好，收获的也会很有限，最后可能还赚不到钱。- From Myself
 下面的代码是github.com/gin-gonic/gin/binding中获取Binding实例的逻辑。我在想：这段代码体现的是什么设计模式呢？写法上肯定是工厂模式，因为它基于不同的contentType创建返回具体的实例。但从宏观上来看，它算不算一个适配器呢？
func Default(method, contentType string) Binding { if method == &amp;quot;GET&amp;quot; { return Form } switch contentType { case MIMEJSON: return JSON case MIMEXML, MIMEXML2: return XML case MIMEPROTOBUF: return ProtoBuf case MIMEMSGPACK, MIMEMSGPACK2: return MsgPack default: //case MIMEPOSTForm, MIMEMultipartPOSTForm: return Form } }  Adapter Pattern 适配器模式不仅仅局限于代码设计，在现实世界中也经常会看到。比如苹果手机的转接线，将新的、方形的Lighting接口适配到旧的、圆孔的耳机上。
Adapter Pattern主要被用来适配两个不兼容的接口，给两个独立或者不兼容的类提供一个兼容模式，而不需要修改两者内部的具体实现。Adapter Pattern可以是一个独立的新对象或者新方法，在设计中扮演一个桥梁的作用，或者是对不相互兼容的数据格式进行转换。又或者是重用系统老的既存类，来提供新的功能。
Purpose 适配器主要通过转换数据格式，组合、引用不兼容的对象，最终实现我们期盼的功能。
 老系统到新系统的业务迁移。新老系统首先在接收数据的格式上不尽相同，其次新系统可能也需要调用老系统的内部实现。 重新对对象进行封装，用来提供业务期望的新功能。或者让不兼容的对象可以一起工作。  Design Pattern Diagram  Target：Client端调用的新接口。 Adapter：将Adaptee适配到Target，实现两者间的转换。 Adaptee：需要去适配的既存接口  Implementation 常见的适配实现主要有两种方式：</description>
    </item>
    
    <item>
      <title>Go 调度模型（二）</title>
      <link>/blog/2019/19-03-30-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%BA%8C/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-30-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%BA%8C/</guid>
      <description>真的猛士，敢于直面惨淡的人生，敢于正视淋漓的鲜血。这是怎样的哀痛者和幸福者？然而造化又常常为庸人设计，以时间的流驶，来洗涤旧迹，仅使留下淡红的血色和微漠的悲哀。在这淡红的血色和微漠的悲哀中，又给人暂得偷生，维持着这似人非人的世界。我不知道这样的世界何时是一个尽头！
 有一种感觉，Go调度模型可能还得再来几篇博客，才能真正读出感觉来。现在还是一个门外汉。越是深入了解，越觉得知之甚少，后背发凉。
OS调度结构 Go Runtime实现了自己的调度策略，从OS调度结构的演变来看，调度思想都是相似的。当然，我始终觉得设计背后的思想才是整个系统的核心。思想从无到有、从中心化到去中心化、从单任务到并行多任务，当然这肯定不是调度策略的专利，在很多场景都有这种思想的体现，比如分布式设计。
那为什么我还是没有一眼看出来呢，可能是阅历太少，思考的深度不够，积极接受现状，懒于了解历史吧。不过，越是去了解，就会发现不会的太多，思维发散的太广，难以为继。
single scheduler 存在一个全局的任务队列和一个全局的调度器，因为整个过程不需要加锁，所以单核吞吐量很高，但无法充分利用多核资源。
有点类似于：给一个数据表中的所有用户PUSH消息，虽然我们有10台服务器，但我们只在其中一台服务上执行该任务。优点是设计开发简单，缺点是没有充分利用资源，效率不高。
multi scheduler with global queue 多个调度器共享一个全局的任务队列，该模型需要频繁的对任务队列进行加锁，并发性能存在明显的瓶颈。这同时让我想起了Go的并发问题中介绍的例子，加锁保证了计算的正确性，但却牺牲了效率。当然，这不仅仅是调度系统会面临的问题，比如本地缓存BigCache也遇到了同样的问题。
还接着上面的例子说，当在多台服务器上都启动Task执行任务时，为了避免同一个用户不被重复PUSH多次，势必也面临着对单条记录加锁的问题。
multi scheduler with local queue 给每个调度器分配一个本地的任务队列，这样调度器就可以无锁的操作本地任务队列，显著减少锁竞争，提高多核下的调度效率。同时还要保证让各个调度器随时都有事情可做，所以也存在一些任务迁移或者任务窃取的方案。到了这里，我们就已经看到了Go Scheduler的雏形。其实思维很简单，将全局的任务队列划分成多个小的任务队列，各个调度器处理自己的任务队列，跟Database Sharding异曲同工。
继续上面的例子，我们只需要给各个服务器分配用户表的小块数据，当Task执行完分配的数据块后，再去请求新的数据块就可以了。
GPM 现在提到Go Scheduler就会直接想到GPM，但之前的设计里Scalable Go Scheduler Design Doc其实并不存在P。P的引入直接将调度模型由multi scheduler with global queue跨越到multi scheduler with local queue。
 每个Goroutine需要对应一个G结构体，而G保存了当前Goroutine的运行堆栈和状态信息。Goroutine通过G中保存的信息可以执行或恢复执行。 P是专门被引入用来优化原始Go调度系统所抽象的逻辑对象，操作系统并不知道P的存在。对M而言，P提供了其执行的相关环境、以及Goroutine的任务队列等。 M是OS线程的抽象，是物理存在的。M只有和P绑定之后，才可以执行G代码。M本身也不会保存G的状态，在需要任务切换时，M会将堆栈状态保存回G中，任何M都可以根据G中的信息恢复执行。  M阻塞 当M准备执行Goroutine时，首选需要关联一个P，然后从P的队列中取出一个G来执行。如果G中执行的代码使M发生阻塞，比如唤起系统调用。那么M将会一直阻塞，直到系统调用返回。此时全局空闲M队列的另一个M会被唤醒，同时，阻塞状态的M会与P解绑。这样做也是为了保证其他G不会因为缺少M而被阻塞执行。
但如果Goroutine在channel通讯过程中发生阻塞，M并不会展示相似的阻塞行为。因为OS并不了解channel 的执行机制，channel是被Go Runtime来处理的。如果一个Goroutine在channel通讯上发生了阻塞，那没有任何理由让运行它的M也阻塞。这种情况下，G的状态会被设置为等待，M会继续执行别的Goroutine。当G重新变成可运行状态时，等待别的M去执行。
P的改进 原始Go的调度并没有P，仅有G、M以及Sched。当时系统只存在一个全局的G队列，通过Sched锁来进行并发控制。存在的问题有：
 调度的执行依赖全局的Sched锁，修改全局的M队列和G队列、或者其他全局的Sched字段都需要加锁 M的内存问题，执行的内存是跟M相关联的。但即使M并不执行G代码，它也会申请2MB的MCache空间，而这些空间只有M在执行G时才需要。同时，一个阻塞中的M也是不需要MCache的。 系统调用不够清晰，M在执行中会频繁阻塞和恢复，浪费CPU时间 M之间频繁的传递G，而不是选择自己执行它，这增加了系统的额外负载。每个M必须能够执行任何可运行的G，特别是刚刚创建了G的M。  P引入之后，从之前的M和Sched中抽取了部分字段，这样做带来了很多好处：
 MCache就被移动到了P中，而系统最多存在GOMAXPROCES个P，解决了不必要的内存浪费问题 G freelist被移动到P中，每个P都有了一个可运行的本地G队列。本地G队列缓解了全局Sched锁的问题。 当一个G被M创建，它被追加到对应P的本地队列末尾，以保证每个G都能被执行。  参考文章：</description>
    </item>
    
    <item>
      <title>基于Go的Cron Job实现</title>
      <link>/blog/2019/19-03-25-%E5%9F%BA%E4%BA%8Ego%E7%9A%84cron-job%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-25-%E5%9F%BA%E4%BA%8Ego%E7%9A%84cron-job%E5%AE%9E%E7%8E%B0/</guid>
      <description>随风要稳，逆风要浪
 timer  The Timer type represents a single event. When the Timer expires, the current time will be sent on C.
 下面使用timer实现在固定时间点执行task任务。
处理思路：每次在执行task前，计算当前时间和执行时间点的差值，通过设置timer未来的触发时间来执行任务。在完成本次task之后，重置timer的触发时间，等待下一次执行。
const IntervalPeriod time.Duration = 24 * time.Hour // 核心函数：在h:m:s的时候执行task任务 func runningRoutine(hour, minute, second int, task func() error) { ticket := time.NewTimer(GetNextTickDuration(hour, minute, second)) for { &amp;lt;-ticket.C if err := task(); err != nil { } ticket.Reset(GetNextTickDuration(hour, minute, second)) } } // 获取Task执行的时间 func GetNextTickDuration(hour, minute, second int) time.</description>
    </item>
    
    <item>
      <title>Go 调度模型（一）</title>
      <link>/blog/2019/19-03-24-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%80/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-24-go-%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%80/</guid>
      <description>想清楚了就去做，做的时候不要再回头想。
 OS Scheduler 在操作系统中保存了运行的进程列表，以及进程的运行状态(运行中、可运行及不可运行)。当进程运行时长超过了被分配的时间片(比如每10ms)，那么该进程会被系统抢占，然后在该CPU上执行别的进程。所以，OS的调度是抢占式的，可能抢占策略略有不同。
当进程被抢占时，需要保存该进程运行的上下文，并被重新放回到调度器，等待下一次被执行。
Golang Scheduler  Goroutine scheduler
The scheduler&amp;rsquo;s job is to distribute ready-to-run goroutines over worker threads.
 如图所示，OS层看到是只有Go进程以及运行的多个线程，而Goroutine本身是被Golang Runtime Scheduler调度管理的。
对OS而言，Go Binary是一个系统进程。内部Go Program对系统API的调度都是通过Runtime level解释来实现。Runtine记录了每个Goroutine的信息，在当前进程的线程池中按照顺序依次调度Goroutine。
Golang在Runtime内部实现了自己的调度，并不是基于时间切片的抢占式调度，而是基于Goroutines的协作式调度，目的就是要让Goroutine在OS-Thread中发挥出更多的并发优势。所以，在Runtime过程中，只有当正在运行的Goroutine被阻塞或者运行结束时，别的Goroutine才会被调度。常见的阻塞情形包括：
 阻塞的系统调用方式，比如文件或网络操作 垃圾自动回收  整体而言，Goroutine的数量大于Threads数量会更有优势，这样当其他Goroutine阻塞时，别的Goroutine就会被执行。
Goroutine G用于表示Goroutine及它所包含的栈和状态信息。Goroutine存在于Go Runtime的的虚拟空间，而非OS中。
// src/runtime/runtime2.go // 以下结构体精简了很多字段 type g struct { stack stack // offset known to runtime/cgo m *m // current m; offset known to arm liblink sched gobuf stktopsp uintptr // expected sp at top of stack, to check in traceback param unsafe.</description>
    </item>
    
    <item>
      <title>无聊的自我期待</title>
      <link>/life/2019/19-03-24-%E6%97%A0%E8%81%8A%E7%9A%84%E8%87%AA%E6%88%91%E6%9C%9F%E5%BE%85/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/life/2019/19-03-24-%E6%97%A0%E8%81%8A%E7%9A%84%E8%87%AA%E6%88%91%E6%9C%9F%E5%BE%85/</guid>
      <description>我慢慢明白了我为什么不快乐，因为我总是期待一个结果。
 事情并不会按你的意愿去发展 每一个买彩票的人，或者在被抽奖之前，都觉得自己是天选之人。然而，这样的个人意愿甚至至今都没能实现。客观理性的讲，无须执着低概率的事情，由它去吧，脑补它的结果，没有任何意义。
这种幻想发横财的事情如此，真实的世界亦如此。
人与人之间，如果不存在某种特殊关系，比如上下级，那么任何人没有义务执行你的设定。毕竟人是有主观想法的高等动物，有时候不给你使坏就很好很好了。</description>
    </item>
    
    <item>
      <title>数据一致性（二）</title>
      <link>/blog/2019/19-03-16-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E4%BA%8C/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-16-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E4%BA%8C/</guid>
      <description>我们流连于事物的表象，满足浅尝辄止的片刻欢愉，却几乎从不久留。我们在人生的道路上争先恐后，却吝于用片刻思考目标和方向。
 概述 至今没有接触过MySQL多主的情况，即存在多个MySQL实例同时负责读写请求（抛弃只读库）。思考后认为：没有这么实现的技术难点在于：数据的一致性得不到保证。此外，还会涉及：
 MySQL采用自增主键索引的话，多主之间的数据同步简直是灾难。 内部锁机制的优势大打折扣，跨主库间的锁应该也是灾难级别的吧。  那么支持分布式的其他数据库又是怎么搞定这个问题的呢？比如Cassandra，多个节点之间可以同时处理读写请求，那么它是如何处理节点间数据同步以保证一致性的呢？
MySQL数据的一致性  We think this is an unacceptable burden to place ondevelopers and that consistency problems should be solved at the database level
 细细想想，MySQL自身实现的数据一致性也是相当复杂的。以Innodb举例，如果通过普通索引执行查询，首先获取到的仅仅是主键索引，后面还需要通过主键索引来获取完整的记录。查询如此，更新亦如此。
Master-Slave模式 通常情况下，MySQL部署都是一主多从。Master作为更新DB的入口，而Slave的数据通过binlog来进行同步。所以大胆想一想，有没有可能出现一种情况（假设id=1记录原始的name值为neojos）：
## 第一次同步数据 update s-1 set name=&amp;quot;neojos-1&amp;quot; where id = 1; ## 失败 update s-2 set name=&amp;quot;neojos-1&amp;quot; where id = 1; ## 成功 update s-3 set name=&amp;quot;neojos-1&amp;quot; where id = 1; ## 成功 ## 第二次同步数据 update s-1 set name=&amp;quot;neojos-2&amp;quot; where id = 1; ## 成功 update s-2 set name=&amp;quot;neojos-2&amp;quot; where id = 1; ## 失败 update s-3 set name=&amp;quot;neojos-2&amp;quot; where id = 1; ## 成功  最后，数据库从某一个时间点开始，Master和Salve的数据会变得不一致了当然不可能，MySQL在数据同步上做了非常硬的约束。包括Slave_IO_Running、Slave_SQL_Running以及Seconds_Behind_Master等。</description>
    </item>
    
    <item>
      <title>平常心</title>
      <link>/life/2019/19-03-12-%E5%B9%B3%E5%B8%B8%E5%BF%83/</link>
      <pubDate>Tue, 12 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/life/2019/19-03-12-%E5%B9%B3%E5%B8%B8%E5%BF%83/</guid>
      <description>愿你在所得少于付出时，不会终日愤愤；愿你在所得超过付出时，不必终日惶恐。
—— 东野圭吾
 一切都很准时  New York is 3 hours ahead of California
But it does not make California slow
Someone graduated at the age of 22
but waited 5 years before securing a good job
Someone become a CEO at 25
and died at 50
while another become a CEO at 50
and lived to 90 years
Someone is still single
while someone else got married</description>
    </item>
    
    <item>
      <title>Database Sharding</title>
      <link>/blog/2019/19-03-09-database-sharding/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-03-09-database-sharding/</guid>
      <description>Sharding 可以简单地认为Sharding就是对数据进行分组的过程，即将整个大的数据集按照某种规则分割成多个小数据集。类似于网站服务，针对不同的服务，提供服务的链接地址也不相同，而这其实也是一个Sharding的过程。在业务层实现的Sharding，关键就在Route的过程，即将具体的数据请求，发送到对的数据集上。
基本要求：Sharding前后执行相同的查询，返回的结果也相同。
Why Sharding 在一些本地缓存的开发中，如果以map的形式存储数据集，因为该类型不支持并发操作。所以，在读写操作时就需要对map进行加锁，。可想而知，每次操作都加锁、解锁，而读写缓存又是一个高频操作，性能当然上不去。解决的思路就是对数据集进行Sharding操作，将整个数据集拆分成多个小块数据集，这样分别对小块数据集进行加锁、解锁，性能就提高了不少。
如果数据集过大，表的检索性能会越来越低。而如果对数据集进行分片，对分片数据并发检索，以及将某些分片数据直接加载到内存，都可以极大提高操作的效率。
数据均匀分布 拿博客网站举例，老用户发布的博客肯定要多于新注册的用户。那么，在对博客记录进行分表操作时，就需要考虑数据均匀分布的问题，避免老用户都分布在一张表内，造成某张表数据额外大。
解决的办法很简单，即对Sharding Key先做Hash处理，然后再实现数据Sharding过程。比如在系统设计初期，我们考虑基于用户UID将博客数据拆分成4个表。最基础的分表策略：
// 最终的结果就是blog_0, blog_1, blog_2, blog_3 tablePrefix := blog_%d tableName := fmt.Sprintf(&amp;quot;%s&amp;quot;, uid&amp;amp;3)  业务类型 分表过程需要结合实际项目，不同的业务场景，需求千差万别、更别说底层的数据了。拿购物场景来说，以商铺为分表Key，将同商铺的交易数据存储到一起，跟以用户为分表Key，将同用户的交易数据存储到一起。这两种情形完全不同，而这也将决定项目后期数据统计的方式。
所以分区的根本再于业务要执行的具体数据操作，要知道跨分区来之行Join之类的操作，不仅处理麻烦，性能也是问题。
服务化 实际项目中，按单一维度划分业务数据几乎是不可能的。比如上面说到的购物场景，商铺是一个维度，用户同样也是一个维度，当然，产品的类型还是一个维度。
这种情况下，我们一般选择将项目拆分成多个微服务，划分各个微服务间操作数据的边界范围。而服务与服务间的数据交互都通过调用API来实现。
需要注意的是，服务拆分一定要从具体业务来考虑，尽可能将相关的数据放在一起，提高检索性能，避免每次操作都需要通过网络来发送数据。
唯一ID 将数据集划分为多个分区后，基于不同的业务，查询场景也会面临各种各样的问题。比如在博客网站中，假设我们基于用户做了Sharding分表，而查询需求是：按照博客的发布时间顺序来展示列表。所以，这样的分页查询会异常痛苦。假设列表每页展示20条记录，那么代码就需要从每张表中都取出最近的20条记录做merge处理。如果Sharding的个数足够多，那简直无法继续了。
-- 获取最新的20条发帖记录。如果要翻页的话，将Now替换为上一批数据的最早时间 select * from blog_0 where create_time &amp;lt; Now() order by id desc; select * from blog_1 where create_time &amp;lt; Now() order by id desc; select * from blog_2 where create_time &amp;lt; Now() order by id desc; select * from blog_3 where create_time &amp;lt; Now() order by id desc;  正确的处理方式是创建一张关系表，记录所有博客元信息。而上述分页操作都基于该关系表来实现。但Sharding中的各表都有各自的自增主键，都从1开始自增，这就导致各个表中存在相同的标识符，再关系表中无法做区分。解决的办法大致有两种：</description>
    </item>
    
    <item>
      <title>读书有感</title>
      <link>/life/2019/19-03-03-%E8%AF%BB%E4%B9%A6%E6%9C%89%E6%84%9F/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/life/2019/19-03-03-%E8%AF%BB%E4%B9%A6%E6%9C%89%E6%84%9F/</guid>
      <description>19年开始读一些文学书，内心多了30的焦虑，希望从别人的故事中汲取力量。
 创业，从一个小目标开始 创办了zendesk的企业家写的一本回忆录，从透支信用卡、挤经济舱到开全球限量版轿车、坐私人飞机，其中的艰辛只有他自己最清楚。作为一个看客，最触动我的便是：他们年过30，随便去任何一家公司都能领到不错的薪水，但还是毅然决然地选择了自己创业，他们的理由很简单，如果这次创业机会不能好好把握，自己的人生便是死局。
创业，可能是每个有梦想的人都必须走的一条路吧。</description>
    </item>
    
    <item>
      <title>MySQL事务（2）</title>
      <link>/blog/2019/19-02-28-mysql%E4%BA%8B%E5%8A%A12/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-02-28-mysql%E4%BA%8B%E5%8A%A12/</guid>
      <description>花繁柳密能拨开方见手段，风狂雨骤时可立定才是脚跟。
 引言 在MySQL处理事务的过程中，遇到如下报错：
Error 1205: Lock wait timeout exceeded; try restarting transaction  结合日志信息，很快的定位了问题代码并做了修复，但这个报错却一直存在。观念里，只要等待一段时间，这个错误就应该消失啊，是哪里出问题了？
问题 代码在执行Begin之后，处理到某个逻辑直接return处理，没有关闭事务导致的。因为SQL操作的记录一直占着锁得不到释放，所以后续对该行记录进行写操作时，就会报这个错误。示例代码如下：
func (notify *Sign) HandleSign(ctx *context.Context) error { // 事务操作，开启事务 if err := ses.Begin(); err != nil { return err } // 这里需要特别注意，正常情况必须加上 // defer ses.Close() // 更新Log表记录 _, err := ses.Where(&amp;quot;id = ?&amp;quot;, contractLog.Id).Cols(&amp;quot;status&amp;quot;).Update(contractLog) if err != nil { ses.Rollback() return err } //这个地方直接return导致的，这个事务没有关闭，导致上面的锁一直没有释放 return nil if err := ses.Commit(); err !</description>
    </item>
    
    <item>
      <title>HBase Region Split</title>
      <link>/blog/2019/19-02-22-hbase-region-split/</link>
      <pubDate>Fri, 22 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-02-22-hbase-region-split/</guid>
      <description>  Tables are automatically partitioned horizontally by HBase into regions. Each region comprises a subset of a table&#39;s rows
 引言 HBase对Table提供了自动拆分的功能，非常好奇，它的自动拆分是如何实现的呢？在工作中，我们经常会遇到MySQL数据分表的情况，比如一个数据表已经超过20G了，查询耗时相对严重，我们考虑对其进行分表。又比如，我们预测每个月的交易流水会有几十亿条数据，我们按月对表进行拆分，等等。
MySQL提供了分区表，市面上也有很多的Sharding策略。但具体问题需要具体分析，分表策略也必须结合具体的业务来考虑。比如一个博客网站，用户主页需要展示他过去发布的所有博客列表，基于此，按照用户来分表就更为合理，保证同一个用户的所有数据都存储在同一张表里。
Concepts HBase中column被划分到一个column family里，所有的column family成员有一个唯一的前缀来标识。比如info:190101和info:190102就属于info family的的成员。有趣的是，虽然column families必须在表schema definition中被声明，但它的成员却是能够随意添加。
关于Region，作为分布式HBase集群数据的最小单元，每个RegionServer管理着table regions的若干个子集。客户端在和HBase交互时，都需要请求集群的ZooKeeper来获取集群的服务信息，缓存相关的元信息，之后就跟具体的RegionServer做交互了。
Reading Data 专门去查看了HBase获取数据的方式，主要包括两种：
 获取行 获取指定的列  相应的，我们需要了解它写入数据的方式：
put ’&amp;lt;table name&amp;gt;’,’row1’,’&amp;lt;colfamily:colname&amp;gt;’,’&amp;lt;value&amp;gt;’  ConstantSizeRegionSplitPolicy Region有两个重要的属性StartKey和EndKey，用来表示Region维护的RowKey范围，当Region Size达到一定的阈值，HBase就会对其执行split操作，Region被划分为两个，其中一个Region，称呼为Region_1包含的StartKey到MidKey，另一个Region_2包含MidKey到EndKey的数据。这也引申出HBase的前缀范围查询的功能。
之所以可以这样划分，是因为HBase中数据行是以RowKey的字典序排列的，并且RowKey是全局唯一的。这样的划分策略对RowKey的设计有点需求，如果RowKey的随机性不好，在分布式集群中，负责Region_1的RegionServer负载和负责Region_2的负载相差可能会很多，导致资源的利用很不均衡。
其实也很好理解，就比如以中国的姓作为RowKey的前缀，那么负责“王、李”的RegionServer的负载就会很高，因为这些数据都会根据StartKey-EndKey写入到具体的Region上。很多时候，为了使数据分布的更加均匀，充分利用集群的资源，我们需要对原始的RowKey做一次HASH处理，在原始的RowKey和实际存储的RowKey之间生成一个固定的映射关系。
extension 处理问题的思想都是相通的，比如HBase将数据存储到不同Region中。本质上也是：对数据进行分片，来扩展写操作。具体点就是：将数据划分成多个完全独立的子集，客户端通过映射关系，定向到具体的数据分片进行处理。
工作中接触最多的非MySQL莫属，如何对数据分片，根本还在于业务最终要实现怎样的查询。注意：设计中要尽量避免分片数据集的关联表查询，可能会有痛苦。
参考文章：
 HBase - Read Data  </description>
    </item>
    
    <item>
      <title>Mark-sweep GC</title>
      <link>/blog/2019/19-02-15-mark-sweep-gc/</link>
      <pubDate>Fri, 15 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-02-15-mark-sweep-gc/</guid>
      <description>把事做成的才是赢家，在口头上压倒对手，真的没有那么重要！
 Whirlwind introduce 当对象不再被引用时，对象不会立即被垃圾回收。也不存在任何子系统来专门记录使用的内存情况。
当系统没有内存空间时，触发GC处理。它首先会枚举所有的Root对象，然后递归的遍历根对象的引用关系。给遍历到的对象设置一个特殊标记，表明该对象是可达的，空间不能被回收。
当标记结束后，GC进入清洗阶段，任何在内存中没有被这次垃圾回收标记的对象都会被系统回收。
The algorithm 程序主要包含3个阶段：列举所有Root对象、标记起始于Root的对象引用、清除无效的对象。
void GC() { HaltAllProcessing(); ObjectCollection roots = GetRoots(); for(int i = 0; i &amp;lt; roots.Count(); ++i) Mark(roots[i]); Sweep(); }  Root Enumeration Root Enumeration会列举系统所有对象引用。运行系统需要为GC提供一种获取Root对象列表的机制。比如，在.NET中JIT维护了当前活跃的root对象，提供了获取根对象列表的API。
一个函数接受一个指针类型的参数，当方法返回时，jitter会识别出该参数不会再被使用，而将其从root中移除。
Mark 每个对象在创建时创建额外的空间，用于去mark这个对象，这个过程也是递归的
void Mark(Object* pObj) { if (!Marked(pObj)) // Marked returns the marked flag from object header { MarkBit(pObj); // Marks the flag in obj header // Get list of references that the current object has // and recursively mark them as well ObjectCollection children = pObj-&amp;gt;GetChildren(); for(int i = 0; i &amp;lt; children.</description>
    </item>
    
    <item>
      <title>新年彩蛋之中大奖</title>
      <link>/blog/2019/19-02-08-%E6%96%B0%E5%B9%B4%E5%BD%A9%E8%9B%8B%E4%B9%8B%E4%B8%AD%E5%A4%A7%E5%A5%96/</link>
      <pubDate>Sat, 09 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-02-08-%E6%96%B0%E5%B9%B4%E5%BD%A9%E8%9B%8B%E4%B9%8B%E4%B8%AD%E5%A4%A7%E5%A5%96/</guid>
      <description>2019年计划通过福利彩票发家致富的，可以好好看一看这篇博客。作为新年彩蛋来送给大家，也希望大家能真的中大奖。—— 新年快乐，给每个有梦想的程序员
 生成随机号 小概率事件也要做的一丝不苟，大家都是程序员，为啥要用别人家写的随机代码。嘎嘎！
双色球蓝号1-12、红号1-33，非常简单，只需保证生成的红号不相互重复就可以，然后就是考虑如何做到真正的随机。
还有一个问题就是如何存储一组号码。首先，分成红区和蓝区两部分，最后一个号约定为蓝号。另外，为了方便存储，我们放弃了将每个数字用符号连接的方式，而是自定义了34进制，用于保证每组号码的长度都是7。
var redBall = map[int]rune{ 1: &#39;1&#39;, 2: &#39;2&#39;, 3: &#39;3&#39;, //...... 31: &#39;V&#39;, 32: &#39;W&#39;, 33: &#39;X&#39;, } var redFlip = map[rune]int{ &#39;1&#39;: 1, &#39;2&#39;: 2, &#39;3&#39;: 3, //... &#39;V&#39;: 31, &#39;W&#39;: 32, &#39;X&#39;: 33, }  我们提供一个编解码的方法，用于将字符串转换为一组号码。对应的，将一组号码转换为长度为7的字符串。
随机红号范围控制在1-33，蓝号控制在1-16。所以，我们对当前纳秒进行取余，便可以保证数据的正确。对于去重部分，通过map属性来达到目的，map的key存储生成的随机号，value存储对应的编码。因为map结构读取数据时本身也是随机的，所以在生成红号和蓝号的时候便多生成一部分，最后再取6个红号，1个蓝号。
type TwoColor struct { } //encode func (color *TwoColor) Encode(origin []int) string { runes := make([]rune, 0) for _, v := range origin { if elem, ok := redBall[v]; ok { runes = append(runes, elem) } return string(runes) } //decode func (color *TwoColor) Decode(origin string) []int { result := make([]int, 0) for _, v := range origin { if elem, ok := redFlip[v]; ok { result = append(result, elem) } } return result } //generate random numbers func (color *TwoColor) GenerateRandom() string { redResult := make(map[int]rune, 12) for len(redResult) &amp;lt; 12 { key := time.</description>
    </item>
    
    <item>
      <title>垃圾回收之引用计数</title>
      <link>/blog/2019/19-02-01-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E4%B9%8B%E5%BC%95%E7%94%A8%E8%AE%A1%E6%95%B0/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-02-01-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E4%B9%8B%E5%BC%95%E7%94%A8%E8%AE%A1%E6%95%B0/</guid>
      <description>思来想去，决定总结一下垃圾回收机制。引用计数与我结缘最早，也比较简单、基础，遂决定从引用计数入手。
—— 不管人非笑，不管人毁谤，不管人荣辱，任他功夫有进有退，我只是这致良知的主宰不息，久久自然有得力处
 Reference Counting 对象在创建时保存一个自身被引用的计数，初始值为1。每次被新的变量引用，该值加1。相反，则减去1。当该值等于0时，占用空间被系统回收。
什么是对象呢？ var neojos int64 = 32 var ptrNeojos *int64 = &amp;amp;neojos  如上所示，我们创建了一个int64类型的object，命名为neojos。程序中对该object的操作都是通过使用neojos来实现的。而ptrNeojos其实又创建了一个*int64类型的object，但它的值保存的是neojos的地址。
对于ptrNeojos来说，它的生命周期跟普通变量的生命周期没有区别。唯一区别的是，当它生命周期结束后，ptrNeojos会被垃圾回收，而底层指向的object却不会。
如何计数呢？ Object * obj1 = new Object(); // RefCount(obj1) starts at 1 Object * obj2 = obj1; // RefCount(obj1) incremented to 2 as new reference is added Object * obj3 = new Object(); obj2-&amp;gt;SomeMethod(); obj2 = NULL; // RefCount(obj1) decremented to 1 as ref goes away obj1 = obj3; // RefCount(obj1) decremented to 0 and can be collected  obj1指向了一个匿名对象，为了方便，我们叫anonymousObj。上述代码展示了anonymousObj从创建到被垃圾回收的整个过程。垃圾回收对象的内存空间，上述过程中obj1对象的地址不会发生改变，只是底层引用的对象发生了变化。</description>
    </item>
    
    <item>
      <title>gRPC入门</title>
      <link>/blog/2019/19-01-26-grpc%E5%85%A5%E9%97%A8/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-01-26-grpc%E5%85%A5%E9%97%A8/</guid>
      <description>时间飞逝 如一名携带信息的邮差 但那只不过是我们的比喻 人物是杜撰的 匆忙是假装的 携带的也不是人的讯息
 为什么使用grpc 主要包括以下两点原因：
 protocl buffer一种高效的序列化结构。 支持http 2.0标准化协议。  很对人经常拿thrift跟grpc比较，现在先不发表任何看法，后续会深入thrift进行介绍。
http/2  HTTP/2 enables a more efficient use of network resources and a reduced perception of latency by introducing header field compression and allowing multiple concurrent exchanges on the same connection… Specifically, it allows interleaving of request and response messages on the same connection and uses an efficient coding for HTTP header fields.</description>
    </item>
    
    <item>
      <title>Float的基本介绍</title>
      <link>/blog/2019/19-01-16-float%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-01-16-float%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/</guid>
      <description>关于浮点数，为什么它生来就可能存在误差？带着好奇查阅了一些介绍，然后做了简单汇总。这只是一段知识的开始，后续还会继续完善。
—— 荡荡上帝，下民之辟。疾威上帝，其命多辟。天生烝民，其命匪谌。靡不有初，鲜克有终。
 Floating-point represent 浮点数在计算机中是如何表示的？因为它有小数点，那么小数点后面的数，该如何用二进制来表示呢？我们都知道，浮点数本身就存在误差，在工作中，你所使用的float都是一个近似数。这又是什么原因导致的呢？
1. Fixed-point fixed-point 是将bit位拆分成固定的两部分：小数点前的部分和小数点后的部分。拿32 bit的fixed-point表示举例，可以将其中的24 bit用于表示整数部分，剩余的8 bit表示小数部分。
假如要表示1.625，我们可以将小数点后面的第一个bit表示$\frac12$，第二个bit表示1/4，第三个1/8一直到最后一个1/256。最后的表示就是00000000 00000000 00000001 10100000。这样其实也好理解，因为小数点前是从$2^0$开始向左成倍递增，小数点后从$2^{-1}$开始向右递减。
因为小数点后面的部分始终小于1，上面这种表达方式能表达的最大数是255/256。再比这个数小，这种结构就无法表示了。
Floating-point basics 根据上面的思路，我们用二进制表达一下5.5这个十进制数，转化后是$101.1_{(2)}$。继续转换成二进制科学计数法的形式：$1.011_{(2)} * 2^2$。在转换的二进制科学计数法过程中，我们将小数点向左移了2位。就跟转换十进制的效果一样：$101.1_{(10)}$ 的科学计数形式为$1.011 * 10^2$。
对于二进制科学计数法表达的5.5，我们将其拆分成2部分，1.011是一部分，我们称为mantissa。指数2是另一部分，称为exponent。下面我们要将$1.011_{(2)} * 2^2$ 映射到计算机存储的8 bit结构上。
我们用第一个bit来表示正负符号，1表示负数，0表示正数。紧接着的4 bit用来表示exponent + 7后的值。 4 bit最大可以表示到15，这也就意味着当前的exponent不能超过8，不能低于-7。最后的3 bit用于存储mantissa的小数部分。你可能有疑问，它的整数部分怎么办呢？这里我们约定整数部分都调整成1，这样就可以节省1 bit了。举个例子，如果要表示的十进制数是0.5，那么最后的二进制数不是$0.1_{(2)}$，而是$1.0 * 2^{-1}$。最后表示的结果就是：0 1001 011。
再来一个decode的例子，即将0 0101 100还原回原始值。根据之前的描述0101表示的十进制是5，所以exponent = -2，表示回二进制科学计数法的结果：$1.100 * 2^{-2} = 0.011_{(2)}$。我们继续转换成真实精度的数：0.375。
最后可以看在，如果mantissa的长度超过3 bit表示的范围，那么数据的存储就会丢失精度，结果就是一个近似值了。
1. Representable numbers 继续按照上面的思路，现在8 bit的浮点表示能表示的数值区间更大。
要表示最小正数的话，sign置为0，接下来的4 bits置为0000，最后的mantissa也置为000。那么最终的表示结果就是：$1.000_{(2)} * 2^{-7} = 2^{-7} ≈ 0.0079_{(10)}$。</description>
    </item>
    
    <item>
      <title>探讨分布式ID生成系统</title>
      <link>/blog/2019/19-01-11-%E6%8E%A2%E8%AE%A8%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-01-11-%E6%8E%A2%E8%AE%A8%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F/</guid>
      <description>全称Universally Unique Identifier，UUID占128bit，也就是16个英文字符的长度（16byte），需要强调的是，它的生成无需中心处理程序。
UUID被用来标识URN(Uniform Resource Names)，对于Transaction ID以及其他需要唯一标志的场景都可以使用它。
UUID是空间和时间上的唯一标识，它长度固定，内部中包含时间信息。如果服务器时间存在不同步的情况，UUID可能会出现重复。
UUID构成 基本格式，由6部分组成：
time-low - time-mide - time-high-and-version - clock-seq-and-reserved &amp;amp; clock-seq-low - node  一个URN示例：f81d4fae-7dec-11d0-a765-00a0c91e6bf6。
因为UUID占128bit，16进制数占4bit，所以转换成16进制0-f的字符串总共有32位。组成的各个部分具体由几位16进制表示，请查阅 Namespace Registration Template
因为UUID太长且无序，导致其不适合做MySQL的主键索引。而且MySQL自带的auto-increment功能，选择bigint的话也只占用64bit。
 All indexes other than the clustered index are known as secondary indexes. In InnoDB, each record in a secondary index contains the primary key columns for the row, as well as the columns specified for the secondary index. InnoDB uses this primary key value to search for the row in the clustered index.</description>
    </item>
    
    <item>
      <title>Gin使用</title>
      <link>/blog/2019/19-01-06-gin%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/19-01-06-gin%E4%BD%BF%E7%94%A8/</guid>
      <description>Gin对net/http包做了封装，支持路由、中间件等特性，极大的方便对Http Server的开发。文章通过一个Test例子，来简要介绍。对于特别基础的部分，请阅读参考文章。
接口测试 Go中testing包为程序自测提供了便利。可以查阅之前写的博客Go test基础用法，对其内容，我还是挺满意的。
使用Postman 对于接口测试，很多情况都在使用Postman这样的工具。首先在本地启动服务，然后在Postman中配置请求的地址和参数、执行请求、查看结果。
这种方式唯一让人不满意的地方在于：每次修改都需要重启服务。跟直接执行一次Test相比，明显多了一步。
使用Test 测试基类 下面的代码作为接口测试的基类。
TestMain中，我们为所有的测试用例指定通用的配置。之后在执行其他Test前，都会先执行TestMain中的代码。有效的避免了代码冗余。
getRouter方法用于返回一个gin的实例。我们将服务的路由重新在Test中指定，并设置了中间件。
testHttpResponse是我们请求处理的核心代码，发送请求，并保存响应到w中
//common_test.go func TestMain(m *testing.M) { //声明执行Test前的操作 gin.SetMode(gin.TestMode) testutils.NewTestApp(&amp;quot;../conf.test.toml&amp;quot;) flag.Parse() os.Exit(m.Run()) } //设置路由，获取框架Gin的实例 func getRouter() *gin.Engine { router := gin.Default() //配置路由，这是我项目中的自定义配置 router.Use(middleware.HeaderProcess()) RouteAPI(router) return router } //统一处理请求返回结果 func testHttpResponse(t *testing.T, r *gin.Engine, req *http.Request, f func(w *httptest.ResponseRecorder) error) { w := httptest.NewRecorder() r.ServeHTTP(w, req) if err := f(w); err != nil { t.Fatal(err) } }  测试用例 下面是具体的测试用例。我们构造了一个Json数据格式的POST请求，然后通过调用testHttpResponse方法来读取接口的响应数据。</description>
    </item>
    
    <item>
      <title>How to use godog</title>
      <link>/blog/2018/12-29-how-to-use-godog/</link>
      <pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/12-29-how-to-use-godog/</guid>
      <description>首先访问Git的地址：Godog，它也是用来做Go Test一样的事情，只是换了一种形式。引入了一个概念：BDD。通俗的讲，就是虚拟现实场景，完成一个业务的测试。
Godog了解 首先介绍Godog是用来干什么的，我也是根据版本库提供的README来解释的，建议大家自己去看看。首先，我们要定义一个场景：feature。这里我们创建一个文件夹feature，专门用来存储这类文件。然后创建一个文件：godogs.feature。文件内容如下：
# file: $GOPATH/src/godogs/features/godogs.feature Feature: 购买红酒 这里是一堆对这个Feature的描述 描述的继续... Scenario: 买一瓶红酒 Given Neojos Has 5 coins When I buy Red wine Then should be 1 remaining  在控制台执行godog时，控制台会输出默认建议的代码。输出如下：
You can implement step definitions for undefined steps with these snippets: func neojosHasCoins(arg1 int) error { return godog.ErrPending } func iBuyRedWine() error { return godog.ErrPending } func shouldBeRemaining(arg1 int) error { return godog.ErrPending } func FeatureContext(s *godog.Suite) { s.Step(`^Neojos Has (\d+) coins$`, neojosHasCoins) s.</description>
    </item>
    
    <item>
      <title>Net Transport</title>
      <link>/blog/2018/12-08-net-transport/</link>
      <pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/12-08-net-transport/</guid>
      <description>版本 0.02
在调用第三方请求时，正确使用Client也不是一件非常容易的事。
下面是截取的一段描述，建议Client或Transport在整个服务期间最好是全局单例的，Transport本身会维护连接的状态，而且线程安全。强烈建议，不要使用系统提供的任何默认值。
 The Client&amp;rsquo;s Transport typically has internal state (cached TCP connections), so Clients should be reused instead of created as needed. Clients are safe for concurrent use by multiple goroutines.
 Transport 如下是官方的简要描述。Transport字段在Client中被声明为接口类型，而实现这个接口的是Transport类型（略显绕）。在net包内部也提供了默认的实现变量：DefaultTransport。
// Transport specifies the mechanism by which individual // HTTP requests are made. // If nil, DefaultTransport is used. Transport RoundTripper  看一下RoundTripper这个接口，官方描述：
 RoundTripper is an interface representing the ability to execute a single HTTP transaction, obtaining the Response for a given Request.</description>
    </item>
    
    <item>
      <title>Deafult Decimal</title>
      <link>/blog/2018/12-01-deafult-decimal/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/12-01-deafult-decimal/</guid>
      <description>版本 0.00
 我说：version dependent 表示我们的思考时，应该依赖具体的版本。举个例子，你把2015年看到的一些redis机制拿到现在跟别人谈论，很容易闹出笑话。在3年的时间里，它可能已经做了无数的优化。所以，思考要与时俱进。
 引言 在涉及到支付业务的时候，数据库里的钱怎么存：  存储单位是元。在业务处理的时候就会涉及到浮点数，很多商家喜欢将价钱定义为0.99而不是1元。这在使用过程中非常忌讳是否相等的比较。浮点数的比较经常喜欢用|floatA - floatB| &amp;gt; 0.00001来，很多第三方库也提供了比较方法。 存储单位时分。为了避免浮点类型比较时的不确定性，决定使用整形来替代。一般来说没有问题，可如果是要严格缺心眼打折，比如给一个售价4.99的打5折，那么最后就会存在5里的情况。一般都喜欢向上取整，应收用户2.495，实收用户2.50.  那么在MySQL的Column中该如何存储呢？  如果是分的话，肯定时当整形来存储的。但如果时浮点的，大家都会选择decimal，因为该类型不会丢失精度。 存储为字符串。浮点数保留指定位数的字符串。在Go中我也尝试过，fmt.Sprintf(&amp;quot;%.2f&amp;quot;, 3.091)还是靠谱的。  这篇文章当然不是来分析这两种存储方式的，也不是来分析该存储什么数据类型的。而仅仅时想阐述一个之前不了解的知识点（知识点太少，写点别的来凑）。
deault value 在MySQL建表的过程中，一般都会指定DEFAULT VALUE。在执行INSERT时，如果不指定该字段，MySQL会默认使用该默认值来替代。下面是创建的一个decimal类型字段，在Go中使用xorm 来表示，可以看出，xorm使用字符串类型来接收decimal类型的值。
type table_test struct { PayPrice string `xorm:&amp;quot;not null default 0.00 comment(&#39;支付价钱&#39;) DECIMAL(10,2)&amp;quot;` }  最后发现：在测试环境下，向数据库插入记录时，不指定PayPrice没有任何问题。但到了正式服数据表插入便失败了。报错信息如下：
{ &amp;quot;Number&amp;quot;: 1366, &amp;quot;Message&amp;quot;: &amp;quot;Incorrect decimal value: &#39;&#39; for column &#39;pay_price&#39; at row 1&amp;quot; }  STRICT_TRANS_TABLES 查询sql_mode如下：
show variables like &#39;sql_mode&#39;  下面的内容截取至:Strict SQL Mode:</description>
    </item>
    
    <item>
      <title>NSS初窥</title>
      <link>/blog/2018/11-20-nss%E5%88%9D%E7%AA%A5/</link>
      <pubDate>Tue, 20 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/11-20-nss%E5%88%9D%E7%AA%A5/</guid>
      <description>版本 0.01
在使用wireshark分析https时，加密传输的内容会解析失败。而NSS可以存储TLS握手过程中的Key，用于解密。很好奇，它是如何实现的？
wireshark配置  Edit→Preferences→Protocols→SSL→(Pre)-Master-Secret log filename.  key log文件格式 文件的格式：&amp;lt;Label&amp;gt; &amp;lt;space&amp;gt; &amp;lt;ClientRandom&amp;gt; &amp;lt;space&amp;gt; &amp;lt;Secret&amp;gt;
比较常见的Label和对应日志的内容:
CLIENT_HANDSHAKE_TRAFFIC_SECRET CLIENT_RANDOM CLIENT_TRAFFIC_SECRET_0 EXPORTER_SECRET SERVER_HANDSHAKE_TRAFFIC_SECRET SERVER_TRAFFIC_SECRET_0  CLIENT_RANDOM 8e40057e8e1c32f42faf87ddc17a81da9e02aa6c4ef4fcec2dcb504982e50691 0df84b6a904cb47940666a9e198dceab94b58dd0f0e61775db52716a37759d25b600b44601b541f5b21669ef0814770e CLIENT_HANDSHAKE_TRAFFIC_SECRET 9aef967472d9d65bf269989ac68c68c6374fd8c2cf9edf98c91593c8df7ffa3f 23b0b29e3edfe6e53114c6d9cb85159902462801c5540fc806c09f5d1711d992 CLIENT_TRAFFIC_SECRET_0 cd12af49c901682f29777821369c167854b047ac27a6a85c3db9dce565debbcf f826d8181c8b751cafd8c60bdbe2e4a4974113eea2a46c2a615e2115d70c9544 EXPORTER_SECRET 51c27befc24dceb18ade59b4a10c1398725848f7130b2ebeeb2e01483ed95e4d 58cc23d6d1db1e674200f7457dc9833ec826a5faf71830a6c5d3f4e63fa2144f  TLS加解密 case 1. 使用wireshark抓取一个TLS的握手过程来简单示例。其中Client Key Exchange, Change Cipher Spec, Encrypted Handshake Message在一个数据包中返回。
我在网上找到了严格一致的流程分析图（画的真不错）。
计算对称密钥的过程:
 在Client Hello阶段生成random_C 在Server Hello阶段生成random_S 在Client Key Exchange阶段，客户端根据这两个随机数，生成Pre-master，并用公钥加密，发送给服务端 Change Cipher Spec阶段，服务端私钥解密，获取PRe-master，用同样的算法计算加密密钥。  关于Change Cipher Spec Protocol  The change cipher spec protocol is used to change the encryption being used by the client and server.</description>
    </item>
    
    <item>
      <title>singleton pattern</title>
      <link>/blog/2018/11-03-singleton-pattern/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/11-03-singleton-pattern/</guid>
      <description>版本 0.02
在服务运行期间，针对所有goroutine共用一份数据的情况，比如配置信息，都可以选择只读取一次配置文件。但还是要特别注意：
 单例中不要保存只属于具体的goroutine的数据，否则会出现相互覆盖的情况。 单例中使用的具体业务数据要通过参数的形式传递，避免有成员变量存在。  单例也是Lazy Initialization的一种，对于经常不使用的变量，只有在使用的时候才进行实例化，整体来说，还是节约资源的。
或者类似net/http中的client这种类型：
 Clients and Transports are safe for concurrent use by multiple goroutines and for efficiency should only be created once and re-used.
 sync.Once简介 Go语言通过sync包可以方便的实现线程安全的单例模式。最叹为观止的是，sync包的实现如此简单。通常用来处理在服务运行期间，只需要初始化一次的变量。
// Once is an object that will perform exactly one action. type Once struct { m Mutex done uint32 } func (o *Once) Do(f func()) { if atomic.LoadUint32(&amp;amp;o.done) == 1 { return } // Slow-path.</description>
    </item>
    
    <item>
      <title>数据一致性（一）</title>
      <link>/blog/2018/10-20-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E4%B8%80/</link>
      <pubDate>Sat, 20 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/10-20-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E4%B8%80/</guid>
      <description>版本：0.00
MySQL的事务是数据一致性的典范，事务内的执行要么都成功，要么都失败。但业务系统涉及系统间的相互调用，涉及的数据库也不尽相同，所以实现数据一致性还是有挑战的。
首先了解强一致性和弱一致性。在微服务中，系统间通过HTTP的方式相互调用，很难实现数据的强一致。我们这里主要说弱一致性，也就是数据最终一致性。
数据一致性还有个重要的前提：支持幂等。也就是说，只要请求参数不变，那么无论重复请求多少次，结果都一样。在对接第三方支付时，这个词出现的频率还是老高的。
购买业务 蜗牛要在一家电商网站买电子书，整个购买流程和涉及的系统虚构如下图。过程涉及检查它是否已经买过，然后是生成订单号、支付、交付（实际上订单系统不包含支付功能，这里简化处理）。


交付涉及三个系统，在任何一个系统内，数据库的事务都只能保证它服务内的数据一致。而且，如果在事务过程中引入了调用第三方的HTTP请求，数据库的事务执行结果甚至有可能会被污染。比如，HTTP请求超时返回失败，但实际上请求却执行成功的场景。
代码设计 参考之前写的 Saga Pattern模式，对任何一个外部服务的调用都引入两个行为：执行和补偿。补偿是对执行结果的修正。比如对于用户支付失败的场景，补偿行为可以是接口重试、可以是直接退款、还可以推送MQ异步修复等。
统一使用interface来定义一套规范。每一种支付方式以及购买产品所调用的外部服务可能不尽相同，用interface来达到统一调用的目的。补偿的行为都基于执行动作返回的错误，所以我们需要实现自己的错误码。
type DeliverPattern interface { //是否需要执行交付流程 Check(ctx *context.Context) (bool, error) //支付及支付补偿 DoPay(ctx *context.Context) error PayCompensate(ctx *context.Context, doErr error) error //交付及对应的补偿 DoDeliver(ctx *context.Context) DeliverCompensate(ctx *context.Context, doErr error) error }  如何补偿 对于如何补偿，不同的业务有不同的补偿方式，当让不能一概而论。但整体的思想，我觉得还是不外乎两种。当然，下面的两种描述是自己这样称呼的。
事务类 首先便是数据库事务类，任何一个流程失败，整个事务内的操作全部反向回滚。沿着这样的思路，接口定义中PayCompensate应该实现DoPay的回滚操作，而DeliverCompensate应该实现DoPay以及DoDeliver的回滚操作。
我们需要在操作的同时维护一个回滚操作的队列，任何一个Do行为的完成，都需要在回滚队列中插入对应的回滚方法。当后面任何一个Do操作失败，统一执行回滚队列的方法。
这样的困境在于你不能完全保证回滚方法一定成功执行。而且出于性能考虑，还需要结合异步队列，通过后台重试来保证整个业务流程彻底回滚成功或回滚失败。
状态类 每个业务都会拆分成各个更小的块，就跟写代码空行一样，这里的DeliverPattern也是根据业务流程拆分成更小的执行粒度。我们可以为每个Do行为都设置一个状态码，类似于状态机，记录每一次购买的各个状态。
const ( StatusDoPaySuccess = 1 StatusDoPayCompensateSuccess = 2 StatusDoPayCompensateFailure = 3 )  这样我们补偿方法中执行的不再是回滚操作，而是Do方法的重试。如果补偿成功，继续执行后续的操作，如果补偿失败，记录下该状态，后续看看怎么补偿。</description>
    </item>
    
    <item>
      <title>Go Interface 类型</title>
      <link>/blog/2018/10-12-go-interface-%E7%B1%BB%E5%9E%8B/</link>
      <pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/10-12-go-interface-%E7%B1%BB%E5%9E%8B/</guid>
      <description>草稿 0.02
introduction duck typing 很形象的解释了interface的本意。它是一种特别的数据类型，内部声明了一组要实现的方法集合，任何实现了这些方法的数据类型都可以认为实现了这个interface。这跟其他语言中的抽象类有异曲同工之处，但却不需要去明确声明实现了这个interface。
空的interface类型没有声明任何方法，所以GO中所有数据类型都实现了interface{}。这也为我们实现泛型编程提供了可能，虽然使用起来并不舒服。
protocol interface可以做为一组不相关的对象进行交流的一种规范或约束，类比protobuf，数据字段必须严格按照声明进行传递。只不过interface约束的是待实现的方法。
比如error接口，所有实现了Error()方法的类型都可以赋值给error类型变量，无需明确声明继承关系，就实现了多态。
// The error built-in interface type is the conventional interface for // representing an error condition, with the nil value representing no error. type error interface { Error() string }  interface用法 generic algorithm interface类型接受任意类型的参数，结合reflect或者断言可以确定参数的实际类型。比如fmt包就有这样用（具体需要深入方法内部）：
// Println formats using the default formats for its operands and writes to standard output. // Spaces are always added between operands and a newline is appended.</description>
    </item>
    
    <item>
      <title>Go net 超时处理</title>
      <link>/blog/2018/10-10-go-net-%E8%B6%85%E6%97%B6%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 10 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/10-10-go-net-%E8%B6%85%E6%97%B6%E5%A4%84%E7%90%86/</guid>
      <description>序 这篇文章详细介绍了，net/http包中对应HTTP的各个阶段，如何使用timeout来进行读/写超时控制以及服务端和客户端支持设置的timeout类型。本质上，这些timeout都是代码层面对各个函数设置的处理时间。比如，读取客户端读取请求头、读取响应体的时间，本质上都是响应函数的超时时间。
作者强烈不建议，在工作中使用net/http包上层封装的默认方法（没有明确设置timeout），很容易出现系统文件套接字被耗尽等令人悲伤的情况。比如：
// 相信工作中也不会出现这样的代码 func main() { http.ListenAndServe(&amp;quot;127.0.0.1:3900&amp;quot;, nil) }  正文 在使用Go开发HTTP Server或client的过程中，指定timeout很常见，但也很容易犯错。timeout错误一般还不容易被发现，可能只有当系统出现请求超时、服务挂起时，错误才被严肃暴露出来。
HTTP是一个复杂的多阶段协议，所以也不存在一个timeout值适用于所有场景。想一下StreamingEndpoint、JSON API、 Comet， 很多情况下，默认值根本不是我们所需要的值。
这篇博客中，我会对HTTP请求的各个阶段进行拆分，列举可能需要设置的timeout值。然后从客户端和服务端的角度，分析它们设置timeout的不同方式。
SetDeadline 首先，你需要知道Go所暴露出来的，用于实现timeout的方法：Deadline。
timeout本身通过 net.Conn包中的Set[Read|Write]Deadline(time.Time)方法来控制。Deadline是一个绝对的时间点，当连接的I/O操作超过这个时间点而没有完成时，便会因为超时失败。
Deadlines不同于timeouts. 对一个连接而言，设置Deadline之后，除非你重新调用SetDeadline，否则这个Deadline不会变化。前面也提了，Deadline是一个绝对的时间点。因此，如果要通过SetDeadline来设置timeout，就不得不在每次执行Read/Write前重新调用它。
你可能并不想直接调用SetDeadline方法，而是选择 net/http提供的更上层的方法。但你要时刻记住：所有timeout操作都是通过设置Deadline实现的。每次调用，它们并不会去重置的deadline。
Server Timeouts 关于服务端超时，这篇帖子So you want to expose Go on the Internet也介绍了很多信息，特别是关于HTTP/2和Go 1.7 bugs的部分.
对于服务端而言，指定timeout至关重要。否则，一些请求很慢或消失的客户端很可能导致系统文件描述符泄漏，最终服务端报错：
http: Accept error: accept tcp [::]:80: accept4: too many open files; retrying in 5ms  在创建http.Server的时候，可以通过ReadTimeout和WriteTimeout来设置超时。你需要明确的声明它们：
srv := &amp;amp;http.Server{ ReadTimeout: 5 * time.Second, WriteTimeout: 10 * time.Second, } log.</description>
    </item>
    
    <item>
      <title>Tcp Bulk Data</title>
      <link>/blog/2018/09-24-tcp-bulk-data/</link>
      <pubDate>Mon, 24 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/09-24-tcp-bulk-data/</guid>
      <description>TCP在数据传输中有receive buffer和send buffer。通过连接中的window size可以看出数据的读取情况。
sliding window client不需要等待已发送的packet的ACK，可以发送多个准备好的packet。换句话说，server端并不需要对每一个收到的packet，都执行ack操作。因为有缓冲去的存在，所以可以对收到的多个packet，统一回复一个ack。
window size 通过握手，TCP两端交换window size的大小。sender可以连续发送多个packet来填满receiver&#39;s window，当应用层从buffer中读取数据之后，window size便会更新。比如，在ack的回复中，如果显示win 0，则表示receiver接收到了所有数据，但数据还在buffer中，尚未被应用读取。之后数据被读取，window size便会被更新，通过ack来重新通知sender。需要注意的是，该ack仅仅只是通知window size的更新。
对于window size，相关的是sliding windows。可以简单的想象成固定长度的“队列”，长度一定，表示window size是固定的。应用读取数据之后，队列末尾便会追加对应数量的size，供sender发送新的数据，队首的数据便彻底被移除了。
还有一点需要注意的：client端经常有数据需要发送，当收到ACK之后，client就会立即发送buffer中准备好的数据，应用端无法同时读取buffer中的数据来更新window size。所以，一般发送数据的client端的window size要比约定的小点。
PUSH PUSH flag这是TCP header中的一个标识，用于表示sender不想让该packet在tcp buffer中被缓存，去等待额外的数据到来，而是应立即传递给receiver处理。
slow start sender和receiver网络连接中可能存在很多hop或者slower links，这也就导致了window size确定的复杂性。这便引入了congestion window,术语slow start。sender在建立连接后，先初始化window为一个segment，每次收到ack，sender便增加一个segment（exponential increae），最终，segment的大小便是congestion window size。
Delayed Acks sliding window有效的提升了TCP的数据传输效率，使得接收数据的一端可以在收到多个packet后统一回复最后一个packet的ACK消息。发送数据的一端完全不需要等待数据被ACK之后，再才开始发送下一个packet。</description>
    </item>
    
    <item>
      <title>sync.Once</title>
      <link>/blog/2018/09-03-sync.once/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/09-03-sync.once/</guid>
      <description>草稿0.0
sync.Once Go语言通过sync包可以方便的实现线程安全的单利模式。最叹为观止的是，sync包的实现如此简单。
// Once is an object that will perform exactly one action. type Once struct { m Mutex done uint32 } func (o *Once) Do(f func()) { if atomic.LoadUint32(&amp;amp;o.done) == 1 { return } // Slow-path. o.m.Lock() defer o.m.Unlock() if o.done == 0 { defer atomic.StoreUint32(&amp;amp;o.done, 1) f() } }  问题用法 下面声明一个获取计算签名的配置包，通过name来获取对应的值。获取是一个Lazy Initialization的过程，在需要使用的时候才会初始化config变量。
package encrypt_config //key-secret pairs var config map[string]string func loadConfig(name string) string { if config == nil { config = map[string]string{ &amp;quot;zi-ru&amp;quot;: &amp;quot;Mji9##a0LY&amp;quot;, &amp;quot;baidu&amp;quot;: &amp;quot;Kj8*0okhHH&amp;quot;, } } return config[name] }  上述代码最显而易见的问题：并发的情况下，包内变量config被初始化多次。因为各个goroutine访问config时可能都是nil。但还存在一种可能导致错误：某一个goroutine发现config ！= nil，但是当通过name去获取对应的值时，返回的却是空字符串。</description>
    </item>
    
    <item>
      <title>本地缓存BigCache</title>
      <link>/blog/2018/08-19-%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98bigcache/</link>
      <pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/08-19-%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98bigcache/</guid>
      <description>BigCache的作者做了详细的阐述，尽在这里：Writing a very fast cache service with millions of entries in Go。不得不说，作者的表述非常完美，给它点赞。GitHub地址在：github.com/allegro/bigcache。Usage非常简单。
Omitting GC 当map中存储过百万的object时，Go语言自身的GC甚至会影响不相关的请求，即使是对一个空对象做Marsh操作，响应时间也可能在1s以上。所以，如何避免Go默认对map做的Garbage Collector至关重要。
 GC回收heap中对象，所以我们不把对象创建在heap中就可以避过垃圾回收。查阅offheap。 使用freecache. 在map结构的key和value中不存储pointer，这样便可以将map创建在堆上，同时忽略GC的影响。这来源于Go的优化.  Concurrency 为了避免加锁成为系统的瓶颈，BigTable采用了Shared的方式来解决，确实也有点Redis单线程的感觉。将一块大的数据划分成多块小的数据，为小数据块加锁，确实很好的缓解了加锁的瓶颈。这体现出了拆分的思想，突然想到了曾经被面试的问题：“请将2G的数据进行排序”。
我比较好奇它的Hash方法，客户端的key转换为实际存储的hashedKey的过程。请看通过hashedKey获取shard的部分，作者没有使用%取余来实现，而是使用了&amp;amp;与运算来替代，确实很注重细节啊！
说到与运算:0&amp;amp;0=0; 0&amp;amp;1=0; 1&amp;amp;0=0; 1&amp;amp;1=1;，所以，最终拆分个数完全取决与二进制中1的数量。如果shardMask等于3，那就可以拆分成4份，如果等于4，那结果就是2份，以此类推。
//通过客户端的key获取实际存储的key // Sum64 gets the string and returns its uint64 hash value. func (f fnv64a) Sum64(key string) uint64 { var hash uint64 = offset64 for i := 0; i &amp;lt; len(key); i++ { hash ^= uint64(key[i]) hash *= prime64 } return hash } //通过实际存储的key获取shard块，使用与运算。 func (c *BigCache) getShard(hashedKey uint64) (shard *cacheShard) { return c.</description>
    </item>
    
    <item>
      <title>Golang下的Error</title>
      <link>/blog/2018/08-11-golang%E4%B8%8B%E7%9A%84error/</link>
      <pubDate>Sat, 11 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/08-11-golang%E4%B8%8B%E7%9A%84error/</guid>
      <description>感觉error确实没啥可说的，这个简单到极致的package总共也不超过10行有效代码。而且常用的fmt也提供了很方便的返回error的方法：
// Package errors implements functions to manipulate errors. package errors // New returns an error that formats as the given text. func New(text string) error { return &amp;amp;errorString{text} } // errorString is a trivial implementation of error. type errorString struct { s string } func (e *errorString) Error() string { return e.s }  自定义error error设计的如此简单，导致其判断错误类型就比较麻烦。比如我想判断MySQL的报错是否由主键冲突导致，我可以这样处理：
const PrimaryKeyDuplicateCode = &amp;quot;1062&amp;quot; if strings.Contains(err.Error(), PrimaryKeyDuplicateCode) { //commands }  这样的判断逻辑，如果仅是用于特殊情况，还勉强可以接收。但如果你要整个项目都使用这种形式的话，就会觉得精神崩溃，心理无法承受（反正我是这样感觉的）。所以，我们要自定义实现一个Error结构。当然，这样搞还有syscall这个package。</description>
    </item>
    
    <item>
      <title>Shell必备基础（1）</title>
      <link>/blog/2018/08-09-shell%E5%BF%85%E5%A4%87%E5%9F%BA%E7%A1%801/</link>
      <pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/08-09-shell%E5%BF%85%E5%A4%87%E5%9F%BA%E7%A1%801/</guid>
      <description>Shell是对Linux命令的深加工，用得好，事半功倍。 本来只想加深Array的用法，但一不小心，又变成了一篇基础大全。
比较运算 习惯在if语句中使用[[...]]对条件进行比较，字符串、数值以及文件，统统笑纳。
算数比较 常用的操作符有-eq、-ne、-gt、-lt、-le等。
文件系统 当编译文件、或者查看pid文件、日志时会经常用到。
[ -e $var ] 如果指定的变量包含的文件存在，则返回真 [ -f $var ] 如果指定的变量包含正常的文件路径或文件名，则返回真  字符串比较 判空还是相当常见的操作。比如，查看当前系统是否启用了notify的进程，如果有的话，kill掉。
pid=`ps -ef | grep notify | grep -v &#39;grep&#39; | awk &#39;{print $2}&#39;` if [[ -n $pid ]] then echo -e &amp;quot;\033[31m Kill掉当前正在运行的进程... \033[0m\n&amp;quot; kill $pid fi  常见的操作符如下：
[[ -z $str ]] 如果str包含的是空字符串，则返回真 [[ -n $str ]] 如果str包含的是非空字符串，则返回真  其他的操作符包括：==、!=、&amp;gt;、&amp;lt;
逻辑运算 使用逻辑&amp;amp;&amp;amp;和||来表示与和或的逻辑关系。
比如
if [[ -n $str ]] &amp;amp;&amp;amp; [[ -z $str ]] then echo $str fi  基本语句Example if语句 if command1 then # .</description>
    </item>
    
    <item>
      <title>Git撤销本地修改</title>
      <link>/blog/2018/08-07-git%E6%92%A4%E9%94%80%E6%9C%AC%E5%9C%B0%E4%BF%AE%E6%94%B9/</link>
      <pubDate>Tue, 07 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/08-07-git%E6%92%A4%E9%94%80%E6%9C%AC%E5%9C%B0%E4%BF%AE%E6%94%B9/</guid>
      <description>懂得了好多大道理，但还是过不好这一生！
 用了git就会发现，再也不想用svn了。
Note：在执行push操作前，所有的修改都发生在本地，可以使用reset随便回滚本地的提交。但要注意：本地修改一旦回滚，无法找回。在push后，想要回滚到指定的版本，便需要使用revert，这样的代价就是：你的回滚记录被记录在了log中，所有人都可以看见。
使用reset回退 本质上是commit操作的回退。Git工作流可以简化为三个部分：Working Directory、index、HEAD。后两部分对应的git命令便是add和commit。如果使用的是Sourcetree工具，那么这三部分就更直观了。
该命令的具体功能是移动HEAD，即移动分支的指针。将当前的HEAD重新指向之前的版本，本地工作环境也会跟着切换。适用场景：本地已经commit，但尚未push到远端仓库的回滚操作。
该命令提供了三个属性：分别是soft、mixed、和hard。
 soft撤销上一次的commit命令，返回到HEAD前的index状态。 mixed撤销了上一次的git add和git commit命令，将index的修改回滚到Working Directory。 hard撤销了最后git add 和 git commit 命令以及工作目录中的所有修改。  所以reset重写的顺序如下：
 移动 HEAD 指向的分支（如果是soft，则到此停止）。 使索引看起来像 HEAD（如果是mixed，则到此停止）。 使工作目录看起来像索引。  Example 当执行pull命令发生冲突时时，本地代码需要做merge操作。但本地代码只是临时调试修改，并不需要保存提交。执行如下命令，便会清空本地的修改，hard相当于一个版本的指针，origin/master可以替换为具体的版本号
git reset --hard origin/master git reset --hard version-number git reset --hard HEAD  获取版本号可以通过git log直接查看。
更多详细介绍，可以查看： 高级合并及 重置揭密
checkout stash储藏 将工作区的修改进行存储，使本地重新成为一个干净的环境，同时方便在之后应用这些改动。可以用于存储已被索引的文件、或者未跟踪的文件。执行git stash -a来暂存所有改动的文件。常见的包括如下命令：
 git stash 储藏修改 git stash list 查看储藏的列表 将储藏重新应用到当前分支：git statsh apply stash@{1}或者git stash pop stash@{1}。后者会在应用暂存之后从堆栈上删除 git stash drop stash@{1} 移除暂存 git stash clear 清除本地所有的贮藏历史  使用clean清空 用于从==工作区==移除==未被追踪的文件==，执行git clean -d -f来移除所有未被追踪的文件或目录。</description>
    </item>
    
    <item>
      <title>IP Routing</title>
      <link>/blog/2018/08-04-ip-routing/</link>
      <pubDate>Sat, 04 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/08-04-ip-routing/</guid>
      <description> 假设你在跟小米公司对接服务，那你有没有好奇过：自家的服务器是如何找到小米公司的服务器的。为了安全，公司的服务器可都是在内网的，用户是无法直接访问到的。好好了解一下Ip Routing，它可以给你部分答案。
概述 当主机IP(Network)层收到一个datagram后，它首先会检查datagram的目标主机是不是自己。如果是，那么它会将datagram发送给其他协议处理。如果不是，它便检查自己是否被配置作为一个路由。如果是，它将按照规则将datagram继续传递。否则，默默的把datagram丢掉。
所有routing提供的IP地址，都假设下一个hop是离目标地址最近的。在datagram传递的过程中，我们会修改link-layer的地址为下一个hop的地址，而Destination IP一般是不做修改的。
Routing Table 在命令行执行 netstat -rn，查看系统的routing table。
# 展示结果做了修改，非真实值 neojos@BJ ~ $ netstat -rn Kernel IP routing table Destination Gateway Genmask Flags MSS Window irtt Iface 140.168.0.0 10.3.206.240 255.255.0.0 UG 0 0 0 eth0  了解Falgs参数：  U 表示当前路由是在使用的 G 表示路由到一个gateway。如果没有设置该flag，则表示跟Destination是直接连接的。 H 表示路由到一个主机地址。如果没有设置该flag，则Destination是一个网络地址。 D 表示路由是通过ICMP Redirect设置的。一般来说，Redirect设置的route都是主机地址。  路由查询的顺序：  查找匹配的主机地址 查找匹配的网络地址 查找default的路由。  Host unreachable 当route没有找到destination时，Destination Host Unreachable的错误会返回给源主机。你可以通过ping路由表中配置错误的destination host address来查看。
ICMP Redirect Error </description>
    </item>
    
    <item>
      <title>ngrep抓包</title>
      <link>/blog/2018/08-01-ngrep%E6%8A%93%E5%8C%85/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/08-01-ngrep%E6%8A%93%E5%8C%85/</guid>
      <description>简单的工具其实挺有用的。警告：不许瞧不起工具，尤其是你还不会用的工具。
ngrep还是之前跟花椒直播的同事对接项目时，了解到的一个工具。它可以用来抓取服务器上通过网卡的所有请求。跟tcpdump差不多，但却更简单。tcpdump需要借助Wireshark才可以将请求完美展示出来，但这个就跟使用grep一样。
以前专门请教过一个同事如何使用Wireshark分析网络请求，自己也专门看了相关的Wireshark操作。但最终却发现，如果是抓客户端的请求(非分析TCP)，Wireshark使用起来并不方便。而如果分析的是服务端之间的请求，还需要借助tcpdump先来把请求记录下来，然后再到Wireshark中打开分析。
Example 经常使用的模式
# 匹配特定host ngrep -q host api.open.huajiao.com -d any -W byline # 匹配特定host和端口 ngrep –q host api.open.huajiao.com and port 80 –W byline # 报文中包含&amp;quot;search&amp;quot;关键字 ngrep –q –W byline &amp;quot;search&amp;quot; host www.google.com and port 80  Options 详细还是通过man直接查看工具说明吧！这里列举一个：
-d By default ngrep will select a default interface to listen on. Use this option to force ngrep to listen on interface dev.  结果示例 以下是命令输出的结果，跟curl是不是很像：
interface: any filter: ( host api.</description>
    </item>
    
    <item>
      <title>Tcp Server Design</title>
      <link>/blog/2018/07-28-tcp-server-design/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/07-28-tcp-server-design/</guid>
      <description> 绝大多数的TCP服务都是支持并发的。当一个连接请求到达时，服务端接收这个连接，然后创建一个新的线程(或进程)来处理这个连接。
listen状态 在本地启动Go的服务，使用netstat查看：
netstat -an -f inet  可以看到listen状态的请求连接。其中Local Address的*表示请求会被本地的任意地址处理(如果有多重地址的话)。Foreign Address中*.*表示客户端的ip和port都是未知的。
Active Internet connections (including servers) Proto Recv-Q Send-Q Local Address Foreign Address (state) tcp46 0 0 *.3900 *.* LISTEN  当新的请求到达，并被接收时，服务器内核中会创建一个ESTABLISHED状态的连接。而listen继续去接收新的连接。
Proto Recv-Q Send-Q Local Address Foreign Address (state) tcp4 0 0 127.0.0.1.3900 127.0.0.1.51133 ESTABLISHED tcp46 0 0 *.3900 *.* LISTEN  request queue 当listening状态的应用正在忙于处理新的连接，同时有其他的请求进来时，服务器是如何处理的呢？引入另一个概念：请求队列。
 每一个监听状态的终端都有一个固定长度的队列，用来存放TCP三次握手完成，但还没有被应用接收的连接。client会认为该连接已经创建成功，所以它此时发送的数据也会被缓存起来。如果queue中的连接长时间不被应用读取，便会导致client超时。 当队列满了后，TCP会直接忽略进来的SYN，而非回复RST报文头。这样做便是要client稍后重新发送SYN。因为服务器比较繁忙的状况，可能马上就会恢复。 如果TCP三次握手完成，连接也就被创建成功了。如果此时服务端不想为该ip提供服务，服务端要么发送FIN关闭这个连接，或者发送RST中断这个连接。整个过程中，TCP没有权限去限制client端。  </description>
    </item>
    
    <item>
      <title>重试</title>
      <link>/blog/2018/07-24-%E9%87%8D%E8%AF%95/</link>
      <pubDate>Tue, 24 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/07-24-%E9%87%8D%E8%AF%95/</guid>
      <description>为了克服网络问题，重试是我们常用的手段之一。但必须记住：重试的姿势非常重要。照应一句古话：“差若毫厘，谬以千里”。
正确的姿势是便是：
 如果请求没有成功，以指数型延迟重试
 指数退避  Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate
 通俗的的讲，网络上的节点在发送数据冲突之后，不应立即尝试重发，而应该等待一段时间再发送，等待时间是指数增长，从而避免频繁的触发冲突。在计算机网络中，二进制指数退避算法常常作为避免网络堵塞的一部分，用于同一数据块的重发策略。
发生n次冲突之后，等待时间在0~2^n-1个间隙时间(slot time)之间随机选择。比如第一次冲突之后，每个发送方会等待0或者1个间隙；第二次冲突之后，或等待时间会在0到3个间隙任意选择，依次类推，随着冲突次数的增加，发送方等待的时间可能成倍增加。
冲突达到一定次数，指数运算会停止，表示等待时间不会无限制增加下去。比如设置上限n=10,则最长等待时间为1023个时间间隙。同样，发送不可能永远的尝试下去，所以流程一般会在16次重试之后终止。
具体的退避算法：
1. 确定基本退避时间：争用期 2. 确定等待时间上限(max)。假设重传次数超过10次之后,k就不再增大。计算公式：k表示计算冲突等待时间的指数， k=min（重传次数, max） 3. 当重传达到16次仍不成功，则数据需要丢弃，并向高层报告。  退避算法的应用场景：
1. 三方支付中交易结果的推送通知。 2. 轮询，不间断的固定时间间隔的请求接口。  重试的问题 以下面的代码说明一下：
retryTimes := 1 for err != nil &amp;amp;&amp;amp; retryTimes &amp;lt;= 3 { //请求失败后，重新尝试 body, err = curl.</description>
    </item>
    
    <item>
      <title>siege压测</title>
      <link>/blog/2018/07-19-siege%E5%8E%8B%E6%B5%8B/</link>
      <pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/07-19-siege%E5%8E%8B%E6%B5%8B/</guid>
      <description> 关于压测，首先要了解TPS和并发用户数之间的关系：
 TPS就是每秒事务数，但是事务是基于虚拟用户数的。假如1个虚拟用户在1秒内完成1笔事务，那么TPS明显就是1；如果某笔业务响应时间是1ms，那么1个用户在1秒内能完成1000笔事务，TPS就是1000了；如果某笔业务响应时间是1s,那么1个用户在1秒内只能完 成1笔事务，要想达到1000TPS，至少需要1000个用户；因此可以说1个用户可以产生1000TPS，1000个用户也可以产生1000TPS，无非是看响应时间快慢。
 针对上面的描述，引申出了命令的三个属性：
-c : This option allows you to set the concurrent number of users -r : This option tells each siege user how times it should run. -t : This option specify the number of times each user should run  对于linux的命令，其实man查看就足够了。
example 提交json格式的数据请求到服务器。POST后跟数据内容，不需要使用引号处理。
# linux下执行命令 siege -f ./url.txt -H &amp;quot;Content-Type: application/json&amp;quot; # url.txt中的内容 HOST=neojos.com $(HOST)/v1/buy POST {&amp;quot;bid&amp;quot;: 0, &amp;quot;type&amp;quot;: 13 }  </description>
    </item>
    
    <item>
      <title>Kafka的offset初窥(1)</title>
      <link>/blog/2018/07-14-kafka%E7%9A%84offset%E5%88%9D%E7%AA%A51/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/07-14-kafka%E7%9A%84offset%E5%88%9D%E7%AA%A51/</guid>
      <description>kafka适用的场景很多，但用它来异步通知却是让我略感头痛！
引言 对于kafka的offset问题，先从这篇文章说起：How to disable auto commit? 它阐述了一个重要的信息：
 To disable auto-commit, simply delay your MarkOffset calls. A commit will only occur when the offsets have been changed. If you are not ready to commit, then don&amp;rsquo;t mark the offset as ready.
 对于其中的另一个建议：即由我们主动调用consumer.CommitOffsets()。当然它最后补了一刀：
 Auto commit gives us some trouble in this case, as we might commit offsets which we have not yet written to the database. Having the ability to disable it by simply setting config.</description>
    </item>
    
    <item>
      <title>Linux查看监听的端口进程</title>
      <link>/blog/2018/07-10-linux%E6%9F%A5%E7%9C%8B%E7%9B%91%E5%90%AC%E7%9A%84%E7%AB%AF%E5%8F%A3%E8%BF%9B%E7%A8%8B/</link>
      <pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/07-10-linux%E6%9F%A5%E7%9C%8B%E7%9B%91%E5%90%AC%E7%9A%84%E7%AB%AF%E5%8F%A3%E8%BF%9B%E7%A8%8B/</guid>
      <description>Netstat 是一款命令行工具，可用于列出系统上所有的网络套接字连接情况，包括 tcp, udp 以及 unix 套接字。另外它还能列出处于监听状态（即等待接入请求）的套接字。经常使用 netstat 用于查看网络连接信息和系统开启的端口号。
 还有一个完美替代它的命令ss，全称是socket statistics。ss的优点就在于“天下武功唯快不破”。
引言 已知服务监听的端口，想查找当前服务的PID。我们可以拆解成2个子问题： 1. 如何查看该端口是否在被服务监听 2. 如何查看该端口的PID
ps 用于查看服务器上的进程信息.最最常用的就是:
ps -ef  netstat 显示的状态列：Proto，Recv-Q，Send-Q，Local Address，Foreign Address，State。其中Recv-Q和Send-Q分别代表接收队列和发送队列。这些数字一般都是0，如果不是，则表示软件包正在队列中堆积。
Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 enlightened:domain *:* LISTEN  比较有用的选项是：
 -n：Show network addresses as numbers (normally netstat interprets addresses and attempts to display them symbolically). This option may be used with any of the display formats.</description>
    </item>
    
    <item>
      <title>mitmproxy使用</title>
      <link>/blog/2018/07-06-mitmproxy%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Fri, 06 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/07-06-mitmproxy%E4%BD%BF%E7%94%A8/</guid>
      <description>介绍一款非常好用的抓包工具，官网地址：https://www.mitmproxy.org。实际上，在调试苹果IAP支付时，始终没有抓成功过，反而因为设置了代理，导致苹果沙盒用户无法成功支付。它名字的全拼是Man-in-the-middle-proxy，代表中间人攻击。
常用的快捷键  在列表界面，按回车进入详情界面 在详情界面，按q返回列表界面 在详情界面，按tab键在Request,Response,Detail三个tab之间切换。按j，k可以滚动查看详情. 在列表界面，按G跳到最新一个请求 在列表界面，按g跳到第一个请求 在列表界面，按d删除当前选中的请求，按D恢复刚才删除的请求 在列表界面，按z清空请求列表  常用的过滤表达式 列表界面,按f进入过滤模式。详细的过滤表达式，可以查看：Filter expressions。
 ~h regex Header ~u regex URL ~m regex Method  原理  Subject Alternative Name：is an extension to X.509 that allows various values to be associated with a security certificate using a subjectAltName field. These values are called Subject Alternative Names (SANs). Names include Server Name Indication： is an extension to the TLS computer networking protocol by which a client indicates which hostname it is attempting to connect to at the start of the handshaking process.</description>
    </item>
    
    <item>
      <title>MySQL事务</title>
      <link>/blog/2018/07-01-mysql%E4%BA%8B%E5%8A%A1/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/07-01-mysql%E4%BA%8B%E5%8A%A1/</guid>
      <description>关于MySQL事务的诡异问题，至今没有调查出原因。但却也是一个契机，带我重新回忆之前的遇到的事务问题。
诡异的问题 系统中存在A和B两个表。B表中有两个关键字段：一个是唯一索引transaction_id，还有一个是标识处理状态的status。当status=0表示记录未被处理，status=1表示记录处理过了，不需要再处理了。
如果B中记录未处理，则在A表中插入一条权益记录，同时更新status=1，后续就不能再给用户加权益了。
代码做了如下处理：
func sessPart() { //开启事务 session := engine.NewSession() sess.Begin() defer session.Close() defer sess.Rollback() //插入价钱100分的权益交付记录 exchange := models.Exchange{Money: 100, Uid: 1} _, err := sess.Insert(exchange) if err != nil { sess.Rollback() return } //更新status为1 //并且使用乐观锁，防止因没有匹配到数据，直接返回成功 testModel := Test{ Status: 1, } affectRows, err := sess.Where(&amp;quot;transaction_id = ? AND status = 0&amp;quot;, 1). Cols(&amp;quot;status&amp;quot;).Update(&amp;amp;testModel) if err != nil || affectRows == 0 { sess.Rollback() return } sess.Commit() } //测试事务的并发情况 func BenchmarkLock(b *testing.</description>
    </item>
    
    <item>
      <title>TIME_WAIT状态解读</title>
      <link>/blog/2018/06-15-time_wait%E7%8A%B6%E6%80%81%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Fri, 15 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/06-15-time_wait%E7%8A%B6%E6%80%81%E8%A7%A3%E8%AF%BB/</guid>
      <description>版本：0.01
突然想梳理一下TIME_WAIT,毕竟自己遇到它好多次了。经常一块出现的问题：too many open file，当然，这个问题本身跟TIME_WAIT状态没啥必然的关系。
截取一下官方对TIME_WAIT的描述：
 The socket connection has been closed by the local application, the remote peer has closed its half of the connection, and the system is waiting to be sure that the remote peer received the last acknowledgement.
 time_wait status time_wait作为HTTP连接关闭的一个正常状态。当系统time_wait过多，超过操作系统设定的文件套接字上限时，就会导致整个服务不可用。
唯一确定连接的4个组成部分，它们是客户端及服务端的IP和PORT。一般来说，处于time_wait状态的port在2mls内是无法被重复使用的。所以瞬间的wait_wait过多，直接导致整个系统无法服务。
关闭连接包含4次握手，TCP是全双工的，有一端需要主动提出关闭。相应的，对端来被动来关闭。对于我们常见的CS模式，主动和被动的角色是没有明确界限的。


active close端的系统中才会出现time_wait的状态。拿请求https://google.com来举例，客户端在创建连接时，其实并不关心连接的端口号，它是系统随机创建的。google服务存在一个443端口,一直处于listen状态。当客户端断开连接时，客户端系统其实就会出现time_wait。当服务端主动断开连接时，客户端会出现close_wait状态。
2MLS time_wait也被称为2MLS wait。全名maximum segment lifetime, 表示一个数据块在被丢弃之前，在网络中能存在的最长时间。TCP的数据包是作为IP数据传输的，而IP数据包是否有效受限于设置的TTL，所以该MSL存在上限。
 在2MLS内，该连接不会处理那些迟到的请求，占用的端口号也无法被系统的其他程序使用。 在TCP连接中,ACK消息本身是不安全，因为peer不需要对ACK回复ACK。所以，2MLS保证了当被动关闭的一端没有收到ACK时，重新发送一次FIN报文。  可以通过tcp_tw_reuse来重用time_wait状态的端口号。
shell查询time_wait连接 查看连接的状态，主要有两个命令netstat和ss。netstat有的ss都有，而且运行非常快。
netstat -n | awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) {print a, S[a]} }&#39;  匹配tcp连接，声明了数组S，$NF用于获取最后一列的数据，也就是tcp status，最后通过for语句输出。</description>
    </item>
    
    <item>
      <title>订单系统初识</title>
      <link>/blog/2018/06-08-%E8%AE%A2%E5%8D%95%E7%B3%BB%E7%BB%9F%E5%88%9D%E8%AF%86/</link>
      <pubDate>Fri, 08 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/06-08-%E8%AE%A2%E5%8D%95%E7%B3%BB%E7%BB%9F%E5%88%9D%E8%AF%86/</guid>
      <description>计划将订单系统做一下梳理，包括之前写的IAP支付。其中一些细节，自己描述的也不是特别满意。后续慢慢的完善。
整个订单系统，概括的讲，其实就是创建订单、支付订单、交付权益这三个过程。但里面涉及的东西，可能必我们想象的要多很多。
订单号 在调用支付时，一般都会要求创建订单号。该订单号是标识本地交易的凭证，必须做到系统唯一。
创建订单号也有很多学问。最差也得做到：首先，不重复；其次，别人不能通过订单号，推测出任何有意义的信息；再次，订单号长度一定要做好控制。
订单再支付 描述一个场景：用户下单之后，并没有立即支付，而是过了一段时间，重新支付之前下的订单。这种情况，如果涉及到扣减库存，还会设置订单的过期时间。
重新支付的时候可能会有点问题：很多支付接口，不允许使用同一个订单号支付两次，即使上一次没有支付成功。换句话说，当你想重新支付时，系统需要生成一个新的订单号。
这样会导致：用户仅仅成功购买了一个商品，后台却生成了数笔订单。后期的统计变得麻烦了不少。
我们的做法便是：在确认支付时，生成两个订单，一个父订单，一个子订单。父订单用来标识用户的购买行为，子订单用户跟第三方支付。
订单统一管理 对于业务简单的部门来说，订单号自己创建，自己管理完全足够了。
但公司想汇总各个部门的订单数据时会变得异常麻烦。所以需要统一的订单创建平台。该平台负责订单的创建、以及后续订单的状态管理。
整个系统也开始变得复杂起来了。订单创建、订单状态更新都需要通知“订单系统”。数据也开始出现不一致。
交付 在用户下单、支付成功之后，将购买的商品或权益给到用户。
实物商品 将用户的购买行为添加到他的购买记录等，让快递员将商品送到用户手中等。
虚拟商品 用户在王者荣耀上买了一套皮肤，这就属于购买虚拟商品。最终腾讯只需要给用户的数据库写一条权益记录就好，不存在发货的过程。
签收 拆单 用户可能一次性购买多个商品，但系统只生成一个订单。对于实物商品，会涉及的拆单流程。比如用户买了肥皂和书，后台系统需要将肥皂交给卖家A来发货，将书交给卖家B来发。同样的商品，可能还会根据卖家和买家的距离来拆单。
签收 那么当更新订单签收状态时，就有疑问了。一个订单，它可能对应多个商品，那么只有商品全部被签收成功，才应该修改为已签收状态。但这种全部成功或者全部失败的状态，本身就很难保证。
所以签收应该针对具体商品。商城系统中，有两个常用的概念：SPU和SKU。那小米手机来举例，小米Note可以当作是一个SPU,而具体的红色-32G等能具体到一个实际的个体的就是SKU。
最终，签收状态应该是订单号+SKU_ID来决定的。
表结构设计 在用户下单、支付的过程中，跟订单内部商品的SKU关系不大，而且为了保证订单的幂等性,将订单设置为唯一索引是必须的。
在签收的过程中，无法做到针对订单来签收，而应该对订单下的SKU做签收。所以签收表应该独立出来。</description>
    </item>
    
    <item>
      <title>Go反射</title>
      <link>/blog/2018/05-31-go%E5%8F%8D%E5%B0%84/</link>
      <pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-31-go%E5%8F%8D%E5%B0%84/</guid>
      <description>人生的挫折感并不取决于境遇本身，而取决于境遇和自我期待之间的落差。
 概述 对interface类型操作，如何对内部的值进行处理和分析。比如判断interface是否底层存储的是struct类型，以及该struct是否含有某个特定的Field值。
interface类型包含两部分内容：dynamic type和dynamic value。当转换为interface类型后（操作是默认的），原类型下声明的方法，interface类型就无法再调用了。
实际工作中，interface类型会接收任意类型的值，处理的过程很多都是通过reflect实现的。
reflect.Value reflect里两个主要角色：Value和Type。Value用于处理值的操作，反射过程中处理的值是原始值的值拷贝，所以操作中要注意区分值传递和地址传递。
对于指针类型的值，只有获取其原始值，才可以达到修改的目的。如下所示，obj实际类型是一个struct的指针，想要将其转换成“值类型”，调用Elem方法来实现。
//获取指针的实际类型 v := reflect.ValueOf(obj) //Kind == Ptr v = v.Elem() if v.Kind() != reflect.Struct { return NewError(http.ErrorInvalidParam, &amp;quot;interface类型必须是struct结构&amp;quot;, nil) }  一些其他的操作，比如通过reflect.Value获取reflect.Type类型，通过诸如Index、Elem、MapIndex、Field等来操作不同的数据类型，当然调用前最好结合Kind对实际类型进行判断，保证调用的安全性。
查找指定的Field 我们假设struct中包含有某个特殊Field，那么在接口层面该如何进行判断呢？比如，查看结构体中是否含有Data的Field.
reflect本身提供了多种判断形式。以FieldByName为例，Type和Value都实现了该方法，但返回值不相同。reflect要求调用的值本身需要是struct类型才可以。
h := v.FieldByName(HeaderHField) //HeaderHField为自定义常亮 if h.IsValid() { }  将value转换为interface类型 reflect操作的interface类型，即由interface转换为reflect.Value类型，同样，逆向的转换也是可以的。
它提供了interface()方法。转换之后，我们就可以继续使用断言进行实际类型转换了。
value := h.Interface() //将value转换为interface customHead, isOk := value.(string) // 断言为string类型  安全设置结构体中的值 为了使反射过程变得更安全，需要了解几个函数
 CanAddr   CanAddr reports whether the value&#39;s address can be obtained with Addr.</description>
    </item>
    
    <item>
      <title>shell操作文本实例</title>
      <link>/blog/2018/05-15-shell%E6%93%8D%E4%BD%9C%E6%96%87%E6%9C%AC%E5%AE%9E%E4%BE%8B/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-15-shell%E6%93%8D%E4%BD%9C%E6%96%87%E6%9C%AC%E5%AE%9E%E4%BE%8B/</guid>
      <description>从掌握awk的基本指令，到在工作中熟练使用，中间还有一段路要走！通过总结一些工作中需要的案例，来加深理解。
打印符合条件的前一行记录 这个案例很抽象，可能非常难遇到，除非自己给自己挖坑。
业务代码中处理每条数据，都会顺序输出两条日志。第一条表示要处理的数据内容，第二条表示处理的结果。现在想滤出来所有处理成功的记录。比如日志文件如下：
uid=1 HAPPY uid=2 SAD uid=3 SAD  处理的AWK脚本如下：
#! /bin/bash # testPage表示日志文件 pieces=$(awk &#39;/HAPPY/{line=NR-1;print line}&#39; testPage | xargs) for piece in $pieces do echo $(awk NR==$piece testPage) done  脚本总结：
 首先通过正则表达式过滤，获取执行成功的数据行号。然后传递给xargs，转换为空格分隔的字符串。 遍历每个行号。值得注意，shell中对空格分隔的字符串可以直接使用for...in 通过awk条件判断，打印该行内容  求解两个文件的差集 使用comm实现 存在A和B两个系统，理论上A系统中的数据都应该存在于B系统中。但当核对数据时，发现两者数据不一致。如何有效的找出数据的差集。
具体到真实环境，通过MySQL，导出了满足条件的A、B系统数据的id，文件格式是csv。但当我执行如下列命令，获取仅仅在文件a中存在的记录时，发现数据完全不正确。
comm -2 -3 a.txt b.txt  调查发现，需要将两个文本先排好序，才能正常返回。
sort a.txt &amp;gt; a-sort.txt sort b.txt &amp;gt; b-sort.txt comm -2 -3 a-sort.txt b-sort.txt  使用uniq及sort实现 如下这种方式，仅仅可以找出不匹配的记录。无法区分数据是仅仅存在A系统，还是仅仅存在B系统。只能获取数据的交集和差集两部分。
原理很简单，将两个系统的文件合并到一个文件，然后排序。最终交集的数据，应该有2条记录。差集的记录，只有1条。
cat a.txt b.</description>
    </item>
    
    <item>
      <title>代码重构</title>
      <link>/blog/2018/05-12-%E4%BB%A3%E7%A0%81%E9%87%8D%E6%9E%84/</link>
      <pubDate>Sat, 12 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-12-%E4%BB%A3%E7%A0%81%E9%87%8D%E6%9E%84/</guid>
      <description>重构自己的代码是一件幸福的事，重构别人的代码确是一件不幸的事。尤其是被重构代码的人还没有离职的时候&amp;hellip;
重构切记引入意外复杂度，同时要保证代码功能单一，没有冗余的、意义不明的代码存在。还要保证最简API原则。一个接口好坏，一定要明确区分：它不能再处理更多业务了，还是这些业务已经不能再被剥离了。
临时变量 这是重构的一个重要切入点。方法体内临时变量太多，尤其是一些词不表意的临时变量，理解维护是一件非常痛苦的事情。
是否可以将临时变量直接替换成方法调用，或者将临时变量规整到一个粒度更小的方法体中。
重复 代码重复，多处copy相同的代码，会让人迫不及待的想要重构。
项目中重复代码过多，会给维护、开发带来很大的不便。一个简单的逻辑修改，但却涉及修改好多处代码，还不能保证涉及的部分都修改了。这确实是一件头疼的事。
所以代码要做好封装：
 封装的粒度要把握好。一方面便于测试需要，另一方面通过组合，还能满足其他需求。 封装要考虑参数如何传递。包括是否应该包含成员变量，参数的个数多少合适。 封装的方法应该如何归类。怎样可以将方法归到最合适的类。  多态 当遇到很多的switch或者if-elseif的时候，可以考虑是否能用多态来替换。
比如下面的方法：
switch type { case movie.TV: case movie.Release }  需要特别提醒：movie包的常量作为判断条件，该方法就最好应该在movie包中。这样当需求变更时，便体现出最小修改原则。
我们提取一个movieType的抽象类，然后依次对每个类型实现相应的方法，通过声明类型为movieType的成员变量，实现不同类型的统一调用。
这里体现的是模块化的思想。将系统拆分成独立的模块，降低耦合度。这样做的好处：便于扩展。当新的类型添加时，对老的业务来说：零干扰。
但某些情况下，这样的拆分模式可能会有小缺陷。当类型是一个频繁被添加、修改的参数时，这样的模式就显得很冗余。这时可以使用属性拆分。将各个类型中的属性实现多态，也可以称做是一种策略转移。
使用类替换枚举类型 当一个类内含有多个常量枚举类型时，可以考虑将枚举类型的值封装成新的对象。这也是一个切入点。举个例子：
// original code const( Month int = iota Year int ) //modified code //将这些常量封装到另一个package中  访问成员变量 两个分歧：直接访问VS间接访问。两者均有好处，</description>
    </item>
    
    <item>
      <title>MySQL使用总结(一)</title>
      <link>/blog/2018/05-09-mysql%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%E4%B8%80-/</link>
      <pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-09-mysql%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%E4%B8%80-/</guid>
      <description>查询的执行时间 第一次遇到查询时候报超时。很好奇，别的工具是如何修改查询的操时时间。
set max_statement_time = 0;  By using max_statement_time, it is possible to limit the execution time of individual queries.
 The MySQL version of max_statement_time is defined in millseconds, not seconds. MySQL&amp;rsquo;s implementation can only kill SELECTs.  left join 这个语句执行起来特别的费劲，但需求是：找出A表中存在，但B表中不存在的记录。
on条件 一直以为on是在执行表关联时的判断逻辑，即两个表的记录要不要关联，全靠on。直到遇到left join。发现它完全没有理会on提供的左表过滤条件，它返回了左表的全部记录，需要将条件放到where中才生效。
举个例子
-- table_a.id &amp;gt; 2018 无效 select * from table_a left join table_b on table_a.id = table_b.a_id and table_a.id &amp;gt; 2018 -- 正确的方式 select * from table_a left join table_b on table_a.</description>
    </item>
    
    <item>
      <title>Go test基础用法</title>
      <link>/blog/2018/05-02-go-test%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95/</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-02-go-test%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95/</guid>
      <description>版本：0.04
 当直接使用IDE进行单元测试时，有没有好奇它时如何实现的？比如GoLand写的测试用例。
 所有的代码都需要写测试用例。这不仅仅是对自己的代码负责，也是对别人的负责。
最近工作中使用glog这个库，因为它对外提供的方法都很简单，想封装处理一下。但却遇到了点麻烦：这个包需要在命令行传递log_dir参数，来指定日志文件的路径。
所以，正常运行的话，首先需要编译可执行文件，然后命令行指定参数执行。如下示例：
go build main.go ./main -log_dir=&amp;quot;/data&amp;quot; //当前目录作为日志输出目录  但在go test的时候，如何指定这个参数了？
Test 调查发现，发现go test也可以生成可执行文件。需要使用-c来指定。示例如下：
go test -c param_test_dir //最后一个参数是待测试的目录  执行后就会发现：这样的做法，会运行所有的Test用例。如何仅仅执行某一个测试用例了（编译器到底是如何做到的？）。
这里有另一个属性-run，用来指定执行的测试用例的匹配模式。举个例子：
func TestGetRootLogger(t *testing.T) { writeLog(&amp;quot;测试&amp;quot;) } func TestGetRootLogger2(t *testing.T) { writeLog(&amp;quot;测试2&amp;quot;) }  当我在命令行明确匹配执行Logger2，运行的时候确实仅仅执行该测试用例
go test -v -run Logger2 ./util/ //-v表示verbose，输出相信信息  但是，我发现，在指定了c参数之后，run参数无法生效！这样的话，还真是没有好的办法来处理这种情况。
使用map的测试 可以结合使用闭包，设置期望值，来写测试用例。Run函数内部是阻塞的，所以TestSum方法依次执行测试。
同时testSumFunc返回了test方法使用了闭包的特性，对返回函数内部的值是无法确定的。
func TestSum(t *testing.T) { t.Run(&amp;quot;A&amp;quot;, testSumFunc([]int{1, 2, 3}, 7)) t.Run(&amp;quot;B&amp;quot;, testSumFunc([]int{2, 3, 4}, 8)) } func Sum(numbers []int) int { total := 0 for _, v := range numbers { total += v } return total } func testSumFunc(numbers []int, expected int) func(t *testing.</description>
    </item>
    
    <item>
      <title>Saga Pattern</title>
      <link>/blog/2018/04-24-saga-pattern/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-24-saga-pattern/</guid>
      <description>在微服务中，用的比较多的分布式事务模式：SAGA。下面是lysu/go-saga库中的描述：
 Saga is a long-lived transaction came up with many small sub-transaction.ExecutionCoordinator(SEC) is coordinator for sub-transactions execute and saga-log written.Sub-transaction is normal business operation, it contain a Action and action&amp;rsquo;s Compensate. Saga-Log is used to record saga process, and SEC will use it to decide next step and how to recovery from error.Log presents Saga Log. Saga Log used to log execute status for saga, and SEC use it to compensate and retry.</description>
    </item>
    
    <item>
      <title>docker基本使用</title>
      <link>/blog/2018/04-20-docker%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-20-docker%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid>
      <description>知道一点比完全不知道要好，对问题有深入了解比仅知道皮毛要好。作为docker的一个初学者，现在对docker做简单记录。希望随着工作、生活，更深入的了解学习docker。这也是一件很有意义的事。
docker有几个相关的概念：
 image 镜像 container 容器  我觉得之所以说docker好用，是因为Docker Hub提供了很多镜像，比如MySQL、Redis等。对它们安装、卸载异常方便。
Example 我们想搭建测试服务，安装MySQL，Redis等依赖。我们将他们当作一个项目的依赖，声明一个配置文件·db.yml，然后将这些依赖，类似于composer编辑：
version: &amp;quot;3&amp;quot; services: db: image: mysql:5.7 volumes: - /Users/neojos/dockerData/mysql restart: always environment: MYSQL_ROOT_PASSWORD: paytest MYSQL_DATABASE: paytest MYSQL_USER: neojos MYSQL_PASSWORD: neojos-pwd ports: - &amp;quot;3306:3306&amp;quot; myredis: image: redis restart: always volumes: - /Users/neojos/dockerData/redis ports: - &amp;quot;6379:6379&amp;quot; command: redis-server --appendonly yes  执行如下命令，MySQL和Redis的服务就启动了
docker-composer -f db.yml up  可以通过执行如下命令查看，确认是否有两个容器在运行。
docker container ls  这样很好，但当我想进去MySQL的容器内执行一些命令时，该怎么办呢？比如，我想确认下面的MySQL连接语句是否正确,而且我还一定要进去容器内执行MySQL命令行语句：
mysql -h 127.0.0.1 -P 3306 -u neojos -p&#39;neojos-pwd&#39; paytest  很简单,只需要执行如下指令。可以发现，已经进到MySQL命令行了。</description>
    </item>
    
    <item>
      <title>xorm使用reverse指令创建模版</title>
      <link>/blog/2018/04-19-xorm%E4%BD%BF%E7%94%A8reverse%E6%8C%87%E4%BB%A4%E5%88%9B%E5%BB%BA%E6%A8%A1%E7%89%88/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-19-xorm%E4%BD%BF%E7%94%A8reverse%E6%8C%87%E4%BB%A4%E5%88%9B%E5%BB%BA%E6%A8%A1%E7%89%88/</guid>
      <description>这只能算作一次小的功能介绍
结合我们使用过的go数据操作类的库，执行的逻辑基本都是：将数据库返回的数据，转换成我们提前声明的结构体对象，然后返回。
今天要介绍的就是如何自动创建每个table对应的结构体。
查看 xorm tool的介绍：
xorm reverse mysql root:@/xorm_test?charset=utf8 templates/goxorm  初看这个介绍，让我费了一段时间才理解。你可以在命令行查看它的具体含义：
xorm help reverse  命令中templates/goxorm其实是xorm提供好的模版路径。我错误的理解成了：执行命令生成结果的存储路径。
tmplPath Template dir for generated. the default templates dir has provide 1 template  其次就是mysql的连接语句：一般来说，都是这样写的：
username:pwd@ip:port/db?charset=utf8  但是使用上述方式却无法正常执行命令，正确的方式是：
xorm reverse &amp;quot;username:pwd@tcp(ip:port)/db?charset=utf&amp;quot; templates/goxorm  </description>
    </item>
    
    <item>
      <title>了解Laravel依赖注入</title>
      <link>/blog/2018/04-05-%E4%BA%86%E8%A7%A3laravel%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-05-%E4%BA%86%E8%A7%A3laravel%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/</guid>
      <description>随笔 突然想了解一下Laravel，然后发现：它没有我想象的那么简单，很多的调用都找不到入口。加载view的逻辑，看了很长时间，还是没有搞明白：一个值传递的参数，怎么好好的就变了呢？下面都是看别的的文章的总结，我还要继续完善，直到搞清楚这个view是怎么实现的。
看了两篇文章，介绍了如何使用xdebug断点调试php及测试性能。作为了解Laravel的必要工具，也介绍进来。
 How to Install Xdebug with PHPStorm and Vagrant Debugging and Profiling PHP with Xdebug  概要 使用use Illuminate\Container\Container;作为参考的例子。
可以浏览原创：Laravel Container (容器) 深入理解 (下)。
摘抄Laravel
 The Laravel service container is a powerful tool for managing class dependencies and performing dependency injection.
 通过config/app.php可以查看Laravel的Service container。Service下的register便是用来创建binding的。通过php artisan make:provider CustomServiceProvider创建自定义的ServiceProvider。
 There is no need to bind classes into the container if they do not depend on any interfaces.</description>
    </item>
    
    <item>
      <title>包管理工具</title>
      <link>/blog/2018/03-31-%E5%8C%85%E7%AE%A1%E7%90%86/</link>
      <pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/03-31-%E5%8C%85%E7%AE%A1%E7%90%86/</guid>
      <description>反思之前的过程，一直没有试图跟上技术的发展。恍然觉得，其实技术比买股票更能让我找到快乐。新的技术越来越多，能做的便是，持续保持蜗牛锲而不舍的精神，慢慢爬！
首先感谢这篇文章2018 年了，你还是只会 npm install 吗？，让我重新开始审视包管理工具。因为在PHP开发中有Composer，在Go的开发中有glide。但却没有尝试思考它们背后的那些为什么。
npm包管理 我一直不理解package.json和package-lock.json这两个文件的作用。直观上看，前者是我们项目所依赖的包，后者是各个包自身的明细依赖。但这样的设计却是经过多个版本迭代最终确定的形式。
当我们执行install或者update的时候，package-lock.json会根据nodemodules的更新而进行相应更新。当前就理解到这里，请看Composer
包的版本 包的版本号采用semver约束，由3个数字组成，格式必须为 MAJOR.MINOR.PATCH, 意为： 主版本号.小版本号.修订版本号。
约束还有一条：主版本号相同的升级版本必须提供向下兼容，但这仅仅是口头约束。测试版本的匹配，可以访问网址：https://semver.npmjs.com/。
 ^开头的版本：主版本号相同，大于等于小版本号的所有版本。 ~开头的版本：主版本、小版本号相同，大于等于修正版本的版本。 *或者x的版本：两者表示通配符。 在常规仅包含数字的版本号之外：表示不稳定的发布版本。  管理依赖 有时候，项目和项目之间存在引用依赖关系。比如将多个项目间共同使用的类在common项目下维护，然后其他项目project-1和project-2分别引用项目common。当project项目变得越来越多时，每次新的项目都需要手动拷贝common代码。
可以将common做为一个包来管理。创建package.json文件，将common项目托管到git仓库。执行npm install git_url就可以将common作为依赖包进行安装了。
npm除了安装git仓库的代码，也可以安装本地的代码。
npm install file:local-package-path  版本管理 svn或者git只需要提交package.json, package-lock.json, 不需要提交node_modules目录。
每次升级或降级版本，执行如下代码，相应的package.json，package-lock.json会自动更新：
npm install &amp;lt;package-name&amp;gt;@&amp;lt;version&amp;gt;  删除依赖包：
npm uninstall &amp;lt;package&amp;gt;  Composer管理 Composer生成的包管理目录叫vendor，它也是生成两个文件composer.lock和composer.json。composer.lock描述了项目的依赖以及其它的一些元信息。
composer.lock用来明确锁定安装包的具体版本信息，包证所有人安装的版本都是一致的。具体的原因在于：
 composer.json中指定的安装包版本，比如^2.0，只能确定该包的主版本号一定是2，当Composer在install的过程中，具体安装了该包符合条件的哪个版本，是无法从.json中看出来的。 同理，还是上面的例子，如果一个同事，数月前执行install安装的版本是2.0.0，后来这个包在2版本下发布了一个小版本2.1.0。另一个同事后来执行install，很可能就安装成了2.1.0  综上所述，composer.lock用来保证安装包的一致性，避免安装到不同的版本包，给生产环境带来的不确定性。
install/update install主要用来安装新包。当安装新包的时候，需要首先查看.lock文件是否存在，如果存在，安装.lock中指定的具体版本。如果不存在，直接安装。同时更新.json和.lock两个文件。
update主要用来更新.lock中安装的包。随着时间的推移，.json中的包可能又发布了新版本，所以update就是用来检查.json中包的新版本，更新.lock文件用的。
我在使用的过程中，比较倾向于使用下面的单个包操作的方式：
php composer.phar update monolog/monolog [...]  版本管理 在git环境中.json和.lock都需要被提交的版本控制。vendor目录就不需要啦。
Go下的版本管理工具 glide glide是go的版本管理工具。其实glide也是参考composer设计的，所以上面对composer的说法也同样有效。
在项目开发中也仅需要对 glide.</description>
    </item>
    
    <item>
      <title>HTTP总结-状态码</title>
      <link>/blog/2018/03-17-http%E6%80%BB%E7%BB%93-%E7%8A%B6%E6%80%81%E7%A0%81/</link>
      <pubDate>Sat, 17 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/03-17-http%E6%80%BB%E7%BB%93-%E7%8A%B6%E6%80%81%E7%A0%81/</guid>
      <description>给别人轻松讲明白一个问题，才能算自己真正了解这个问题。 &amp;gt; Origin Header 头让我熟悉了一次sheme
从HTTP的头Origin说起，想起之前客户端定义scheme，因为不了解，问了开发的同事“scheme是什么？”反正我当时是不明白他们讲的。
在了解HTTP Origin语法的时候，我其实才真正明白：scheme 指请求所使用的协议，通常是HTTP、HTTPS或者其他。
Origin: &amp;lt;scheme&amp;gt; &amp;quot;://&amp;quot; &amp;lt;host&amp;gt; [&amp;quot;:&amp;quot; &amp;lt;port&amp;gt;]  Origin表示请求来至哪个站点。在WebSocket通信的时候，明确指明要校验这个参数。
405 方法不被允许 (Method not allowed)。用来访问本页面的谓词不被允许，有时将POST请求修改为GET请求之后异常就解决了。
比如：Web端通过Ajax异步提交数据，并且是POST的方式。莫名奇妙的的发现返回的状态码是405。很有可能是服务端在处理请求时出错了，在Nginx返回时，返回了404.html或者500.html导致的。
500 服务器内部错误。比如：服务端处理出现异常。同时，在PHP错误日志中可以查看异常发生的调用栈信息。
502 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。比如nginx从php-fpm接收到了不完整的response数据。
比如：服务端尝试连接mysql，但长时间链接不上，就会返回502错误。
可以浏览一些具体的文章：
 http 502 和 504 的区别 Nginx一次奇怪的502 报错探究  503 服务暂时不可用，一段时间后服务就可以正常工作了。
在代码发布的时候可能会使用到。比如go服务，在发版前需要将之前运行的进程kill掉，之后启动新的进程。但这个过程间隙，会导致已连接的客户端处理中断。所以在发版之前，先返回503,等待已经接收的请求处理完成，然后升级。
504 网关超时。为了完成您的 HTTP 请求，该服务器访问一个上游服务器，但没得到及时的响应
比如：nginx超过了自己设置的超时时间，不等待php-fpm的返回结果，直接给客户端返回504错误。但是此时php-fpm依然还在处理请求（在没有超出自己的超时时间的情况下）。
0、超时、客户端主动断开连接 If you connect with the server, then you can get a return code from it, otherwise it will fail and you get a 0.</description>
    </item>
    
    <item>
      <title>Nginx总结</title>
      <link>/blog/2018/03-01-nginx%E6%80%BB%E7%BB%93/</link>
      <pubDate>Sat, 10 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/03-01-nginx%E6%80%BB%E7%BB%93/</guid>
      <description>location 指令 工作中经常用到的一个指令，用来对某个路径的请求做特殊处理。比如同样的链接在PC和Web显示不同的页面。
location修饰符 location block匹配request url中domain name 或者ip/por之后的请求部分，即请求资源的路径。
形式如下：
location optional_modifier location_match { }  如下是optional_modifier的类型：
  ​ optional_modifier ​ 含义   ​  =  ​ 请求的url必须严格匹配被location指定的路径，必须完全相同   ​ none ​  如果没有修饰符，将对url做前缀匹配 ​   ​ ^~  ​ 最佳的非正则表达式前缀匹配    ​ ~  ​ 大小写敏感的正则匹配    ​ ~*  ​ 大小写不敏感的正则匹配    location匹配规则  nginx会查找一个精确匹配。如果匹配到了 = modifier，匹配会立即终止，该location就会被选择处理这个请求。 如果没有精确匹配（= modifier），nginx继续进行前缀匹配，对于给定的url，选择最长的前缀匹配。然后依据下列规则，继续匹配。 如果最长的前缀匹配有（^~ modifier），nginx会立即结束查询，选择该location。如果没有 ^~ modifier，该匹配会被暂时存起来，以便搜索可以继续。 最长的匹配被存起来后，nginx会继续匹配正则表达式。nginx移动到 location list 的顶部，然后试着去匹配正则表达式，第一个被匹配的正则表达式会立即被选择处理请求，结束匹配。 如果没有正则表达式被匹配，则之前存储的最长location被选择用来处理请求。  特别需要理解的：nginx正则匹配结果优先于前缀匹配。但是前缀匹配在先，同时允许通过 ^~ 和 = 来改变这种趋势。</description>
    </item>
    
    <item>
      <title>IAP支付初识</title>
      <link>/blog/2018/02-08-iap%E6%94%AF%E4%BB%98%E5%88%9D%E8%AF%86/</link>
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/02-08-iap%E6%94%AF%E4%BB%98%E5%88%9D%E8%AF%86/</guid>
      <description>IAP全称In-App Purchase，也可以叫内购。查看百度百科，IAP是一种智能移动终端应用程序的付费模式。大概的意思：用户在APP内通过付费，来享受APP内提供的服务或体验。
我不仅仅想总结一下苹果的IAP，还想反思一下支付要注意的细节。
从问题入手：如何确认苹果交易的唯一标识。想要做到支付的幂等性，每一笔订单都应该有一个唯一的标识。来避免出现类似这样的现象：用户支付了一次，服务端却创建了多个订单。
交易的唯一标识 我们使用服务端校验支付流程，每笔交易都通过服务器请求苹果服务器来完成校验。
IAP支付的流程  苹果IAP支付有一个“事务”的概念。当用户支付完成，苹果会回调APP，传递一个receipt的凭证。 APP端本地校验receipt或者APP回传到自己Server端对其校验 校验通过后，APP端主动finish调该transaction  transaction理解 对于每一次支付，都会产生一个新的transaction，用来唯一标识该订单。客户端每次finish的对象也是它。
对于它是不是唯一的疑问，查阅了部分文档，有很多订阅型的产品的开发反馈：transaction在一段时间后可能会发生变化。但我查询的结果认为：transaction可以唯一确定一笔交易。
如下摘录苹果论坛的一段描述：
 There are two transactionIdentifiers - the one that comes with the particular purchase and the one in the purchase receipt. Any call to updatedTransactions, including the call when you originally purchase the IAP, has a transaction.transactionIdentifier that is always unique. When you originally purchase an IAP or when you repurchase an IAP for free or when you restore an IAP the receipt will also contain the &amp;ldquo;unique&amp;rdquo; transaction_id of the original purchase transaction.</description>
    </item>
    
    <item>
      <title>WebSocket基础开发</title>
      <link>/blog/2018/02-25-websocket%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/02-25-websocket%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%91/</guid>
      <description>WebSocket是一种网络通讯协议。在服务器端可以将HTTP请求升级为WebSocket请求。区别于普通的HTTP请求，WebSocket中存在特殊的字段标识：
GET /chat HTTP/1.1 Host: server.example.com Upgrade: websocket Connection: Upgrade Sec-WebSocket-Key: x3JJHMbDL1EzLkh9GBhXDw== Sec-WebSocket-Protocol: chat, superchat Sec-WebSocket-Version: 13 Origin: http://example.com  这种协议升级，是在应用层实现的。所以一个服务器本身既可以提供WebSocket服务，也可以提供正常的HTTP服务。
我们下面对服务做区分。/ws负责对外提供WebSocket服务。
http.HandleFunc(&amp;quot;/&amp;quot;, serveForHttp) http.HandleFunc(&amp;quot;/ws&amp;quot;, serveForWs)  将HTTP请求升级为WebSocket请求,处理连接的读写操作：
func serveForWs(w http.ResponseWriter, r *http.Request) { if r.Method != &amp;quot;GET&amp;quot; { http.Error(w, &amp;quot;Method not allowed&amp;quot;, http.StatusMethodNotAllowed) return } conn, err := upgrader.Upgrade(w, r, nil) if err != nil { log.Println(err) return } go client.write() go client.read() }  Upgrader负责连接升级，同时指定连接的部分属性。包括ReadBufferSize，WriteBufferSize，CheckOrigin等。Upgrader默认会检查header头中Origin是否有效，如果你要使用这个默认函数的话，需要确保客户端请求头中包含Origin。
var upgrader = websocket.</description>
    </item>
    
    <item>
      <title>Protobufs在Logstash中的应用</title>
      <link>/blog/2018/01-30-protobufs%E5%9C%A8logstash%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</link>
      <pubDate>Tue, 30 Jan 2018 20:10:33 +0000</pubDate>
      
      <guid>/blog/2018/01-30-protobufs%E5%9C%A8logstash%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</guid>
      <description>ELK 分别是Elasticsearch、Logstash、Kibana技术栈的结合。主要解决的问题在于：系统服务器多，日志数据分散难以查找，日志数据量大，查询速度慢，或者不够实时。
 在trivago，我们主要依靠ELK来处理日志。我们通过使用Kafaka，将服务器的访问日志、错误日志、性能基准数据及各种各样的诊断日志，传递给Logstash，Logstash处理之后将日志存放到Elasticsearch。在数据传输中，我们更倾向使用protocol buffer对数据进行编码。这篇博客，我们将主要介绍如何使用Logstash来解析protobuf编码的消息。 相比无模式的JSON格式，Protobufs是一种有模式、高效的数据序列化格式。我们在Kafaka中传输的数据，很多都在使用Protocol Buffers进行编码。它的优势就在于：首先，编码后的数据size明显要比其他的编码方式要小。以JSON编码举例，消息体中不仅仅包含实际数据，还有对应的Key值及很多的中括号。对于文档结构基本不变的数据，传输中包含这些附加信息，是一种资源的浪费。当发送端和接收端对交互的文档结构达成一致后，传输过程还携带这部分结构信息就显得多余。在整个日志处理过程中，该部分消耗的资源是可以被节省下来。其次，消费者所处理的数据，数据格式都是约定好的，完全不会像JSON一样，莫名奇妙多出一个字段。同时，给数据字段的理解产生误解。
不开心的是，Logstash不支持Protobufs编解码。目前，它支持纯文本、JSON格式和其他别的消息格式。因此，我们决定自己实现这部分功能。
如何写Logstash的编码器 写一个Logstash插件是相对容易的，你只需要掌握一些基本的Ruby知识。Ruby语言天生直观简单，你很可能在查看示例代码的同时，就将它学会了。对初学者而言，tryruby.org是很不错的学习网站。你需要在电脑上安装Jruby，其他环境部分请参照elastic&amp;rsquo;s documentation。当你发现github上codec项目是空的时候，请不要困惑，请你clone JSON codec或plain codec来代替。通过这个过程，你将会了解到现存的插件是如何开发的，同时，你还能掌握ruby的相关知识。
校验JAVA环境变量是否设置成功
java -version  获取protobufs 最后，你下载logstash-codec-protobuf插件来解码protobuf消息。要使用这个插件，你需要一些proto文件和使用该proto格式编码的数据。如果这个proto文件已经在别的工程中使用了，那么你就仅仅需要创建proto文件的Ruby版本。如果这完全是一个新的项目，那么你可能需要先从Google&amp;rsquo;s developer pages了解一下proto 文件的语法规则，找到适合你项目的工具链，然后编译proto文件。
列举一个proto文件的sample，使用Go编码：
//使用proto3， 不支持optional选项 syntax = &amp;quot;proto3&amp;quot;; package tutorial; message Person { string name = 1; int32 id = 2; // Unique ID number for this person. string email = 3; }  安装插件 从rubygems下载logstash的插件，执行如下命令：
bin/plugin install PATH_TO_DOWNLOADED FILE  这个解码器支持Logstash 1.x和2.x版本。
创建Ruby版本的protobufs文件 假设下面的 unicorn.pb文件是我们定义的proto文件，我们将要使用它去解码消息：
package Animal; message Unicorn { // colour of unicorn optional string colour = 1; // horn length optional int32 horn_length = 2; // unix timestamp for last observation optional int64 last_seen = 3; }  下载ruby-protoc的编译器，然后运行:</description>
    </item>
    
    <item>
      <title>RequireJS初识</title>
      <link>/blog/2018/01-29-requirejs%E5%88%9D%E8%AF%86-/</link>
      <pubDate>Mon, 29 Jan 2018 20:10:33 +0000</pubDate>
      
      <guid>/blog/2018/01-29-requirejs%E5%88%9D%E8%AF%86-/</guid>
      <description>开发上想找一个时间选择控件，无意中就找到了layDate 日期与时间组件。
直接下载源码，在文件中引入.js和.css文件，但是调用的时候产生异常了。很好奇！作为一名服务端开发，我一直都是这样搞的，百试不爽。今天却翻车了！！查看错误发现是require这个方法报的错，主要的原因是：laydate未定义。
为什么要引入requirejs，这个东西到底该怎么用呢？从后端的角度看：它其实就是扮演PHP中的 spl_autoload_register的角色。当执行JS的时候，自动去调用执行脚本所需的js。
所以对代码做如下修改,主要用来配置加载js的路径。
注：如果是本地资源，千万不要写成如下代码所示的：域名+路径的形式（见注释掉的baseUrl）。当正式服和测试服域名不相同时，就比较麻烦。我就是因为我们测试服的域名是woniu-test，正式服我的域名是woniu，结果require的加载请求一直请求的是woniu的域名，找了半天才发现这个问题。
&amp;lt;script&amp;gt; requirejs.config({ //baseUrl: &#39;http://woniu/resource&#39; baseUrl: &#39;/resource/&#39;, //paths: { // laydate: &#39;js/laydate&#39; //}, //shim:{ // &#39;laydate&#39;: { // deps: [&#39;js/laydate&#39;], // exports: &#39;laydate&#39; // } //} }); &amp;lt;/script&amp;gt;  调用的时候直接如下调用，就ok了！！
&amp;lt;script&amp;gt; require([&#39;js/laydate&#39;], function (_){ // called once the DOM is ready laydate.render({ elem: &#39;#input-end-time&#39;, //指定元素 type: &#39;datetime&#39; }); laydate.render({ elem: &#39;#input-start-time&#39;, //指定元素 type: &#39;datetime&#39; }); }); &amp;lt;/script&amp;gt;  </description>
    </item>
    
    <item>
      <title>Memcached遇到的json_decode问题</title>
      <link>/blog/2018/01-21-memcached%E9%81%87%E5%88%B0%E7%9A%84json_decode%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sun, 21 Jan 2018 15:48:33 +0000</pubDate>
      
      <guid>/blog/2018/01-21-memcached%E9%81%87%E5%88%B0%E7%9A%84json_decode%E9%97%AE%E9%A2%98/</guid>
      <description>Memcached 是一个高性能的分布式缓存系统，使用Key-Value存储字符串和对象。通常来说，它主要用于缓存从数据库中检索到的数据以及第三方服务的数据等。简单的说，它可以提升服务器的性能。几乎所有的程序语言都可以接入它的API。如下例子所示：
public function getYouData(string $key) { $yourData = $memcached-&amp;gt;get($key); if (!$yourData) { $yourData = $yourDb-&amp;gt;getAll(); $memcached-&amp;gt;set($key, $yourData); } return $yourData; }  在trivago， 我们使用Memcached做缓存层，而且我们对外仅提供缓存接口。开发过程中，程序员不需要考虑缓存的内部实现，仅仅知道如何调用接口就可以了。目前，该API在PHP的代码库中几乎都有使用。我们使用Memchached的场合已经相当多了，随着每次新版本发布，使用量还在增加。
一天，系统日志文件里几乎全是Memcached的报错，get方法调用失败，导致所有的请求直接打到了数据库上。当然，在巨大负载的情况下，这些请求最终也失败了。最终，我们遇到了影响trivago整个平台能正常运行的问题。
那么到底发生了什么？为什么Memcached开始出问题了？
 Botnet也就是我们所说的僵尸网络，是指采用一种或多种传播手段，将大量主机感染bot程序（僵尸程序），从而在控制者和被感染主机之间所形成的一个可一对多控制的网络。
 原因是来至于200个国家，70K独立IP的网络攻击，直接导致当时负载飙升到平时的40倍。10分钟后我们的蛛网节流机制被触发，攻击的影响被慢慢减弱。
攻击造成Memcache的网络带宽饱和，直接原因是其中一个库的get/hit请求引起的。查看发现，这个库已经使用了大约4GB内存。很明显，这里有一些问题。
之后，我们对缓存记录了更加详细的日志。当然，我们之前也记日志，只是无法从现有日志中发现，究竟是哪些key消耗了大部分内存。因此，我们特别对value的占用内存大小做了额外的记录。
一天后，通过log记录，我们终于发现了这‘怪物’：ItemRepository 下的静态方法getAllItemData ，缓存的数据平均有10M左右。
仅仅只是名字，听起来就怪吓人的吧？更可怕的是，这个方法是2014年写的，从2015年起就再也没有被改动过。根据Blackfire的性能剖析，每加载一个页面就会调用30次getAllItemdata 方法。
接下来，我们单独对这个方法debug，为什么缓存的值会这么大？结论是：我们正在使用默认的Memcached serialization方法，更精确的讲，是原生的PHP serialize / unserialize方法（自从我们迁移到PHP7，我们就停止使用igbinary 的扩展，因为两者结合的时候会出现问题，因此序列化的工作又重新交给了php）。这也就意味着除了存储必要的数据之外，还需要额外存储对象的类名、属性等信息。
问题很明显了，这调整起来应该也非常简单。将php serizlization 成某种更加紧凑的数据存储格式。
当前的环境，使用igbinary的话，改动会非常大。因此我们考虑使用JSON或Protobuf，但是基于灵活性、快速实现的考虑，我们最终决定使用json，它是一种简单、轻量的数据存储格式。
JSON是无模式的数据结构，对数据进行编码非常方便，但解码的时候，需要将数据映射到对应的类上。
//people是一个类，json_encode不会编码对象的私有变量 $zhangsan = new people(&#39;zhangsan&#39;, &#39;boy&#39;, &#39;23&#39;) $json = array(&#39;people&#39; =&amp;gt; $zhangsan); $jsonEncode = json_encode($json); $jsonDecode = json_decode($jsonEncode);  我们考虑是否要使用一个外部的扩展：Symfony组件Serializer，然而经过一系列基准测试之后，我们还是决定手动实现数据编码和对象的映射关系。主要还是出于对PHP性能的考虑，对我们而言，手动实现也仅仅只是额外调用一次内部实例对象，并且，我们还可以灵活的对它进行调整。
 实现json_ecode方法，编码从数据库检索到数据 改变缓存中key的前缀，确保跟之前的不存在冲突 增加json_decode方法，用于从Memcached中获取数据 将数据转换成对应的PHP实体或对象  听起来很简单，是吧？但我们运行测试时，json_decode持续地返回错误：语法错误、控制字符错误、或者错误的UTF-8格式。</description>
    </item>
    
    <item>
      <title>Git分支模型</title>
      <link>/blog/2018/01-14-git%E5%88%86%E6%94%AF%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Sun, 14 Jan 2018 20:10:33 +0000</pubDate>
      
      <guid>/blog/2018/01-14-git%E5%88%86%E6%94%AF%E6%A8%A1%E5%9E%8B/</guid>
      <description>Git分支模型 文章将围绕下图来描述我们所使用的分支模型。主要包括master和develop两个主线分支以及feature、release、hotfixes分支。
为什么选择Git 针对“centralized”和“distributed”版本管理工具的争论，可以在GitSvnComparsion查看。就我个人而言，我更喜欢Git。Git改变了开发者对merge操作和branch操作的思考方式，而且两者也是Git日常工作流中的最常用的操作。
不集中式又集中式 Git是分布式版本管理系统，不存在集中式版本管理系统的中央存储库。这在技术角度上确实不存在，但在观念上，我们可以将origin看作整个版本管理的中央存储库。
如下图所示，开发者除了可以从origin中push或pull代码，还可以从别的分支中pull代码。当多个同事共同开发产品的新功能时，彼此间的代码同步显得尤为重要。
主要分支 Git中央存储库中包含两个重要的分支，它们在项目的生命周期中都一直存在：
 master develop  两个分支有如下特性：
 origin/master分支的HEAD 指针反映的一直都是发布就绪的状态。master分支上的代码也是生产服务的代码。 origin/develop分支的HEAD指针反映当前项目的修改，该分支集成其他分支所做的一切修改。甚至可以运行一个自动化脚本，每天晚上将各个分支的修改merge到develop分支。  当develop分支中的代码趋于稳定，准备发新版的时候，应该将其merger到master分支，并标记本次发布的版本号。稍后详细讨论。
原则上，master分支的代码都是可发布的，所以我们对merge到master的代码有严格的要求。理论上，我们可以运行一个脚本，一旦检测到master的代码有提交，自动执行编译、并同步代码到生产服务器。
支承分支 如master和develop旁边的其他分支，它们的生命周期有限，最终会从代码库中被移除。而我们使用分支主要来实现：
 来帮助各个团队之间并行开发 为新版本发布做准备 修复当前生产环境的bug。  我们使用的分支有以下几种:
 Feature branches Release branches Hotfix branches  各个分支根据不同的目的被创建，对它们的操作也遵循严格的规则。比如分支如何创建、开发完成之后merge到的对象等。
另外，这些分支其实都是普通的git分支。只是根据我们使用的目的策略给他们赋予了不同的功能。
Feature 分支 Feature 分支主要用来开新功能。一般来说，只要功能还没有开发完善，它就应该一直存在。但最终应该被merge回develop分支或者丢弃。feature分支遵循以下规则：
 从develop分支上创建feature分支 feature分支最终merge回develop分支 分支的命名规则：除了master, develop, release-*, or hotfix-*的任何名字  feature分支通常只存在于开发人员的版本库中，而不应该存在于origin仓库中。但考虑到团队成员协作开发的情况，彼此之间需要定期merge对方的代码，这是就需要借助develop分支来实现了。
创建feature分支 git checkout -b myfeature develop  合并feature 分支 git check develop git merge --no-off myfeature git branch -d myfeature git push origin develop  release分支 release分支主要用来为代码发布做准备。在合并代码之前，它允许做小的bug修改、为版本发布做准备工作（指定版本号、建数据表等）。通过在release分支上做这些操作，可以保证develop分支是干净的，不影响当前新功能的开发。release分支遵循下面的规则：</description>
    </item>
    
    <item>
      <title>Redis学习的惨痛经历</title>
      <link>/blog/2018/01-01-redis%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%83%A8%E7%97%9B%E7%BB%8F%E5%8E%86/</link>
      <pubDate>Mon, 01 Jan 2018 20:10:33 +0000</pubDate>
      
      <guid>/blog/2018/01-01-redis%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%83%A8%E7%97%9B%E7%BB%8F%E5%8E%86/</guid>
      <description>我们开发的产品类似于 trivago hotel search，Redis也多用来缓存临时数据。比如将操作频繁的流水数据先存储到redis，之后迁移到关系型数据库做持久化。
旅店查找的功能，前端主要是靠PHP和Symfony Framework开发，后端是Java。本章我们主要强调PHP和Redis的协作，目前它运行的非常稳定，但我们为实现这一步却花费了很大的精力。下面来说我们学习Redis的经历。
前言 起初我们使用的库是 Predis，一直到2013年我们开始使用phpredis (C实现)，主要因为二者的性能差异。
在2014年，我们给平台开发了新的特性，导致http 请求短时间内翻了一倍，结果有40%的请求HTTP 500: Internal Server Error.
之后查看日志，发现错误多是redis的连接问题： read error on connection 和 Redis server went away。
| WARN | ... Redis\ConnectException: Unable to connect: read error on connection ... #0 /.../vendor/.../Redis/RedisPool.php(106): ...\Redis\RedisPool-&amp;gt;connect(Object(Redis), Object(...\Redis\RedisServerConfiguration)) #1 /.../vendor/.../Redis/RedisClient.php(130): ...\Redis\RedisPool-&amp;gt;get(&#39;default&#39;, true) #2 /.../vendor/.../Redis/RedisClient.php(94): ...\Redis\RedisClient-&amp;gt;setMode(false) ... #17 /.../app/bootstrap.php.cache(551): Symfony\Bundle\FrameworkBundle\HttpKernel-&amp;gt;handle(Object(Symfony\Component\HttpFoundation\Request), 1, true) #18 /.../web/app.php(15): Symfony\Component\HttpKernel\Kernel-&amp;gt;handle(Object(Symfony\Component\HttpFoundation\Request)) #19 {main} | 12.34.56.78 | www.trivago.de | /?aDateRange%5Barr%5D=2014-05-20&amp;amp;aDateRange%5Bdep%5D=2014-05-21&amp;amp;iRoomType=1&amp;amp;iPathId=44742... | Mozilla/5.0 (WindowsNT 6.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>网站是使用hugo + GitHub搭建的静态网页。在这之前，一直用的是hexo，自从无意间看到了https://yihui.name/。不得不说，自己被博主的优秀以及博客的简单明了折服的一塌糊涂。
简单了解hugo之后，发现它更简单、更方便，所以将自己的博客也做了迁移。</description>
    </item>
    
  </channel>
</rss>